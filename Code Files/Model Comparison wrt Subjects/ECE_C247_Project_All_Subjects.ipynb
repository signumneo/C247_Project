{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JA-5zb09MvcQ"
      },
      "source": [
        "# ECE C247 - Neural Networks & Deep Learning\n",
        "# EEG Dataset\n",
        "\n",
        "# Group Members\n",
        "### Anirudh Krishna \n",
        "### Swagath Babu \n",
        "### Jacob Thomas"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The **models** are defined in the **models.py** file. \n",
        "### The **solver.py** python file contains the functions essential in the training and evaluation process. \n",
        "### The **utils.py** python file contains the filter definitions,functions needed for dataset loading and Gaussian noise definition. "
      ],
      "metadata": {
        "id": "3MVuQ9OfXRJa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the Dataset"
      ],
      "metadata": {
        "id": "jSLFeZ7CWfem"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33ma69ROKKx9",
        "outputId": "27daf603-55b1-435c-d1af-55fd42ded9f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUTceQ9hNnQ2"
      },
      "source": [
        "from models import *\n",
        "from solver import *\n",
        "from utils import *\n",
        "\n",
        "%matplotlib inline\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UwlDe0UQ3KQ"
      },
      "source": [
        "# Defining the Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHDBStjfQ45O"
      },
      "source": [
        "LR      = 0.0005\n",
        "BETAS   = (0.9, 0.999)\n",
        "EPS     = 1e-08\n",
        "DECAY   = 0.0005\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS  = 100"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### These are the final Set of Parameters Decided for the Model Training"
      ],
      "metadata": {
        "id": "ojP206vgW4Pf"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u59xJ4i0Kini"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijYr79w7PbX4",
        "outputId": "e1afa318-2c26-4120-e354-50d1806046a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "data_path = \"/content/drive/My Drive/Dataset/C247/\"\n",
        "\n",
        "# load data files\n",
        "X_train_valid, y_train_valid, X_test, y_test = load_data(data_path, subjects=[1,2,3,4,5,6,7,8,9], verbose=True) #Considering all the subject classes"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training/Valid data shape: (2115, 22, 1000)\n",
            "Test data shape: (443, 22, 1000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLwTyAw7Ki3J"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vh43GoyeKsWt"
      },
      "source": [
        "# filter inputs\n",
        "X_train_valid = filter_data(X_train_valid, fs=250, order=6, lowcut=7, highcut=30)\n",
        "X_test = filter_data(X_test, fs=250, order=6, lowcut=7, highcut=30)\n",
        "\n",
        "# smooth inputs\n",
        "X_train_valid = smooth_data(X_train_valid, ws=5)\n",
        "X_test = smooth_data(X_test, ws=5)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oElNaA-4KjbO"
      },
      "source": [
        "## PyTorch Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rq3HS1tXKuEa"
      },
      "source": [
        "# set up PyTorch dataloaders\n",
        "data_loaders = dataloader_setup(X_train_valid, y_train_valid, X_test, y_test, batch_size=BATCH_SIZE)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Tr4gG-DQvbc"
      },
      "source": [
        "# Model Comparison for all subjects"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0w34hNie-H1"
      },
      "source": [
        "## CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ty1ScpYGR4wQ",
        "outputId": "4f97f737-6cbb-4227-a21f-82a2f91e4c3e",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = CNN().cuda()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR, betas=BETAS, eps=EPS, weight_decay=DECAY)\n",
        "train_and_evaluate(model, optimizer, data_loaders, num_epochs=EPOCHS)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Progress: \tEpoch 1 [0/1692 (0.00%)]\t\tLoss: 1.40352\n",
            "Training Progress: \tEpoch 1 [320/1692 (18.87%)]\t\tLoss: 1.34767\n",
            "Training Progress: \tEpoch 1 [640/1692 (37.74%)]\t\tLoss: 1.29396\n",
            "Training Progress: \tEpoch 1 [960/1692 (56.60%)]\t\tLoss: 1.43321\n",
            "Training Progress: \tEpoch 1 [1280/1692 (75.47%)]\t\tLoss: 1.51185\n",
            "Training Progress: \tEpoch 1 [1600/1692 (94.34%)]\t\tLoss: 1.48559\n",
            "\tTrain loss: 0.04400, Accuracy: 462/1692 (27.30%)\n",
            "\tValidation loss: 0.00330, Accuracy: 123/423 (29.08%)\n",
            "\tTest loss: 0.00313, Accuracy: 133/443 (30.02%)\n",
            "\n",
            "Training Progress: \tEpoch 2 [0/1692 (0.00%)]\t\tLoss: 1.34617\n",
            "Training Progress: \tEpoch 2 [320/1692 (18.87%)]\t\tLoss: 1.35446\n",
            "Training Progress: \tEpoch 2 [640/1692 (37.74%)]\t\tLoss: 1.41832\n",
            "Training Progress: \tEpoch 2 [960/1692 (56.60%)]\t\tLoss: 1.39265\n",
            "Training Progress: \tEpoch 2 [1280/1692 (75.47%)]\t\tLoss: 1.36742\n",
            "Training Progress: \tEpoch 2 [1600/1692 (94.34%)]\t\tLoss: 1.39427\n",
            "\tTrain loss: 0.04355, Accuracy: 462/1692 (27.30%)\n",
            "\tValidation loss: 0.00327, Accuracy: 127/423 (30.02%)\n",
            "\tTest loss: 0.00311, Accuracy: 131/443 (29.57%)\n",
            "\n",
            "Training Progress: \tEpoch 3 [0/1692 (0.00%)]\t\tLoss: 1.44425\n",
            "Training Progress: \tEpoch 3 [320/1692 (18.87%)]\t\tLoss: 1.36434\n",
            "Training Progress: \tEpoch 3 [640/1692 (37.74%)]\t\tLoss: 1.29485\n",
            "Training Progress: \tEpoch 3 [960/1692 (56.60%)]\t\tLoss: 1.35573\n",
            "Training Progress: \tEpoch 3 [1280/1692 (75.47%)]\t\tLoss: 1.39062\n",
            "Training Progress: \tEpoch 3 [1600/1692 (94.34%)]\t\tLoss: 1.44938\n",
            "\tTrain loss: 0.04341, Accuracy: 445/1692 (26.30%)\n",
            "\tValidation loss: 0.00328, Accuracy: 115/423 (27.19%)\n",
            "\tTest loss: 0.00314, Accuracy: 96/443 (21.67%)\n",
            "\n",
            "Training Progress: \tEpoch 4 [0/1692 (0.00%)]\t\tLoss: 1.36478\n",
            "Training Progress: \tEpoch 4 [320/1692 (18.87%)]\t\tLoss: 1.36772\n",
            "Training Progress: \tEpoch 4 [640/1692 (37.74%)]\t\tLoss: 1.40411\n",
            "Training Progress: \tEpoch 4 [960/1692 (56.60%)]\t\tLoss: 1.42435\n",
            "Training Progress: \tEpoch 4 [1280/1692 (75.47%)]\t\tLoss: 1.42684\n",
            "Training Progress: \tEpoch 4 [1600/1692 (94.34%)]\t\tLoss: 1.42940\n",
            "\tTrain loss: 0.04335, Accuracy: 468/1692 (27.66%)\n",
            "\tValidation loss: 0.00329, Accuracy: 106/423 (25.06%)\n",
            "\tTest loss: 0.00314, Accuracy: 111/443 (25.06%)\n",
            "\n",
            "Training Progress: \tEpoch 5 [0/1692 (0.00%)]\t\tLoss: 1.32591\n",
            "Training Progress: \tEpoch 5 [320/1692 (18.87%)]\t\tLoss: 1.47281\n",
            "Training Progress: \tEpoch 5 [640/1692 (37.74%)]\t\tLoss: 1.42539\n",
            "Training Progress: \tEpoch 5 [960/1692 (56.60%)]\t\tLoss: 1.34578\n",
            "Training Progress: \tEpoch 5 [1280/1692 (75.47%)]\t\tLoss: 1.42395\n",
            "Training Progress: \tEpoch 5 [1600/1692 (94.34%)]\t\tLoss: 1.44150\n",
            "\tTrain loss: 0.04325, Accuracy: 463/1692 (27.36%)\n",
            "\tValidation loss: 0.00329, Accuracy: 103/423 (24.35%)\n",
            "\tTest loss: 0.00313, Accuracy: 101/443 (22.80%)\n",
            "\n",
            "Training Progress: \tEpoch 6 [0/1692 (0.00%)]\t\tLoss: 1.39463\n",
            "Training Progress: \tEpoch 6 [320/1692 (18.87%)]\t\tLoss: 1.39390\n",
            "Training Progress: \tEpoch 6 [640/1692 (37.74%)]\t\tLoss: 1.33698\n",
            "Training Progress: \tEpoch 6 [960/1692 (56.60%)]\t\tLoss: 1.32628\n",
            "Training Progress: \tEpoch 6 [1280/1692 (75.47%)]\t\tLoss: 1.46060\n",
            "Training Progress: \tEpoch 6 [1600/1692 (94.34%)]\t\tLoss: 1.41584\n",
            "\tTrain loss: 0.04326, Accuracy: 453/1692 (26.77%)\n",
            "\tValidation loss: 0.00328, Accuracy: 113/423 (26.71%)\n",
            "\tTest loss: 0.00313, Accuracy: 109/443 (24.60%)\n",
            "\n",
            "Training Progress: \tEpoch 7 [0/1692 (0.00%)]\t\tLoss: 1.38093\n",
            "Training Progress: \tEpoch 7 [320/1692 (18.87%)]\t\tLoss: 1.37831\n",
            "Training Progress: \tEpoch 7 [640/1692 (37.74%)]\t\tLoss: 1.35168\n",
            "Training Progress: \tEpoch 7 [960/1692 (56.60%)]\t\tLoss: 1.36716\n",
            "Training Progress: \tEpoch 7 [1280/1692 (75.47%)]\t\tLoss: 1.46943\n",
            "Training Progress: \tEpoch 7 [1600/1692 (94.34%)]\t\tLoss: 1.41476\n",
            "\tTrain loss: 0.04329, Accuracy: 462/1692 (27.30%)\n",
            "\tValidation loss: 0.00329, Accuracy: 116/423 (27.42%)\n",
            "\tTest loss: 0.00313, Accuracy: 112/443 (25.28%)\n",
            "\n",
            "Training Progress: \tEpoch 8 [0/1692 (0.00%)]\t\tLoss: 1.37758\n",
            "Training Progress: \tEpoch 8 [320/1692 (18.87%)]\t\tLoss: 1.36787\n",
            "Training Progress: \tEpoch 8 [640/1692 (37.74%)]\t\tLoss: 1.32753\n",
            "Training Progress: \tEpoch 8 [960/1692 (56.60%)]\t\tLoss: 1.39136\n",
            "Training Progress: \tEpoch 8 [1280/1692 (75.47%)]\t\tLoss: 1.40147\n",
            "Training Progress: \tEpoch 8 [1600/1692 (94.34%)]\t\tLoss: 1.37835\n",
            "\tTrain loss: 0.04319, Accuracy: 460/1692 (27.19%)\n",
            "\tValidation loss: 0.00328, Accuracy: 125/423 (29.55%)\n",
            "\tTest loss: 0.00313, Accuracy: 121/443 (27.31%)\n",
            "\n",
            "Training Progress: \tEpoch 9 [0/1692 (0.00%)]\t\tLoss: 1.36388\n",
            "Training Progress: \tEpoch 9 [320/1692 (18.87%)]\t\tLoss: 1.39057\n",
            "Training Progress: \tEpoch 9 [640/1692 (37.74%)]\t\tLoss: 1.35058\n",
            "Training Progress: \tEpoch 9 [960/1692 (56.60%)]\t\tLoss: 1.34354\n",
            "Training Progress: \tEpoch 9 [1280/1692 (75.47%)]\t\tLoss: 1.46250\n",
            "Training Progress: \tEpoch 9 [1600/1692 (94.34%)]\t\tLoss: 1.42357\n",
            "\tTrain loss: 0.04310, Accuracy: 487/1692 (28.78%)\n",
            "\tValidation loss: 0.00328, Accuracy: 116/423 (27.42%)\n",
            "\tTest loss: 0.00313, Accuracy: 120/443 (27.09%)\n",
            "\n",
            "Training Progress: \tEpoch 10 [0/1692 (0.00%)]\t\tLoss: 1.36505\n",
            "Training Progress: \tEpoch 10 [320/1692 (18.87%)]\t\tLoss: 1.35327\n",
            "Training Progress: \tEpoch 10 [640/1692 (37.74%)]\t\tLoss: 1.36548\n",
            "Training Progress: \tEpoch 10 [960/1692 (56.60%)]\t\tLoss: 1.36765\n",
            "Training Progress: \tEpoch 10 [1280/1692 (75.47%)]\t\tLoss: 1.40048\n",
            "Training Progress: \tEpoch 10 [1600/1692 (94.34%)]\t\tLoss: 1.39311\n",
            "\tTrain loss: 0.04300, Accuracy: 510/1692 (30.14%)\n",
            "\tValidation loss: 0.00329, Accuracy: 102/423 (24.11%)\n",
            "\tTest loss: 0.00313, Accuracy: 122/443 (27.54%)\n",
            "\n",
            "Training Progress: \tEpoch 11 [0/1692 (0.00%)]\t\tLoss: 1.37934\n",
            "Training Progress: \tEpoch 11 [320/1692 (18.87%)]\t\tLoss: 1.39492\n",
            "Training Progress: \tEpoch 11 [640/1692 (37.74%)]\t\tLoss: 1.35894\n",
            "Training Progress: \tEpoch 11 [960/1692 (56.60%)]\t\tLoss: 1.36378\n",
            "Training Progress: \tEpoch 11 [1280/1692 (75.47%)]\t\tLoss: 1.42008\n",
            "Training Progress: \tEpoch 11 [1600/1692 (94.34%)]\t\tLoss: 1.44808\n",
            "\tTrain loss: 0.04295, Accuracy: 485/1692 (28.66%)\n",
            "\tValidation loss: 0.00329, Accuracy: 114/423 (26.95%)\n",
            "\tTest loss: 0.00313, Accuracy: 120/443 (27.09%)\n",
            "\n",
            "Training Progress: \tEpoch 12 [0/1692 (0.00%)]\t\tLoss: 1.37937\n",
            "Training Progress: \tEpoch 12 [320/1692 (18.87%)]\t\tLoss: 1.35973\n",
            "Training Progress: \tEpoch 12 [640/1692 (37.74%)]\t\tLoss: 1.35808\n",
            "Training Progress: \tEpoch 12 [960/1692 (56.60%)]\t\tLoss: 1.37015\n",
            "Training Progress: \tEpoch 12 [1280/1692 (75.47%)]\t\tLoss: 1.41674\n",
            "Training Progress: \tEpoch 12 [1600/1692 (94.34%)]\t\tLoss: 1.39757\n",
            "\tTrain loss: 0.04287, Accuracy: 512/1692 (30.26%)\n",
            "\tValidation loss: 0.00329, Accuracy: 113/423 (26.71%)\n",
            "\tTest loss: 0.00315, Accuracy: 116/443 (26.19%)\n",
            "\n",
            "Training Progress: \tEpoch 13 [0/1692 (0.00%)]\t\tLoss: 1.35647\n",
            "Training Progress: \tEpoch 13 [320/1692 (18.87%)]\t\tLoss: 1.38837\n",
            "Training Progress: \tEpoch 13 [640/1692 (37.74%)]\t\tLoss: 1.33827\n",
            "Training Progress: \tEpoch 13 [960/1692 (56.60%)]\t\tLoss: 1.36897\n",
            "Training Progress: \tEpoch 13 [1280/1692 (75.47%)]\t\tLoss: 1.38094\n",
            "Training Progress: \tEpoch 13 [1600/1692 (94.34%)]\t\tLoss: 1.45684\n",
            "\tTrain loss: 0.04283, Accuracy: 507/1692 (29.96%)\n",
            "\tValidation loss: 0.00328, Accuracy: 113/423 (26.71%)\n",
            "\tTest loss: 0.00313, Accuracy: 116/443 (26.19%)\n",
            "\n",
            "Training Progress: \tEpoch 14 [0/1692 (0.00%)]\t\tLoss: 1.35147\n",
            "Training Progress: \tEpoch 14 [320/1692 (18.87%)]\t\tLoss: 1.44827\n",
            "Training Progress: \tEpoch 14 [640/1692 (37.74%)]\t\tLoss: 1.33279\n",
            "Training Progress: \tEpoch 14 [960/1692 (56.60%)]\t\tLoss: 1.33362\n",
            "Training Progress: \tEpoch 14 [1280/1692 (75.47%)]\t\tLoss: 1.44140\n",
            "Training Progress: \tEpoch 14 [1600/1692 (94.34%)]\t\tLoss: 1.46418\n",
            "\tTrain loss: 0.04275, Accuracy: 525/1692 (31.03%)\n",
            "\tValidation loss: 0.00328, Accuracy: 124/423 (29.31%)\n",
            "\tTest loss: 0.00313, Accuracy: 122/443 (27.54%)\n",
            "\n",
            "Training Progress: \tEpoch 15 [0/1692 (0.00%)]\t\tLoss: 1.37691\n",
            "Training Progress: \tEpoch 15 [320/1692 (18.87%)]\t\tLoss: 1.37147\n",
            "Training Progress: \tEpoch 15 [640/1692 (37.74%)]\t\tLoss: 1.34238\n",
            "Training Progress: \tEpoch 15 [960/1692 (56.60%)]\t\tLoss: 1.38508\n",
            "Training Progress: \tEpoch 15 [1280/1692 (75.47%)]\t\tLoss: 1.45186\n",
            "Training Progress: \tEpoch 15 [1600/1692 (94.34%)]\t\tLoss: 1.39518\n",
            "\tTrain loss: 0.04260, Accuracy: 536/1692 (31.68%)\n",
            "\tValidation loss: 0.00327, Accuracy: 125/423 (29.55%)\n",
            "\tTest loss: 0.00312, Accuracy: 116/443 (26.19%)\n",
            "\n",
            "Training Progress: \tEpoch 16 [0/1692 (0.00%)]\t\tLoss: 1.34981\n",
            "Training Progress: \tEpoch 16 [320/1692 (18.87%)]\t\tLoss: 1.35100\n",
            "Training Progress: \tEpoch 16 [640/1692 (37.74%)]\t\tLoss: 1.30401\n",
            "Training Progress: \tEpoch 16 [960/1692 (56.60%)]\t\tLoss: 1.33137\n",
            "Training Progress: \tEpoch 16 [1280/1692 (75.47%)]\t\tLoss: 1.40428\n",
            "Training Progress: \tEpoch 16 [1600/1692 (94.34%)]\t\tLoss: 1.40657\n",
            "\tTrain loss: 0.04263, Accuracy: 526/1692 (31.09%)\n",
            "\tValidation loss: 0.00328, Accuracy: 124/423 (29.31%)\n",
            "\tTest loss: 0.00313, Accuracy: 125/443 (28.22%)\n",
            "\n",
            "Training Progress: \tEpoch 17 [0/1692 (0.00%)]\t\tLoss: 1.32458\n",
            "Training Progress: \tEpoch 17 [320/1692 (18.87%)]\t\tLoss: 1.38161\n",
            "Training Progress: \tEpoch 17 [640/1692 (37.74%)]\t\tLoss: 1.29214\n",
            "Training Progress: \tEpoch 17 [960/1692 (56.60%)]\t\tLoss: 1.34452\n",
            "Training Progress: \tEpoch 17 [1280/1692 (75.47%)]\t\tLoss: 1.42972\n",
            "Training Progress: \tEpoch 17 [1600/1692 (94.34%)]\t\tLoss: 1.36011\n",
            "\tTrain loss: 0.04280, Accuracy: 513/1692 (30.32%)\n",
            "\tValidation loss: 0.00330, Accuracy: 116/423 (27.42%)\n",
            "\tTest loss: 0.00314, Accuracy: 124/443 (27.99%)\n",
            "\n",
            "Training Progress: \tEpoch 18 [0/1692 (0.00%)]\t\tLoss: 1.36610\n",
            "Training Progress: \tEpoch 18 [320/1692 (18.87%)]\t\tLoss: 1.40565\n",
            "Training Progress: \tEpoch 18 [640/1692 (37.74%)]\t\tLoss: 1.31624\n",
            "Training Progress: \tEpoch 18 [960/1692 (56.60%)]\t\tLoss: 1.41001\n",
            "Training Progress: \tEpoch 18 [1280/1692 (75.47%)]\t\tLoss: 1.46274\n",
            "Training Progress: \tEpoch 18 [1600/1692 (94.34%)]\t\tLoss: 1.37413\n",
            "\tTrain loss: 0.04284, Accuracy: 517/1692 (30.56%)\n",
            "\tValidation loss: 0.00330, Accuracy: 121/423 (28.61%)\n",
            "\tTest loss: 0.00313, Accuracy: 129/443 (29.12%)\n",
            "\n",
            "Training Progress: \tEpoch 19 [0/1692 (0.00%)]\t\tLoss: 1.31306\n",
            "Training Progress: \tEpoch 19 [320/1692 (18.87%)]\t\tLoss: 1.33828\n",
            "Training Progress: \tEpoch 19 [640/1692 (37.74%)]\t\tLoss: 1.32794\n",
            "Training Progress: \tEpoch 19 [960/1692 (56.60%)]\t\tLoss: 1.33407\n",
            "Training Progress: \tEpoch 19 [1280/1692 (75.47%)]\t\tLoss: 1.35512\n",
            "Training Progress: \tEpoch 19 [1600/1692 (94.34%)]\t\tLoss: 1.37653\n",
            "\tTrain loss: 0.04271, Accuracy: 507/1692 (29.96%)\n",
            "\tValidation loss: 0.00329, Accuracy: 123/423 (29.08%)\n",
            "\tTest loss: 0.00313, Accuracy: 123/443 (27.77%)\n",
            "\n",
            "Training Progress: \tEpoch 20 [0/1692 (0.00%)]\t\tLoss: 1.40322\n",
            "Training Progress: \tEpoch 20 [320/1692 (18.87%)]\t\tLoss: 1.35300\n",
            "Training Progress: \tEpoch 20 [640/1692 (37.74%)]\t\tLoss: 1.24310\n",
            "Training Progress: \tEpoch 20 [960/1692 (56.60%)]\t\tLoss: 1.29823\n",
            "Training Progress: \tEpoch 20 [1280/1692 (75.47%)]\t\tLoss: 1.42841\n",
            "Training Progress: \tEpoch 20 [1600/1692 (94.34%)]\t\tLoss: 1.39330\n",
            "\tTrain loss: 0.04280, Accuracy: 513/1692 (30.32%)\n",
            "\tValidation loss: 0.00330, Accuracy: 119/423 (28.13%)\n",
            "\tTest loss: 0.00316, Accuracy: 120/443 (27.09%)\n",
            "\n",
            "Training Progress: \tEpoch 21 [0/1692 (0.00%)]\t\tLoss: 1.37052\n",
            "Training Progress: \tEpoch 21 [320/1692 (18.87%)]\t\tLoss: 1.31552\n",
            "Training Progress: \tEpoch 21 [640/1692 (37.74%)]\t\tLoss: 1.33192\n",
            "Training Progress: \tEpoch 21 [960/1692 (56.60%)]\t\tLoss: 1.34542\n",
            "Training Progress: \tEpoch 21 [1280/1692 (75.47%)]\t\tLoss: 1.45829\n",
            "Training Progress: \tEpoch 21 [1600/1692 (94.34%)]\t\tLoss: 1.35392\n",
            "\tTrain loss: 0.04189, Accuracy: 595/1692 (35.17%)\n",
            "\tValidation loss: 0.00325, Accuracy: 127/423 (30.02%)\n",
            "\tTest loss: 0.00310, Accuracy: 122/443 (27.54%)\n",
            "\n",
            "Training Progress: \tEpoch 22 [0/1692 (0.00%)]\t\tLoss: 1.32588\n",
            "Training Progress: \tEpoch 22 [320/1692 (18.87%)]\t\tLoss: 1.32276\n",
            "Training Progress: \tEpoch 22 [640/1692 (37.74%)]\t\tLoss: 1.23352\n",
            "Training Progress: \tEpoch 22 [960/1692 (56.60%)]\t\tLoss: 1.34295\n",
            "Training Progress: \tEpoch 22 [1280/1692 (75.47%)]\t\tLoss: 1.32692\n",
            "Training Progress: \tEpoch 22 [1600/1692 (94.34%)]\t\tLoss: 1.34395\n",
            "\tTrain loss: 0.04246, Accuracy: 556/1692 (32.86%)\n",
            "\tValidation loss: 0.00330, Accuracy: 119/423 (28.13%)\n",
            "\tTest loss: 0.00315, Accuracy: 120/443 (27.09%)\n",
            "\n",
            "Training Progress: \tEpoch 23 [0/1692 (0.00%)]\t\tLoss: 1.29550\n",
            "Training Progress: \tEpoch 23 [320/1692 (18.87%)]\t\tLoss: 1.42848\n",
            "Training Progress: \tEpoch 23 [640/1692 (37.74%)]\t\tLoss: 1.30764\n",
            "Training Progress: \tEpoch 23 [960/1692 (56.60%)]\t\tLoss: 1.35571\n",
            "Training Progress: \tEpoch 23 [1280/1692 (75.47%)]\t\tLoss: 1.39756\n",
            "Training Progress: \tEpoch 23 [1600/1692 (94.34%)]\t\tLoss: 1.39390\n",
            "\tTrain loss: 0.04181, Accuracy: 607/1692 (35.87%)\n",
            "\tValidation loss: 0.00325, Accuracy: 139/423 (32.86%)\n",
            "\tTest loss: 0.00311, Accuracy: 127/443 (28.67%)\n",
            "\n",
            "Training Progress: \tEpoch 24 [0/1692 (0.00%)]\t\tLoss: 1.30174\n",
            "Training Progress: \tEpoch 24 [320/1692 (18.87%)]\t\tLoss: 1.27152\n",
            "Training Progress: \tEpoch 24 [640/1692 (37.74%)]\t\tLoss: 1.24224\n",
            "Training Progress: \tEpoch 24 [960/1692 (56.60%)]\t\tLoss: 1.33891\n",
            "Training Progress: \tEpoch 24 [1280/1692 (75.47%)]\t\tLoss: 1.36129\n",
            "Training Progress: \tEpoch 24 [1600/1692 (94.34%)]\t\tLoss: 1.37501\n",
            "\tTrain loss: 0.04211, Accuracy: 594/1692 (35.11%)\n",
            "\tValidation loss: 0.00330, Accuracy: 114/423 (26.95%)\n",
            "\tTest loss: 0.00315, Accuracy: 117/443 (26.41%)\n",
            "\n",
            "Training Progress: \tEpoch 25 [0/1692 (0.00%)]\t\tLoss: 1.31264\n",
            "Training Progress: \tEpoch 25 [320/1692 (18.87%)]\t\tLoss: 1.33278\n",
            "Training Progress: \tEpoch 25 [640/1692 (37.74%)]\t\tLoss: 1.30418\n",
            "Training Progress: \tEpoch 25 [960/1692 (56.60%)]\t\tLoss: 1.35341\n",
            "Training Progress: \tEpoch 25 [1280/1692 (75.47%)]\t\tLoss: 1.43480\n",
            "Training Progress: \tEpoch 25 [1600/1692 (94.34%)]\t\tLoss: 1.39948\n",
            "\tTrain loss: 0.04337, Accuracy: 529/1692 (31.26%)\n",
            "\tValidation loss: 0.00340, Accuracy: 112/423 (26.48%)\n",
            "\tTest loss: 0.00325, Accuracy: 114/443 (25.73%)\n",
            "\n",
            "Training Progress: \tEpoch 26 [0/1692 (0.00%)]\t\tLoss: 1.27896\n",
            "Training Progress: \tEpoch 26 [320/1692 (18.87%)]\t\tLoss: 1.32860\n",
            "Training Progress: \tEpoch 26 [640/1692 (37.74%)]\t\tLoss: 1.28184\n",
            "Training Progress: \tEpoch 26 [960/1692 (56.60%)]\t\tLoss: 1.31213\n",
            "Training Progress: \tEpoch 26 [1280/1692 (75.47%)]\t\tLoss: 1.37237\n",
            "Training Progress: \tEpoch 26 [1600/1692 (94.34%)]\t\tLoss: 1.39497\n",
            "\tTrain loss: 0.04204, Accuracy: 569/1692 (33.63%)\n",
            "\tValidation loss: 0.00331, Accuracy: 125/423 (29.55%)\n",
            "\tTest loss: 0.00316, Accuracy: 117/443 (26.41%)\n",
            "\n",
            "Training Progress: \tEpoch 27 [0/1692 (0.00%)]\t\tLoss: 1.29876\n",
            "Training Progress: \tEpoch 27 [320/1692 (18.87%)]\t\tLoss: 1.39304\n",
            "Training Progress: \tEpoch 27 [640/1692 (37.74%)]\t\tLoss: 1.23317\n",
            "Training Progress: \tEpoch 27 [960/1692 (56.60%)]\t\tLoss: 1.28072\n",
            "Training Progress: \tEpoch 27 [1280/1692 (75.47%)]\t\tLoss: 1.32129\n",
            "Training Progress: \tEpoch 27 [1600/1692 (94.34%)]\t\tLoss: 1.41545\n",
            "\tTrain loss: 0.04206, Accuracy: 589/1692 (34.81%)\n",
            "\tValidation loss: 0.00331, Accuracy: 118/423 (27.90%)\n",
            "\tTest loss: 0.00319, Accuracy: 122/443 (27.54%)\n",
            "\n",
            "Training Progress: \tEpoch 28 [0/1692 (0.00%)]\t\tLoss: 1.26581\n",
            "Training Progress: \tEpoch 28 [320/1692 (18.87%)]\t\tLoss: 1.25867\n",
            "Training Progress: \tEpoch 28 [640/1692 (37.74%)]\t\tLoss: 1.32511\n",
            "Training Progress: \tEpoch 28 [960/1692 (56.60%)]\t\tLoss: 1.37583\n",
            "Training Progress: \tEpoch 28 [1280/1692 (75.47%)]\t\tLoss: 1.34888\n",
            "Training Progress: \tEpoch 28 [1600/1692 (94.34%)]\t\tLoss: 1.39126\n",
            "\tTrain loss: 0.04174, Accuracy: 585/1692 (34.57%)\n",
            "\tValidation loss: 0.00331, Accuracy: 117/423 (27.66%)\n",
            "\tTest loss: 0.00317, Accuracy: 130/443 (29.35%)\n",
            "\n",
            "Training Progress: \tEpoch 29 [0/1692 (0.00%)]\t\tLoss: 1.21211\n",
            "Training Progress: \tEpoch 29 [320/1692 (18.87%)]\t\tLoss: 1.32407\n",
            "Training Progress: \tEpoch 29 [640/1692 (37.74%)]\t\tLoss: 1.22432\n",
            "Training Progress: \tEpoch 29 [960/1692 (56.60%)]\t\tLoss: 1.40883\n",
            "Training Progress: \tEpoch 29 [1280/1692 (75.47%)]\t\tLoss: 1.36355\n",
            "Training Progress: \tEpoch 29 [1600/1692 (94.34%)]\t\tLoss: 1.40685\n",
            "\tTrain loss: 0.04164, Accuracy: 611/1692 (36.11%)\n",
            "\tValidation loss: 0.00333, Accuracy: 127/423 (30.02%)\n",
            "\tTest loss: 0.00317, Accuracy: 136/443 (30.70%)\n",
            "\n",
            "Training Progress: \tEpoch 30 [0/1692 (0.00%)]\t\tLoss: 1.21188\n",
            "Training Progress: \tEpoch 30 [320/1692 (18.87%)]\t\tLoss: 1.27965\n",
            "Training Progress: \tEpoch 30 [640/1692 (37.74%)]\t\tLoss: 1.32159\n",
            "Training Progress: \tEpoch 30 [960/1692 (56.60%)]\t\tLoss: 1.32518\n",
            "Training Progress: \tEpoch 30 [1280/1692 (75.47%)]\t\tLoss: 1.33321\n",
            "Training Progress: \tEpoch 30 [1600/1692 (94.34%)]\t\tLoss: 1.24654\n",
            "\tTrain loss: 0.04083, Accuracy: 651/1692 (38.48%)\n",
            "\tValidation loss: 0.00331, Accuracy: 121/423 (28.61%)\n",
            "\tTest loss: 0.00314, Accuracy: 136/443 (30.70%)\n",
            "\n",
            "Training Progress: \tEpoch 31 [0/1692 (0.00%)]\t\tLoss: 1.22446\n",
            "Training Progress: \tEpoch 31 [320/1692 (18.87%)]\t\tLoss: 1.19453\n",
            "Training Progress: \tEpoch 31 [640/1692 (37.74%)]\t\tLoss: 1.28655\n",
            "Training Progress: \tEpoch 31 [960/1692 (56.60%)]\t\tLoss: 1.26574\n",
            "Training Progress: \tEpoch 31 [1280/1692 (75.47%)]\t\tLoss: 1.32855\n",
            "Training Progress: \tEpoch 31 [1600/1692 (94.34%)]\t\tLoss: 1.36646\n",
            "\tTrain loss: 0.04005, Accuracy: 680/1692 (40.19%)\n",
            "\tValidation loss: 0.00327, Accuracy: 131/423 (30.97%)\n",
            "\tTest loss: 0.00313, Accuracy: 131/443 (29.57%)\n",
            "\n",
            "Training Progress: \tEpoch 32 [0/1692 (0.00%)]\t\tLoss: 1.18198\n",
            "Training Progress: \tEpoch 32 [320/1692 (18.87%)]\t\tLoss: 1.22908\n",
            "Training Progress: \tEpoch 32 [640/1692 (37.74%)]\t\tLoss: 1.26734\n",
            "Training Progress: \tEpoch 32 [960/1692 (56.60%)]\t\tLoss: 1.23443\n",
            "Training Progress: \tEpoch 32 [1280/1692 (75.47%)]\t\tLoss: 1.31377\n",
            "Training Progress: \tEpoch 32 [1600/1692 (94.34%)]\t\tLoss: 1.28936\n",
            "\tTrain loss: 0.04068, Accuracy: 654/1692 (38.65%)\n",
            "\tValidation loss: 0.00332, Accuracy: 133/423 (31.44%)\n",
            "\tTest loss: 0.00317, Accuracy: 134/443 (30.25%)\n",
            "\n",
            "Training Progress: \tEpoch 33 [0/1692 (0.00%)]\t\tLoss: 1.24172\n",
            "Training Progress: \tEpoch 33 [320/1692 (18.87%)]\t\tLoss: 1.17018\n",
            "Training Progress: \tEpoch 33 [640/1692 (37.74%)]\t\tLoss: 1.24287\n",
            "Training Progress: \tEpoch 33 [960/1692 (56.60%)]\t\tLoss: 1.38578\n",
            "Training Progress: \tEpoch 33 [1280/1692 (75.47%)]\t\tLoss: 1.31289\n",
            "Training Progress: \tEpoch 33 [1600/1692 (94.34%)]\t\tLoss: 1.44314\n",
            "\tTrain loss: 0.03960, Accuracy: 704/1692 (41.61%)\n",
            "\tValidation loss: 0.00327, Accuracy: 135/423 (31.91%)\n",
            "\tTest loss: 0.00315, Accuracy: 128/443 (28.89%)\n",
            "\n",
            "Training Progress: \tEpoch 34 [0/1692 (0.00%)]\t\tLoss: 1.22351\n",
            "Training Progress: \tEpoch 34 [320/1692 (18.87%)]\t\tLoss: 1.28778\n",
            "Training Progress: \tEpoch 34 [640/1692 (37.74%)]\t\tLoss: 1.12981\n",
            "Training Progress: \tEpoch 34 [960/1692 (56.60%)]\t\tLoss: 1.24783\n",
            "Training Progress: \tEpoch 34 [1280/1692 (75.47%)]\t\tLoss: 1.29517\n",
            "Training Progress: \tEpoch 34 [1600/1692 (94.34%)]\t\tLoss: 1.42167\n",
            "\tTrain loss: 0.03970, Accuracy: 717/1692 (42.38%)\n",
            "\tValidation loss: 0.00333, Accuracy: 133/423 (31.44%)\n",
            "\tTest loss: 0.00317, Accuracy: 128/443 (28.89%)\n",
            "\n",
            "Training Progress: \tEpoch 35 [0/1692 (0.00%)]\t\tLoss: 1.15927\n",
            "Training Progress: \tEpoch 35 [320/1692 (18.87%)]\t\tLoss: 1.32742\n",
            "Training Progress: \tEpoch 35 [640/1692 (37.74%)]\t\tLoss: 1.19177\n",
            "Training Progress: \tEpoch 35 [960/1692 (56.60%)]\t\tLoss: 1.17901\n",
            "Training Progress: \tEpoch 35 [1280/1692 (75.47%)]\t\tLoss: 1.29370\n",
            "Training Progress: \tEpoch 35 [1600/1692 (94.34%)]\t\tLoss: 1.32148\n",
            "\tTrain loss: 0.03964, Accuracy: 719/1692 (42.49%)\n",
            "\tValidation loss: 0.00335, Accuracy: 135/423 (31.91%)\n",
            "\tTest loss: 0.00321, Accuracy: 123/443 (27.77%)\n",
            "\n",
            "Training Progress: \tEpoch 36 [0/1692 (0.00%)]\t\tLoss: 1.16005\n",
            "Training Progress: \tEpoch 36 [320/1692 (18.87%)]\t\tLoss: 1.14827\n",
            "Training Progress: \tEpoch 36 [640/1692 (37.74%)]\t\tLoss: 1.27741\n",
            "Training Progress: \tEpoch 36 [960/1692 (56.60%)]\t\tLoss: 1.28917\n",
            "Training Progress: \tEpoch 36 [1280/1692 (75.47%)]\t\tLoss: 1.32501\n",
            "Training Progress: \tEpoch 36 [1600/1692 (94.34%)]\t\tLoss: 1.18177\n",
            "\tTrain loss: 0.03798, Accuracy: 791/1692 (46.75%)\n",
            "\tValidation loss: 0.00326, Accuracy: 148/423 (34.99%)\n",
            "\tTest loss: 0.00310, Accuracy: 143/443 (32.28%)\n",
            "\n",
            "Training Progress: \tEpoch 37 [0/1692 (0.00%)]\t\tLoss: 1.16352\n",
            "Training Progress: \tEpoch 37 [320/1692 (18.87%)]\t\tLoss: 1.26911\n",
            "Training Progress: \tEpoch 37 [640/1692 (37.74%)]\t\tLoss: 1.13043\n",
            "Training Progress: \tEpoch 37 [960/1692 (56.60%)]\t\tLoss: 1.31893\n",
            "Training Progress: \tEpoch 37 [1280/1692 (75.47%)]\t\tLoss: 1.18145\n",
            "Training Progress: \tEpoch 37 [1600/1692 (94.34%)]\t\tLoss: 1.52156\n",
            "\tTrain loss: 0.03841, Accuracy: 774/1692 (45.74%)\n",
            "\tValidation loss: 0.00333, Accuracy: 128/423 (30.26%)\n",
            "\tTest loss: 0.00318, Accuracy: 142/443 (32.05%)\n",
            "\n",
            "Training Progress: \tEpoch 38 [0/1692 (0.00%)]\t\tLoss: 1.16635\n",
            "Training Progress: \tEpoch 38 [320/1692 (18.87%)]\t\tLoss: 1.09064\n",
            "Training Progress: \tEpoch 38 [640/1692 (37.74%)]\t\tLoss: 1.22351\n",
            "Training Progress: \tEpoch 38 [960/1692 (56.60%)]\t\tLoss: 1.20847\n",
            "Training Progress: \tEpoch 38 [1280/1692 (75.47%)]\t\tLoss: 1.33963\n",
            "Training Progress: \tEpoch 38 [1600/1692 (94.34%)]\t\tLoss: 1.34759\n",
            "\tTrain loss: 0.03643, Accuracy: 871/1692 (51.48%)\n",
            "\tValidation loss: 0.00325, Accuracy: 145/423 (34.28%)\n",
            "\tTest loss: 0.00314, Accuracy: 130/443 (29.35%)\n",
            "\n",
            "Training Progress: \tEpoch 39 [0/1692 (0.00%)]\t\tLoss: 1.25183\n",
            "Training Progress: \tEpoch 39 [320/1692 (18.87%)]\t\tLoss: 1.15341\n",
            "Training Progress: \tEpoch 39 [640/1692 (37.74%)]\t\tLoss: 1.13476\n",
            "Training Progress: \tEpoch 39 [960/1692 (56.60%)]\t\tLoss: 1.05951\n",
            "Training Progress: \tEpoch 39 [1280/1692 (75.47%)]\t\tLoss: 1.28442\n",
            "Training Progress: \tEpoch 39 [1600/1692 (94.34%)]\t\tLoss: 1.34924\n",
            "\tTrain loss: 0.03627, Accuracy: 850/1692 (50.24%)\n",
            "\tValidation loss: 0.00326, Accuracy: 148/423 (34.99%)\n",
            "\tTest loss: 0.00313, Accuracy: 139/443 (31.38%)\n",
            "\n",
            "Training Progress: \tEpoch 40 [0/1692 (0.00%)]\t\tLoss: 1.15930\n",
            "Training Progress: \tEpoch 40 [320/1692 (18.87%)]\t\tLoss: 1.12153\n",
            "Training Progress: \tEpoch 40 [640/1692 (37.74%)]\t\tLoss: 1.04038\n",
            "Training Progress: \tEpoch 40 [960/1692 (56.60%)]\t\tLoss: 1.24402\n",
            "Training Progress: \tEpoch 40 [1280/1692 (75.47%)]\t\tLoss: 1.37959\n",
            "Training Progress: \tEpoch 40 [1600/1692 (94.34%)]\t\tLoss: 1.27028\n",
            "\tTrain loss: 0.03643, Accuracy: 824/1692 (48.70%)\n",
            "\tValidation loss: 0.00331, Accuracy: 141/423 (33.33%)\n",
            "\tTest loss: 0.00322, Accuracy: 143/443 (32.28%)\n",
            "\n",
            "Training Progress: \tEpoch 41 [0/1692 (0.00%)]\t\tLoss: 1.11630\n",
            "Training Progress: \tEpoch 41 [320/1692 (18.87%)]\t\tLoss: 1.08251\n",
            "Training Progress: \tEpoch 41 [640/1692 (37.74%)]\t\tLoss: 1.08788\n",
            "Training Progress: \tEpoch 41 [960/1692 (56.60%)]\t\tLoss: 1.07580\n",
            "Training Progress: \tEpoch 41 [1280/1692 (75.47%)]\t\tLoss: 1.21984\n",
            "Training Progress: \tEpoch 41 [1600/1692 (94.34%)]\t\tLoss: 1.08069\n",
            "\tTrain loss: 0.03441, Accuracy: 917/1692 (54.20%)\n",
            "\tValidation loss: 0.00330, Accuracy: 138/423 (32.62%)\n",
            "\tTest loss: 0.00315, Accuracy: 148/443 (33.41%)\n",
            "\n",
            "Training Progress: \tEpoch 42 [0/1692 (0.00%)]\t\tLoss: 1.09054\n",
            "Training Progress: \tEpoch 42 [320/1692 (18.87%)]\t\tLoss: 1.03000\n",
            "Training Progress: \tEpoch 42 [640/1692 (37.74%)]\t\tLoss: 1.03250\n",
            "Training Progress: \tEpoch 42 [960/1692 (56.60%)]\t\tLoss: 1.05877\n",
            "Training Progress: \tEpoch 42 [1280/1692 (75.47%)]\t\tLoss: 1.25521\n",
            "Training Progress: \tEpoch 42 [1600/1692 (94.34%)]\t\tLoss: 1.23911\n",
            "\tTrain loss: 0.03342, Accuracy: 933/1692 (55.14%)\n",
            "\tValidation loss: 0.00335, Accuracy: 139/423 (32.86%)\n",
            "\tTest loss: 0.00319, Accuracy: 141/443 (31.83%)\n",
            "\n",
            "Training Progress: \tEpoch 43 [0/1692 (0.00%)]\t\tLoss: 1.20934\n",
            "Training Progress: \tEpoch 43 [320/1692 (18.87%)]\t\tLoss: 1.06720\n",
            "Training Progress: \tEpoch 43 [640/1692 (37.74%)]\t\tLoss: 1.08400\n",
            "Training Progress: \tEpoch 43 [960/1692 (56.60%)]\t\tLoss: 1.11948\n",
            "Training Progress: \tEpoch 43 [1280/1692 (75.47%)]\t\tLoss: 1.25787\n",
            "Training Progress: \tEpoch 43 [1600/1692 (94.34%)]\t\tLoss: 1.14375\n",
            "\tTrain loss: 0.03324, Accuracy: 953/1692 (56.32%)\n",
            "\tValidation loss: 0.00334, Accuracy: 136/423 (32.15%)\n",
            "\tTest loss: 0.00322, Accuracy: 140/443 (31.60%)\n",
            "\n",
            "Training Progress: \tEpoch 44 [0/1692 (0.00%)]\t\tLoss: 1.13179\n",
            "Training Progress: \tEpoch 44 [320/1692 (18.87%)]\t\tLoss: 1.01852\n",
            "Training Progress: \tEpoch 44 [640/1692 (37.74%)]\t\tLoss: 1.08983\n",
            "Training Progress: \tEpoch 44 [960/1692 (56.60%)]\t\tLoss: 1.10235\n",
            "Training Progress: \tEpoch 44 [1280/1692 (75.47%)]\t\tLoss: 1.19474\n",
            "Training Progress: \tEpoch 44 [1600/1692 (94.34%)]\t\tLoss: 1.17267\n",
            "\tTrain loss: 0.03130, Accuracy: 1065/1692 (62.94%)\n",
            "\tValidation loss: 0.00331, Accuracy: 124/423 (29.31%)\n",
            "\tTest loss: 0.00321, Accuracy: 139/443 (31.38%)\n",
            "\n",
            "Training Progress: \tEpoch 45 [0/1692 (0.00%)]\t\tLoss: 1.03980\n",
            "Training Progress: \tEpoch 45 [320/1692 (18.87%)]\t\tLoss: 0.98471\n",
            "Training Progress: \tEpoch 45 [640/1692 (37.74%)]\t\tLoss: 0.99286\n",
            "Training Progress: \tEpoch 45 [960/1692 (56.60%)]\t\tLoss: 0.98963\n",
            "Training Progress: \tEpoch 45 [1280/1692 (75.47%)]\t\tLoss: 1.21891\n",
            "Training Progress: \tEpoch 45 [1600/1692 (94.34%)]\t\tLoss: 1.24059\n",
            "\tTrain loss: 0.03079, Accuracy: 1047/1692 (61.88%)\n",
            "\tValidation loss: 0.00335, Accuracy: 148/423 (34.99%)\n",
            "\tTest loss: 0.00327, Accuracy: 131/443 (29.57%)\n",
            "\n",
            "Training Progress: \tEpoch 46 [0/1692 (0.00%)]\t\tLoss: 1.05552\n",
            "Training Progress: \tEpoch 46 [320/1692 (18.87%)]\t\tLoss: 0.99971\n",
            "Training Progress: \tEpoch 46 [640/1692 (37.74%)]\t\tLoss: 1.01067\n",
            "Training Progress: \tEpoch 46 [960/1692 (56.60%)]\t\tLoss: 0.95224\n",
            "Training Progress: \tEpoch 46 [1280/1692 (75.47%)]\t\tLoss: 1.20507\n",
            "Training Progress: \tEpoch 46 [1600/1692 (94.34%)]\t\tLoss: 1.01569\n",
            "\tTrain loss: 0.02867, Accuracy: 1150/1692 (67.97%)\n",
            "\tValidation loss: 0.00337, Accuracy: 136/423 (32.15%)\n",
            "\tTest loss: 0.00318, Accuracy: 139/443 (31.38%)\n",
            "\n",
            "Training Progress: \tEpoch 47 [0/1692 (0.00%)]\t\tLoss: 0.96920\n",
            "Training Progress: \tEpoch 47 [320/1692 (18.87%)]\t\tLoss: 1.26075\n",
            "Training Progress: \tEpoch 47 [640/1692 (37.74%)]\t\tLoss: 1.03254\n",
            "Training Progress: \tEpoch 47 [960/1692 (56.60%)]\t\tLoss: 1.01987\n",
            "Training Progress: \tEpoch 47 [1280/1692 (75.47%)]\t\tLoss: 0.97731\n",
            "Training Progress: \tEpoch 47 [1600/1692 (94.34%)]\t\tLoss: 1.08549\n",
            "\tTrain loss: 0.02711, Accuracy: 1180/1692 (69.74%)\n",
            "\tValidation loss: 0.00338, Accuracy: 139/423 (32.86%)\n",
            "\tTest loss: 0.00319, Accuracy: 142/443 (32.05%)\n",
            "\n",
            "Training Progress: \tEpoch 48 [0/1692 (0.00%)]\t\tLoss: 1.07977\n",
            "Training Progress: \tEpoch 48 [320/1692 (18.87%)]\t\tLoss: 1.00998\n",
            "Training Progress: \tEpoch 48 [640/1692 (37.74%)]\t\tLoss: 1.01541\n",
            "Training Progress: \tEpoch 48 [960/1692 (56.60%)]\t\tLoss: 0.99507\n",
            "Training Progress: \tEpoch 48 [1280/1692 (75.47%)]\t\tLoss: 1.14017\n",
            "Training Progress: \tEpoch 48 [1600/1692 (94.34%)]\t\tLoss: 1.01936\n",
            "\tTrain loss: 0.02687, Accuracy: 1182/1692 (69.86%)\n",
            "\tValidation loss: 0.00337, Accuracy: 140/423 (33.10%)\n",
            "\tTest loss: 0.00320, Accuracy: 139/443 (31.38%)\n",
            "\n",
            "Training Progress: \tEpoch 49 [0/1692 (0.00%)]\t\tLoss: 0.91998\n",
            "Training Progress: \tEpoch 49 [320/1692 (18.87%)]\t\tLoss: 1.08243\n",
            "Training Progress: \tEpoch 49 [640/1692 (37.74%)]\t\tLoss: 0.93275\n",
            "Training Progress: \tEpoch 49 [960/1692 (56.60%)]\t\tLoss: 0.95313\n",
            "Training Progress: \tEpoch 49 [1280/1692 (75.47%)]\t\tLoss: 1.13225\n",
            "Training Progress: \tEpoch 49 [1600/1692 (94.34%)]\t\tLoss: 1.05758\n",
            "\tTrain loss: 0.02606, Accuracy: 1205/1692 (71.22%)\n",
            "\tValidation loss: 0.00353, Accuracy: 120/423 (28.37%)\n",
            "\tTest loss: 0.00334, Accuracy: 123/443 (27.77%)\n",
            "\n",
            "Training Progress: \tEpoch 50 [0/1692 (0.00%)]\t\tLoss: 0.83629\n",
            "Training Progress: \tEpoch 50 [320/1692 (18.87%)]\t\tLoss: 0.88704\n",
            "Training Progress: \tEpoch 50 [640/1692 (37.74%)]\t\tLoss: 0.97148\n",
            "Training Progress: \tEpoch 50 [960/1692 (56.60%)]\t\tLoss: 0.87888\n",
            "Training Progress: \tEpoch 50 [1280/1692 (75.47%)]\t\tLoss: 0.95340\n",
            "Training Progress: \tEpoch 50 [1600/1692 (94.34%)]\t\tLoss: 0.97340\n",
            "\tTrain loss: 0.02534, Accuracy: 1214/1692 (71.75%)\n",
            "\tValidation loss: 0.00360, Accuracy: 133/423 (31.44%)\n",
            "\tTest loss: 0.00335, Accuracy: 140/443 (31.60%)\n",
            "\n",
            "Training Progress: \tEpoch 51 [0/1692 (0.00%)]\t\tLoss: 0.85496\n",
            "Training Progress: \tEpoch 51 [320/1692 (18.87%)]\t\tLoss: 0.95497\n",
            "Training Progress: \tEpoch 51 [640/1692 (37.74%)]\t\tLoss: 1.00332\n",
            "Training Progress: \tEpoch 51 [960/1692 (56.60%)]\t\tLoss: 0.98311\n",
            "Training Progress: \tEpoch 51 [1280/1692 (75.47%)]\t\tLoss: 1.06857\n",
            "Training Progress: \tEpoch 51 [1600/1692 (94.34%)]\t\tLoss: 1.23437\n",
            "\tTrain loss: 0.02209, Accuracy: 1310/1692 (77.42%)\n",
            "\tValidation loss: 0.00364, Accuracy: 132/423 (31.21%)\n",
            "\tTest loss: 0.00338, Accuracy: 133/443 (30.02%)\n",
            "\n",
            "Training Progress: \tEpoch 52 [0/1692 (0.00%)]\t\tLoss: 0.93110\n",
            "Training Progress: \tEpoch 52 [320/1692 (18.87%)]\t\tLoss: 0.89885\n",
            "Training Progress: \tEpoch 52 [640/1692 (37.74%)]\t\tLoss: 0.88948\n",
            "Training Progress: \tEpoch 52 [960/1692 (56.60%)]\t\tLoss: 0.79303\n",
            "Training Progress: \tEpoch 52 [1280/1692 (75.47%)]\t\tLoss: 1.07282\n",
            "Training Progress: \tEpoch 52 [1600/1692 (94.34%)]\t\tLoss: 1.10202\n",
            "\tTrain loss: 0.02130, Accuracy: 1354/1692 (80.02%)\n",
            "\tValidation loss: 0.00358, Accuracy: 131/423 (30.97%)\n",
            "\tTest loss: 0.00331, Accuracy: 143/443 (32.28%)\n",
            "\n",
            "Training Progress: \tEpoch 53 [0/1692 (0.00%)]\t\tLoss: 0.94389\n",
            "Training Progress: \tEpoch 53 [320/1692 (18.87%)]\t\tLoss: 1.01598\n",
            "Training Progress: \tEpoch 53 [640/1692 (37.74%)]\t\tLoss: 0.79419\n",
            "Training Progress: \tEpoch 53 [960/1692 (56.60%)]\t\tLoss: 0.78153\n",
            "Training Progress: \tEpoch 53 [1280/1692 (75.47%)]\t\tLoss: 0.97993\n",
            "Training Progress: \tEpoch 53 [1600/1692 (94.34%)]\t\tLoss: 0.78638\n",
            "\tTrain loss: 0.02115, Accuracy: 1358/1692 (80.26%)\n",
            "\tValidation loss: 0.00357, Accuracy: 135/423 (31.91%)\n",
            "\tTest loss: 0.00337, Accuracy: 132/443 (29.80%)\n",
            "\n",
            "Training Progress: \tEpoch 54 [0/1692 (0.00%)]\t\tLoss: 0.82084\n",
            "Training Progress: \tEpoch 54 [320/1692 (18.87%)]\t\tLoss: 0.72876\n",
            "Training Progress: \tEpoch 54 [640/1692 (37.74%)]\t\tLoss: 0.90852\n",
            "Training Progress: \tEpoch 54 [960/1692 (56.60%)]\t\tLoss: 0.65825\n",
            "Training Progress: \tEpoch 54 [1280/1692 (75.47%)]\t\tLoss: 0.75710\n",
            "Training Progress: \tEpoch 54 [1600/1692 (94.34%)]\t\tLoss: 1.17898\n",
            "\tTrain loss: 0.01913, Accuracy: 1402/1692 (82.86%)\n",
            "\tValidation loss: 0.00368, Accuracy: 142/423 (33.57%)\n",
            "\tTest loss: 0.00341, Accuracy: 139/443 (31.38%)\n",
            "\n",
            "Training Progress: \tEpoch 55 [0/1692 (0.00%)]\t\tLoss: 1.13820\n",
            "Training Progress: \tEpoch 55 [320/1692 (18.87%)]\t\tLoss: 0.67673\n",
            "Training Progress: \tEpoch 55 [640/1692 (37.74%)]\t\tLoss: 0.86309\n",
            "Training Progress: \tEpoch 55 [960/1692 (56.60%)]\t\tLoss: 0.97311\n",
            "Training Progress: \tEpoch 55 [1280/1692 (75.47%)]\t\tLoss: 0.90664\n",
            "Training Progress: \tEpoch 55 [1600/1692 (94.34%)]\t\tLoss: 0.77987\n",
            "\tTrain loss: 0.01837, Accuracy: 1403/1692 (82.92%)\n",
            "\tValidation loss: 0.00377, Accuracy: 125/423 (29.55%)\n",
            "\tTest loss: 0.00355, Accuracy: 136/443 (30.70%)\n",
            "\n",
            "Training Progress: \tEpoch 56 [0/1692 (0.00%)]\t\tLoss: 0.88833\n",
            "Training Progress: \tEpoch 56 [320/1692 (18.87%)]\t\tLoss: 0.91187\n",
            "Training Progress: \tEpoch 56 [640/1692 (37.74%)]\t\tLoss: 0.65105\n",
            "Training Progress: \tEpoch 56 [960/1692 (56.60%)]\t\tLoss: 0.77820\n",
            "Training Progress: \tEpoch 56 [1280/1692 (75.47%)]\t\tLoss: 0.83752\n",
            "Training Progress: \tEpoch 56 [1600/1692 (94.34%)]\t\tLoss: 1.01840\n",
            "\tTrain loss: 0.01765, Accuracy: 1426/1692 (84.28%)\n",
            "\tValidation loss: 0.00374, Accuracy: 137/423 (32.39%)\n",
            "\tTest loss: 0.00343, Accuracy: 142/443 (32.05%)\n",
            "\n",
            "Training Progress: \tEpoch 57 [0/1692 (0.00%)]\t\tLoss: 0.78965\n",
            "Training Progress: \tEpoch 57 [320/1692 (18.87%)]\t\tLoss: 0.69097\n",
            "Training Progress: \tEpoch 57 [640/1692 (37.74%)]\t\tLoss: 0.63609\n",
            "Training Progress: \tEpoch 57 [960/1692 (56.60%)]\t\tLoss: 0.82246\n",
            "Training Progress: \tEpoch 57 [1280/1692 (75.47%)]\t\tLoss: 0.75949\n",
            "Training Progress: \tEpoch 57 [1600/1692 (94.34%)]\t\tLoss: 0.88110\n",
            "\tTrain loss: 0.01619, Accuracy: 1444/1692 (85.34%)\n",
            "\tValidation loss: 0.00381, Accuracy: 135/423 (31.91%)\n",
            "\tTest loss: 0.00354, Accuracy: 137/443 (30.93%)\n",
            "\n",
            "Training Progress: \tEpoch 58 [0/1692 (0.00%)]\t\tLoss: 0.84103\n",
            "Training Progress: \tEpoch 58 [320/1692 (18.87%)]\t\tLoss: 1.00469\n",
            "Training Progress: \tEpoch 58 [640/1692 (37.74%)]\t\tLoss: 0.77589\n",
            "Training Progress: \tEpoch 58 [960/1692 (56.60%)]\t\tLoss: 0.53500\n",
            "Training Progress: \tEpoch 58 [1280/1692 (75.47%)]\t\tLoss: 0.80491\n",
            "Training Progress: \tEpoch 58 [1600/1692 (94.34%)]\t\tLoss: 0.89253\n",
            "\tTrain loss: 0.01441, Accuracy: 1503/1692 (88.83%)\n",
            "\tValidation loss: 0.00394, Accuracy: 134/423 (31.68%)\n",
            "\tTest loss: 0.00370, Accuracy: 126/443 (28.44%)\n",
            "\n",
            "Training Progress: \tEpoch 59 [0/1692 (0.00%)]\t\tLoss: 0.83487\n",
            "Training Progress: \tEpoch 59 [320/1692 (18.87%)]\t\tLoss: 0.73757\n",
            "Training Progress: \tEpoch 59 [640/1692 (37.74%)]\t\tLoss: 1.11589\n",
            "Training Progress: \tEpoch 59 [960/1692 (56.60%)]\t\tLoss: 0.80394\n",
            "Training Progress: \tEpoch 59 [1280/1692 (75.47%)]\t\tLoss: 0.63974\n",
            "Training Progress: \tEpoch 59 [1600/1692 (94.34%)]\t\tLoss: 0.64238\n",
            "\tTrain loss: 0.01419, Accuracy: 1486/1692 (87.83%)\n",
            "\tValidation loss: 0.00395, Accuracy: 129/423 (30.50%)\n",
            "\tTest loss: 0.00368, Accuracy: 131/443 (29.57%)\n",
            "\n",
            "Training Progress: \tEpoch 60 [0/1692 (0.00%)]\t\tLoss: 0.76474\n",
            "Training Progress: \tEpoch 60 [320/1692 (18.87%)]\t\tLoss: 0.84152\n",
            "Training Progress: \tEpoch 60 [640/1692 (37.74%)]\t\tLoss: 0.80568\n",
            "Training Progress: \tEpoch 60 [960/1692 (56.60%)]\t\tLoss: 0.74855\n",
            "Training Progress: \tEpoch 60 [1280/1692 (75.47%)]\t\tLoss: 0.78300\n",
            "Training Progress: \tEpoch 60 [1600/1692 (94.34%)]\t\tLoss: 0.85941\n",
            "\tTrain loss: 0.01352, Accuracy: 1504/1692 (88.89%)\n",
            "\tValidation loss: 0.00400, Accuracy: 123/423 (29.08%)\n",
            "\tTest loss: 0.00369, Accuracy: 137/443 (30.93%)\n",
            "\n",
            "Training Progress: \tEpoch 61 [0/1692 (0.00%)]\t\tLoss: 0.68445\n",
            "Training Progress: \tEpoch 61 [320/1692 (18.87%)]\t\tLoss: 0.65616\n",
            "Training Progress: \tEpoch 61 [640/1692 (37.74%)]\t\tLoss: 0.71058\n",
            "Training Progress: \tEpoch 61 [960/1692 (56.60%)]\t\tLoss: 0.80862\n",
            "Training Progress: \tEpoch 61 [1280/1692 (75.47%)]\t\tLoss: 0.63934\n",
            "Training Progress: \tEpoch 61 [1600/1692 (94.34%)]\t\tLoss: 0.68445\n",
            "\tTrain loss: 0.01157, Accuracy: 1559/1692 (92.14%)\n",
            "\tValidation loss: 0.00400, Accuracy: 138/423 (32.62%)\n",
            "\tTest loss: 0.00370, Accuracy: 146/443 (32.96%)\n",
            "\n",
            "Training Progress: \tEpoch 62 [0/1692 (0.00%)]\t\tLoss: 0.57724\n",
            "Training Progress: \tEpoch 62 [320/1692 (18.87%)]\t\tLoss: 0.60858\n",
            "Training Progress: \tEpoch 62 [640/1692 (37.74%)]\t\tLoss: 0.68613\n",
            "Training Progress: \tEpoch 62 [960/1692 (56.60%)]\t\tLoss: 0.64052\n",
            "Training Progress: \tEpoch 62 [1280/1692 (75.47%)]\t\tLoss: 0.58380\n",
            "Training Progress: \tEpoch 62 [1600/1692 (94.34%)]\t\tLoss: 0.63288\n",
            "\tTrain loss: 0.01081, Accuracy: 1555/1692 (91.90%)\n",
            "\tValidation loss: 0.00408, Accuracy: 126/423 (29.79%)\n",
            "\tTest loss: 0.00375, Accuracy: 135/443 (30.47%)\n",
            "\n",
            "Training Progress: \tEpoch 63 [0/1692 (0.00%)]\t\tLoss: 0.60068\n",
            "Training Progress: \tEpoch 63 [320/1692 (18.87%)]\t\tLoss: 0.51583\n",
            "Training Progress: \tEpoch 63 [640/1692 (37.74%)]\t\tLoss: 0.74977\n",
            "Training Progress: \tEpoch 63 [960/1692 (56.60%)]\t\tLoss: 0.51433\n",
            "Training Progress: \tEpoch 63 [1280/1692 (75.47%)]\t\tLoss: 0.60897\n",
            "Training Progress: \tEpoch 63 [1600/1692 (94.34%)]\t\tLoss: 0.57591\n",
            "\tTrain loss: 0.00911, Accuracy: 1583/1692 (93.56%)\n",
            "\tValidation loss: 0.00417, Accuracy: 129/423 (30.50%)\n",
            "\tTest loss: 0.00381, Accuracy: 133/443 (30.02%)\n",
            "\n",
            "Training Progress: \tEpoch 64 [0/1692 (0.00%)]\t\tLoss: 0.47777\n",
            "Training Progress: \tEpoch 64 [320/1692 (18.87%)]\t\tLoss: 0.61341\n",
            "Training Progress: \tEpoch 64 [640/1692 (37.74%)]\t\tLoss: 0.73681\n",
            "Training Progress: \tEpoch 64 [960/1692 (56.60%)]\t\tLoss: 0.59304\n",
            "Training Progress: \tEpoch 64 [1280/1692 (75.47%)]\t\tLoss: 0.69371\n",
            "Training Progress: \tEpoch 64 [1600/1692 (94.34%)]\t\tLoss: 0.68531\n",
            "\tTrain loss: 0.00976, Accuracy: 1567/1692 (92.61%)\n",
            "\tValidation loss: 0.00431, Accuracy: 125/423 (29.55%)\n",
            "\tTest loss: 0.00393, Accuracy: 122/443 (27.54%)\n",
            "\n",
            "Training Progress: \tEpoch 65 [0/1692 (0.00%)]\t\tLoss: 0.72643\n",
            "Training Progress: \tEpoch 65 [320/1692 (18.87%)]\t\tLoss: 0.56445\n",
            "Training Progress: \tEpoch 65 [640/1692 (37.74%)]\t\tLoss: 0.44735\n",
            "Training Progress: \tEpoch 65 [960/1692 (56.60%)]\t\tLoss: 0.51158\n",
            "Training Progress: \tEpoch 65 [1280/1692 (75.47%)]\t\tLoss: 0.72366\n",
            "Training Progress: \tEpoch 65 [1600/1692 (94.34%)]\t\tLoss: 0.69782\n",
            "\tTrain loss: 0.00669, Accuracy: 1622/1692 (95.86%)\n",
            "\tValidation loss: 0.00433, Accuracy: 133/423 (31.44%)\n",
            "\tTest loss: 0.00395, Accuracy: 139/443 (31.38%)\n",
            "\n",
            "Training Progress: \tEpoch 66 [0/1692 (0.00%)]\t\tLoss: 0.57500\n",
            "Training Progress: \tEpoch 66 [320/1692 (18.87%)]\t\tLoss: 0.40274\n",
            "Training Progress: \tEpoch 66 [640/1692 (37.74%)]\t\tLoss: 0.27384\n",
            "Training Progress: \tEpoch 66 [960/1692 (56.60%)]\t\tLoss: 0.48326\n",
            "Training Progress: \tEpoch 66 [1280/1692 (75.47%)]\t\tLoss: 0.33651\n",
            "Training Progress: \tEpoch 66 [1600/1692 (94.34%)]\t\tLoss: 0.45335\n",
            "\tTrain loss: 0.00709, Accuracy: 1615/1692 (95.45%)\n",
            "\tValidation loss: 0.00439, Accuracy: 127/423 (30.02%)\n",
            "\tTest loss: 0.00397, Accuracy: 133/443 (30.02%)\n",
            "\n",
            "Training Progress: \tEpoch 67 [0/1692 (0.00%)]\t\tLoss: 0.65269\n",
            "Training Progress: \tEpoch 67 [320/1692 (18.87%)]\t\tLoss: 0.51747\n",
            "Training Progress: \tEpoch 67 [640/1692 (37.74%)]\t\tLoss: 0.52416\n",
            "Training Progress: \tEpoch 67 [960/1692 (56.60%)]\t\tLoss: 0.44005\n",
            "Training Progress: \tEpoch 67 [1280/1692 (75.47%)]\t\tLoss: 0.73590\n",
            "Training Progress: \tEpoch 67 [1600/1692 (94.34%)]\t\tLoss: 0.51165\n",
            "\tTrain loss: 0.00580, Accuracy: 1640/1692 (96.93%)\n",
            "\tValidation loss: 0.00455, Accuracy: 133/423 (31.44%)\n",
            "\tTest loss: 0.00401, Accuracy: 144/443 (32.51%)\n",
            "\n",
            "Training Progress: \tEpoch 68 [0/1692 (0.00%)]\t\tLoss: 0.81408\n",
            "Training Progress: \tEpoch 68 [320/1692 (18.87%)]\t\tLoss: 0.48427\n",
            "Training Progress: \tEpoch 68 [640/1692 (37.74%)]\t\tLoss: 0.48652\n",
            "Training Progress: \tEpoch 68 [960/1692 (56.60%)]\t\tLoss: 0.36176\n",
            "Training Progress: \tEpoch 68 [1280/1692 (75.47%)]\t\tLoss: 0.62877\n",
            "Training Progress: \tEpoch 68 [1600/1692 (94.34%)]\t\tLoss: 0.52553\n",
            "\tTrain loss: 0.00562, Accuracy: 1641/1692 (96.99%)\n",
            "\tValidation loss: 0.00467, Accuracy: 125/423 (29.55%)\n",
            "\tTest loss: 0.00400, Accuracy: 141/443 (31.83%)\n",
            "\n",
            "Training Progress: \tEpoch 69 [0/1692 (0.00%)]\t\tLoss: 0.76177\n",
            "Training Progress: \tEpoch 69 [320/1692 (18.87%)]\t\tLoss: 0.47252\n",
            "Training Progress: \tEpoch 69 [640/1692 (37.74%)]\t\tLoss: 0.52282\n",
            "Training Progress: \tEpoch 69 [960/1692 (56.60%)]\t\tLoss: 0.40597\n",
            "Training Progress: \tEpoch 69 [1280/1692 (75.47%)]\t\tLoss: 0.55937\n",
            "Training Progress: \tEpoch 69 [1600/1692 (94.34%)]\t\tLoss: 0.46283\n",
            "\tTrain loss: 0.00518, Accuracy: 1654/1692 (97.75%)\n",
            "\tValidation loss: 0.00451, Accuracy: 126/423 (29.79%)\n",
            "\tTest loss: 0.00408, Accuracy: 136/443 (30.70%)\n",
            "\n",
            "Training Progress: \tEpoch 70 [0/1692 (0.00%)]\t\tLoss: 0.51609\n",
            "Training Progress: \tEpoch 70 [320/1692 (18.87%)]\t\tLoss: 0.28142\n",
            "Training Progress: \tEpoch 70 [640/1692 (37.74%)]\t\tLoss: 0.65336\n",
            "Training Progress: \tEpoch 70 [960/1692 (56.60%)]\t\tLoss: 0.33289\n",
            "Training Progress: \tEpoch 70 [1280/1692 (75.47%)]\t\tLoss: 0.43694\n",
            "Training Progress: \tEpoch 70 [1600/1692 (94.34%)]\t\tLoss: 0.28879\n",
            "\tTrain loss: 0.00455, Accuracy: 1660/1692 (98.11%)\n",
            "\tValidation loss: 0.00454, Accuracy: 124/423 (29.31%)\n",
            "\tTest loss: 0.00417, Accuracy: 128/443 (28.89%)\n",
            "\n",
            "Training Progress: \tEpoch 71 [0/1692 (0.00%)]\t\tLoss: 0.66795\n",
            "Training Progress: \tEpoch 71 [320/1692 (18.87%)]\t\tLoss: 0.55360\n",
            "Training Progress: \tEpoch 71 [640/1692 (37.74%)]\t\tLoss: 0.52402\n",
            "Training Progress: \tEpoch 71 [960/1692 (56.60%)]\t\tLoss: 0.34816\n",
            "Training Progress: \tEpoch 71 [1280/1692 (75.47%)]\t\tLoss: 0.68958\n",
            "Training Progress: \tEpoch 71 [1600/1692 (94.34%)]\t\tLoss: 0.41327\n",
            "\tTrain loss: 0.00401, Accuracy: 1658/1692 (97.99%)\n",
            "\tValidation loss: 0.00487, Accuracy: 130/423 (30.73%)\n",
            "\tTest loss: 0.00437, Accuracy: 131/443 (29.57%)\n",
            "\n",
            "Training Progress: \tEpoch 72 [0/1692 (0.00%)]\t\tLoss: 0.62663\n",
            "Training Progress: \tEpoch 72 [320/1692 (18.87%)]\t\tLoss: 0.33155\n",
            "Training Progress: \tEpoch 72 [640/1692 (37.74%)]\t\tLoss: 0.60881\n",
            "Training Progress: \tEpoch 72 [960/1692 (56.60%)]\t\tLoss: 0.60959\n",
            "Training Progress: \tEpoch 72 [1280/1692 (75.47%)]\t\tLoss: 0.49635\n",
            "Training Progress: \tEpoch 72 [1600/1692 (94.34%)]\t\tLoss: 0.65062\n",
            "\tTrain loss: 0.00382, Accuracy: 1655/1692 (97.81%)\n",
            "\tValidation loss: 0.00465, Accuracy: 139/423 (32.86%)\n",
            "\tTest loss: 0.00432, Accuracy: 144/443 (32.51%)\n",
            "\n",
            "Training Progress: \tEpoch 73 [0/1692 (0.00%)]\t\tLoss: 0.66042\n",
            "Training Progress: \tEpoch 73 [320/1692 (18.87%)]\t\tLoss: 0.50193\n",
            "Training Progress: \tEpoch 73 [640/1692 (37.74%)]\t\tLoss: 0.40175\n",
            "Training Progress: \tEpoch 73 [960/1692 (56.60%)]\t\tLoss: 0.46845\n",
            "Training Progress: \tEpoch 73 [1280/1692 (75.47%)]\t\tLoss: 0.50377\n",
            "Training Progress: \tEpoch 73 [1600/1692 (94.34%)]\t\tLoss: 0.59267\n",
            "\tTrain loss: 0.00345, Accuracy: 1666/1692 (98.46%)\n",
            "\tValidation loss: 0.00473, Accuracy: 123/423 (29.08%)\n",
            "\tTest loss: 0.00414, Accuracy: 146/443 (32.96%)\n",
            "\n",
            "Training Progress: \tEpoch 74 [0/1692 (0.00%)]\t\tLoss: 0.35983\n",
            "Training Progress: \tEpoch 74 [320/1692 (18.87%)]\t\tLoss: 0.38179\n",
            "Training Progress: \tEpoch 74 [640/1692 (37.74%)]\t\tLoss: 0.43859\n",
            "Training Progress: \tEpoch 74 [960/1692 (56.60%)]\t\tLoss: 0.38072\n",
            "Training Progress: \tEpoch 74 [1280/1692 (75.47%)]\t\tLoss: 0.42640\n",
            "Training Progress: \tEpoch 74 [1600/1692 (94.34%)]\t\tLoss: 0.37695\n",
            "\tTrain loss: 0.00377, Accuracy: 1661/1692 (98.17%)\n",
            "\tValidation loss: 0.00494, Accuracy: 123/423 (29.08%)\n",
            "\tTest loss: 0.00426, Accuracy: 136/443 (30.70%)\n",
            "\n",
            "Training Progress: \tEpoch 75 [0/1692 (0.00%)]\t\tLoss: 0.46858\n",
            "Training Progress: \tEpoch 75 [320/1692 (18.87%)]\t\tLoss: 0.55574\n",
            "Training Progress: \tEpoch 75 [640/1692 (37.74%)]\t\tLoss: 0.30649\n",
            "Training Progress: \tEpoch 75 [960/1692 (56.60%)]\t\tLoss: 0.63514\n",
            "Training Progress: \tEpoch 75 [1280/1692 (75.47%)]\t\tLoss: 0.39707\n",
            "Training Progress: \tEpoch 75 [1600/1692 (94.34%)]\t\tLoss: 0.52766\n",
            "\tTrain loss: 0.00314, Accuracy: 1670/1692 (98.70%)\n",
            "\tValidation loss: 0.00492, Accuracy: 131/423 (30.97%)\n",
            "\tTest loss: 0.00436, Accuracy: 142/443 (32.05%)\n",
            "\n",
            "Training Progress: \tEpoch 76 [0/1692 (0.00%)]\t\tLoss: 0.48642\n",
            "Training Progress: \tEpoch 76 [320/1692 (18.87%)]\t\tLoss: 0.40585\n",
            "Training Progress: \tEpoch 76 [640/1692 (37.74%)]\t\tLoss: 0.42602\n",
            "Training Progress: \tEpoch 76 [960/1692 (56.60%)]\t\tLoss: 0.38878\n",
            "Training Progress: \tEpoch 76 [1280/1692 (75.47%)]\t\tLoss: 0.36058\n",
            "Training Progress: \tEpoch 76 [1600/1692 (94.34%)]\t\tLoss: 0.45751\n",
            "\tTrain loss: 0.00284, Accuracy: 1665/1692 (98.40%)\n",
            "\tValidation loss: 0.00501, Accuracy: 130/423 (30.73%)\n",
            "\tTest loss: 0.00442, Accuracy: 143/443 (32.28%)\n",
            "\n",
            "Training Progress: \tEpoch 77 [0/1692 (0.00%)]\t\tLoss: 0.41087\n",
            "Training Progress: \tEpoch 77 [320/1692 (18.87%)]\t\tLoss: 0.32824\n",
            "Training Progress: \tEpoch 77 [640/1692 (37.74%)]\t\tLoss: 0.38425\n",
            "Training Progress: \tEpoch 77 [960/1692 (56.60%)]\t\tLoss: 0.21071\n",
            "Training Progress: \tEpoch 77 [1280/1692 (75.47%)]\t\tLoss: 0.29292\n",
            "Training Progress: \tEpoch 77 [1600/1692 (94.34%)]\t\tLoss: 0.35700\n",
            "\tTrain loss: 0.00252, Accuracy: 1679/1692 (99.23%)\n",
            "\tValidation loss: 0.00495, Accuracy: 134/423 (31.68%)\n",
            "\tTest loss: 0.00428, Accuracy: 151/443 (34.09%)\n",
            "\n",
            "Training Progress: \tEpoch 78 [0/1692 (0.00%)]\t\tLoss: 0.55073\n",
            "Training Progress: \tEpoch 78 [320/1692 (18.87%)]\t\tLoss: 0.36127\n",
            "Training Progress: \tEpoch 78 [640/1692 (37.74%)]\t\tLoss: 0.38372\n",
            "Training Progress: \tEpoch 78 [960/1692 (56.60%)]\t\tLoss: 0.39755\n",
            "Training Progress: \tEpoch 78 [1280/1692 (75.47%)]\t\tLoss: 0.25869\n",
            "Training Progress: \tEpoch 78 [1600/1692 (94.34%)]\t\tLoss: 0.36752\n",
            "\tTrain loss: 0.00209, Accuracy: 1683/1692 (99.47%)\n",
            "\tValidation loss: 0.00498, Accuracy: 127/423 (30.02%)\n",
            "\tTest loss: 0.00431, Accuracy: 147/443 (33.18%)\n",
            "\n",
            "Training Progress: \tEpoch 79 [0/1692 (0.00%)]\t\tLoss: 0.48475\n",
            "Training Progress: \tEpoch 79 [320/1692 (18.87%)]\t\tLoss: 0.37944\n",
            "Training Progress: \tEpoch 79 [640/1692 (37.74%)]\t\tLoss: 0.37911\n",
            "Training Progress: \tEpoch 79 [960/1692 (56.60%)]\t\tLoss: 0.52448\n",
            "Training Progress: \tEpoch 79 [1280/1692 (75.47%)]\t\tLoss: 0.20929\n",
            "Training Progress: \tEpoch 79 [1600/1692 (94.34%)]\t\tLoss: 0.41961\n",
            "\tTrain loss: 0.00224, Accuracy: 1681/1692 (99.35%)\n",
            "\tValidation loss: 0.00509, Accuracy: 131/423 (30.97%)\n",
            "\tTest loss: 0.00439, Accuracy: 142/443 (32.05%)\n",
            "\n",
            "Training Progress: \tEpoch 80 [0/1692 (0.00%)]\t\tLoss: 0.44326\n",
            "Training Progress: \tEpoch 80 [320/1692 (18.87%)]\t\tLoss: 0.39711\n",
            "Training Progress: \tEpoch 80 [640/1692 (37.74%)]\t\tLoss: 0.33148\n",
            "Training Progress: \tEpoch 80 [960/1692 (56.60%)]\t\tLoss: 0.44391\n",
            "Training Progress: \tEpoch 80 [1280/1692 (75.47%)]\t\tLoss: 0.25760\n",
            "Training Progress: \tEpoch 80 [1600/1692 (94.34%)]\t\tLoss: 0.44634\n",
            "\tTrain loss: 0.00185, Accuracy: 1683/1692 (99.47%)\n",
            "\tValidation loss: 0.00519, Accuracy: 134/423 (31.68%)\n",
            "\tTest loss: 0.00449, Accuracy: 139/443 (31.38%)\n",
            "\n",
            "Training Progress: \tEpoch 81 [0/1692 (0.00%)]\t\tLoss: 0.49008\n",
            "Training Progress: \tEpoch 81 [320/1692 (18.87%)]\t\tLoss: 0.39195\n",
            "Training Progress: \tEpoch 81 [640/1692 (37.74%)]\t\tLoss: 0.15154\n",
            "Training Progress: \tEpoch 81 [960/1692 (56.60%)]\t\tLoss: 0.28103\n",
            "Training Progress: \tEpoch 81 [1280/1692 (75.47%)]\t\tLoss: 0.34319\n",
            "Training Progress: \tEpoch 81 [1600/1692 (94.34%)]\t\tLoss: 0.42826\n",
            "\tTrain loss: 0.00207, Accuracy: 1679/1692 (99.23%)\n",
            "\tValidation loss: 0.00512, Accuracy: 146/423 (34.52%)\n",
            "\tTest loss: 0.00455, Accuracy: 135/443 (30.47%)\n",
            "\n",
            "Training Progress: \tEpoch 82 [0/1692 (0.00%)]\t\tLoss: 0.34933\n",
            "Training Progress: \tEpoch 82 [320/1692 (18.87%)]\t\tLoss: 0.43713\n",
            "Training Progress: \tEpoch 82 [640/1692 (37.74%)]\t\tLoss: 0.43017\n",
            "Training Progress: \tEpoch 82 [960/1692 (56.60%)]\t\tLoss: 0.26161\n",
            "Training Progress: \tEpoch 82 [1280/1692 (75.47%)]\t\tLoss: 0.11466\n",
            "Training Progress: \tEpoch 82 [1600/1692 (94.34%)]\t\tLoss: 0.32387\n",
            "\tTrain loss: 0.00147, Accuracy: 1682/1692 (99.41%)\n",
            "\tValidation loss: 0.00536, Accuracy: 131/423 (30.97%)\n",
            "\tTest loss: 0.00475, Accuracy: 137/443 (30.93%)\n",
            "\n",
            "Training Progress: \tEpoch 83 [0/1692 (0.00%)]\t\tLoss: 0.60284\n",
            "Training Progress: \tEpoch 83 [320/1692 (18.87%)]\t\tLoss: 0.24103\n",
            "Training Progress: \tEpoch 83 [640/1692 (37.74%)]\t\tLoss: 0.40423\n",
            "Training Progress: \tEpoch 83 [960/1692 (56.60%)]\t\tLoss: 0.28975\n",
            "Training Progress: \tEpoch 83 [1280/1692 (75.47%)]\t\tLoss: 0.37499\n",
            "Training Progress: \tEpoch 83 [1600/1692 (94.34%)]\t\tLoss: 0.43950\n",
            "\tTrain loss: 0.00173, Accuracy: 1682/1692 (99.41%)\n",
            "\tValidation loss: 0.00535, Accuracy: 121/423 (28.61%)\n",
            "\tTest loss: 0.00473, Accuracy: 130/443 (29.35%)\n",
            "\n",
            "Training Progress: \tEpoch 84 [0/1692 (0.00%)]\t\tLoss: 0.41373\n",
            "Training Progress: \tEpoch 84 [320/1692 (18.87%)]\t\tLoss: 0.26141\n",
            "Training Progress: \tEpoch 84 [640/1692 (37.74%)]\t\tLoss: 0.35820\n",
            "Training Progress: \tEpoch 84 [960/1692 (56.60%)]\t\tLoss: 0.31617\n",
            "Training Progress: \tEpoch 84 [1280/1692 (75.47%)]\t\tLoss: 0.39100\n",
            "Training Progress: \tEpoch 84 [1600/1692 (94.34%)]\t\tLoss: 0.18629\n",
            "\tTrain loss: 0.00135, Accuracy: 1687/1692 (99.70%)\n",
            "\tValidation loss: 0.00534, Accuracy: 133/423 (31.44%)\n",
            "\tTest loss: 0.00459, Accuracy: 141/443 (31.83%)\n",
            "\n",
            "Training Progress: \tEpoch 85 [0/1692 (0.00%)]\t\tLoss: 0.34721\n",
            "Training Progress: \tEpoch 85 [320/1692 (18.87%)]\t\tLoss: 0.45509\n",
            "Training Progress: \tEpoch 85 [640/1692 (37.74%)]\t\tLoss: 0.58138\n",
            "Training Progress: \tEpoch 85 [960/1692 (56.60%)]\t\tLoss: 0.17085\n",
            "Training Progress: \tEpoch 85 [1280/1692 (75.47%)]\t\tLoss: 0.21587\n",
            "Training Progress: \tEpoch 85 [1600/1692 (94.34%)]\t\tLoss: 0.42253\n",
            "\tTrain loss: 0.00130, Accuracy: 1686/1692 (99.65%)\n",
            "\tValidation loss: 0.00547, Accuracy: 130/423 (30.73%)\n",
            "\tTest loss: 0.00480, Accuracy: 136/443 (30.70%)\n",
            "\n",
            "Training Progress: \tEpoch 86 [0/1692 (0.00%)]\t\tLoss: 0.17180\n",
            "Training Progress: \tEpoch 86 [320/1692 (18.87%)]\t\tLoss: 0.30057\n",
            "Training Progress: \tEpoch 86 [640/1692 (37.74%)]\t\tLoss: 0.20960\n",
            "Training Progress: \tEpoch 86 [960/1692 (56.60%)]\t\tLoss: 0.45820\n",
            "Training Progress: \tEpoch 86 [1280/1692 (75.47%)]\t\tLoss: 0.31538\n",
            "Training Progress: \tEpoch 86 [1600/1692 (94.34%)]\t\tLoss: 0.12478\n",
            "\tTrain loss: 0.00130, Accuracy: 1684/1692 (99.53%)\n",
            "\tValidation loss: 0.00552, Accuracy: 124/423 (29.31%)\n",
            "\tTest loss: 0.00485, Accuracy: 145/443 (32.73%)\n",
            "\n",
            "Training Progress: \tEpoch 87 [0/1692 (0.00%)]\t\tLoss: 0.59301\n",
            "Training Progress: \tEpoch 87 [320/1692 (18.87%)]\t\tLoss: 0.24351\n",
            "Training Progress: \tEpoch 87 [640/1692 (37.74%)]\t\tLoss: 0.27677\n",
            "Training Progress: \tEpoch 87 [960/1692 (56.60%)]\t\tLoss: 0.27273\n",
            "Training Progress: \tEpoch 87 [1280/1692 (75.47%)]\t\tLoss: 0.09533\n",
            "Training Progress: \tEpoch 87 [1600/1692 (94.34%)]\t\tLoss: 0.21231\n",
            "\tTrain loss: 0.00094, Accuracy: 1685/1692 (99.59%)\n",
            "\tValidation loss: 0.00563, Accuracy: 127/423 (30.02%)\n",
            "\tTest loss: 0.00492, Accuracy: 135/443 (30.47%)\n",
            "\n",
            "Training Progress: \tEpoch 88 [0/1692 (0.00%)]\t\tLoss: 0.33882\n",
            "Training Progress: \tEpoch 88 [320/1692 (18.87%)]\t\tLoss: 0.30000\n",
            "Training Progress: \tEpoch 88 [640/1692 (37.74%)]\t\tLoss: 0.26888\n",
            "Training Progress: \tEpoch 88 [960/1692 (56.60%)]\t\tLoss: 0.22239\n",
            "Training Progress: \tEpoch 88 [1280/1692 (75.47%)]\t\tLoss: 0.35833\n",
            "Training Progress: \tEpoch 88 [1600/1692 (94.34%)]\t\tLoss: 0.14279\n",
            "\tTrain loss: 0.00109, Accuracy: 1688/1692 (99.76%)\n",
            "\tValidation loss: 0.00551, Accuracy: 124/423 (29.31%)\n",
            "\tTest loss: 0.00486, Accuracy: 130/443 (29.35%)\n",
            "\n",
            "Training Progress: \tEpoch 89 [0/1692 (0.00%)]\t\tLoss: 0.19535\n",
            "Training Progress: \tEpoch 89 [320/1692 (18.87%)]\t\tLoss: 0.23912\n",
            "Training Progress: \tEpoch 89 [640/1692 (37.74%)]\t\tLoss: 0.33047\n",
            "Training Progress: \tEpoch 89 [960/1692 (56.60%)]\t\tLoss: 0.45016\n",
            "Training Progress: \tEpoch 89 [1280/1692 (75.47%)]\t\tLoss: 0.21937\n",
            "Training Progress: \tEpoch 89 [1600/1692 (94.34%)]\t\tLoss: 0.07273\n",
            "\tTrain loss: 0.00102, Accuracy: 1684/1692 (99.53%)\n",
            "\tValidation loss: 0.00564, Accuracy: 130/423 (30.73%)\n",
            "\tTest loss: 0.00492, Accuracy: 137/443 (30.93%)\n",
            "\n",
            "Training Progress: \tEpoch 90 [0/1692 (0.00%)]\t\tLoss: 0.34091\n",
            "Training Progress: \tEpoch 90 [320/1692 (18.87%)]\t\tLoss: 0.13342\n",
            "Training Progress: \tEpoch 90 [640/1692 (37.74%)]\t\tLoss: 0.30124\n",
            "Training Progress: \tEpoch 90 [960/1692 (56.60%)]\t\tLoss: 0.13469\n",
            "Training Progress: \tEpoch 90 [1280/1692 (75.47%)]\t\tLoss: 0.27429\n",
            "Training Progress: \tEpoch 90 [1600/1692 (94.34%)]\t\tLoss: 0.30767\n",
            "\tTrain loss: 0.00086, Accuracy: 1689/1692 (99.82%)\n",
            "\tValidation loss: 0.00576, Accuracy: 122/423 (28.84%)\n",
            "\tTest loss: 0.00503, Accuracy: 138/443 (31.15%)\n",
            "\n",
            "Training Progress: \tEpoch 91 [0/1692 (0.00%)]\t\tLoss: 0.44256\n",
            "Training Progress: \tEpoch 91 [320/1692 (18.87%)]\t\tLoss: 0.41562\n",
            "Training Progress: \tEpoch 91 [640/1692 (37.74%)]\t\tLoss: 0.17803\n",
            "Training Progress: \tEpoch 91 [960/1692 (56.60%)]\t\tLoss: 0.17984\n",
            "Training Progress: \tEpoch 91 [1280/1692 (75.47%)]\t\tLoss: 0.42843\n",
            "Training Progress: \tEpoch 91 [1600/1692 (94.34%)]\t\tLoss: 0.15834\n",
            "\tTrain loss: 0.00072, Accuracy: 1690/1692 (99.88%)\n",
            "\tValidation loss: 0.00573, Accuracy: 130/423 (30.73%)\n",
            "\tTest loss: 0.00504, Accuracy: 138/443 (31.15%)\n",
            "\n",
            "Training Progress: \tEpoch 92 [0/1692 (0.00%)]\t\tLoss: 0.29133\n",
            "Training Progress: \tEpoch 92 [320/1692 (18.87%)]\t\tLoss: 0.25307\n",
            "Training Progress: \tEpoch 92 [640/1692 (37.74%)]\t\tLoss: 0.33792\n",
            "Training Progress: \tEpoch 92 [960/1692 (56.60%)]\t\tLoss: 0.40816\n",
            "Training Progress: \tEpoch 92 [1280/1692 (75.47%)]\t\tLoss: 0.25268\n",
            "Training Progress: \tEpoch 92 [1600/1692 (94.34%)]\t\tLoss: 0.35630\n",
            "\tTrain loss: 0.00077, Accuracy: 1691/1692 (99.94%)\n",
            "\tValidation loss: 0.00569, Accuracy: 124/423 (29.31%)\n",
            "\tTest loss: 0.00507, Accuracy: 136/443 (30.70%)\n",
            "\n",
            "Training Progress: \tEpoch 93 [0/1692 (0.00%)]\t\tLoss: 0.20384\n",
            "Training Progress: \tEpoch 93 [320/1692 (18.87%)]\t\tLoss: 0.17672\n",
            "Training Progress: \tEpoch 93 [640/1692 (37.74%)]\t\tLoss: 0.20818\n",
            "Training Progress: \tEpoch 93 [960/1692 (56.60%)]\t\tLoss: 0.36197\n",
            "Training Progress: \tEpoch 93 [1280/1692 (75.47%)]\t\tLoss: 0.33510\n",
            "Training Progress: \tEpoch 93 [1600/1692 (94.34%)]\t\tLoss: 0.13881\n",
            "\tTrain loss: 0.00061, Accuracy: 1691/1692 (99.94%)\n",
            "\tValidation loss: 0.00557, Accuracy: 132/423 (31.21%)\n",
            "\tTest loss: 0.00508, Accuracy: 134/443 (30.25%)\n",
            "\n",
            "Training Progress: \tEpoch 94 [0/1692 (0.00%)]\t\tLoss: 0.17069\n",
            "Training Progress: \tEpoch 94 [320/1692 (18.87%)]\t\tLoss: 0.34246\n",
            "Training Progress: \tEpoch 94 [640/1692 (37.74%)]\t\tLoss: 0.13682\n",
            "Training Progress: \tEpoch 94 [960/1692 (56.60%)]\t\tLoss: 0.38795\n",
            "Training Progress: \tEpoch 94 [1280/1692 (75.47%)]\t\tLoss: 0.38528\n",
            "Training Progress: \tEpoch 94 [1600/1692 (94.34%)]\t\tLoss: 0.21619\n",
            "\tTrain loss: 0.00078, Accuracy: 1686/1692 (99.65%)\n",
            "\tValidation loss: 0.00567, Accuracy: 121/423 (28.61%)\n",
            "\tTest loss: 0.00516, Accuracy: 125/443 (28.22%)\n",
            "\n",
            "Training Progress: \tEpoch 95 [0/1692 (0.00%)]\t\tLoss: 0.50165\n",
            "Training Progress: \tEpoch 95 [320/1692 (18.87%)]\t\tLoss: 0.36224\n",
            "Training Progress: \tEpoch 95 [640/1692 (37.74%)]\t\tLoss: 0.35517\n",
            "Training Progress: \tEpoch 95 [960/1692 (56.60%)]\t\tLoss: 0.27923\n",
            "Training Progress: \tEpoch 95 [1280/1692 (75.47%)]\t\tLoss: 0.17192\n",
            "Training Progress: \tEpoch 95 [1600/1692 (94.34%)]\t\tLoss: 0.10752\n",
            "\tTrain loss: 0.00069, Accuracy: 1687/1692 (99.70%)\n",
            "\tValidation loss: 0.00581, Accuracy: 119/423 (28.13%)\n",
            "\tTest loss: 0.00515, Accuracy: 127/443 (28.67%)\n",
            "\n",
            "Training Progress: \tEpoch 96 [0/1692 (0.00%)]\t\tLoss: 0.39142\n",
            "Training Progress: \tEpoch 96 [320/1692 (18.87%)]\t\tLoss: 0.23731\n",
            "Training Progress: \tEpoch 96 [640/1692 (37.74%)]\t\tLoss: 0.13097\n",
            "Training Progress: \tEpoch 96 [960/1692 (56.60%)]\t\tLoss: 0.13393\n",
            "Training Progress: \tEpoch 96 [1280/1692 (75.47%)]\t\tLoss: 0.46483\n",
            "Training Progress: \tEpoch 96 [1600/1692 (94.34%)]\t\tLoss: 0.06651\n",
            "\tTrain loss: 0.00052, Accuracy: 1691/1692 (99.94%)\n",
            "\tValidation loss: 0.00579, Accuracy: 129/423 (30.50%)\n",
            "\tTest loss: 0.00521, Accuracy: 129/443 (29.12%)\n",
            "\n",
            "Training Progress: \tEpoch 97 [0/1692 (0.00%)]\t\tLoss: 0.27454\n",
            "Training Progress: \tEpoch 97 [320/1692 (18.87%)]\t\tLoss: 0.30732\n",
            "Training Progress: \tEpoch 97 [640/1692 (37.74%)]\t\tLoss: 0.24315\n",
            "Training Progress: \tEpoch 97 [960/1692 (56.60%)]\t\tLoss: 0.16729\n",
            "Training Progress: \tEpoch 97 [1280/1692 (75.47%)]\t\tLoss: 0.13802\n",
            "Training Progress: \tEpoch 97 [1600/1692 (94.34%)]\t\tLoss: 0.39354\n",
            "\tTrain loss: 0.00060, Accuracy: 1691/1692 (99.94%)\n",
            "\tValidation loss: 0.00593, Accuracy: 117/423 (27.66%)\n",
            "\tTest loss: 0.00525, Accuracy: 129/443 (29.12%)\n",
            "\n",
            "Training Progress: \tEpoch 98 [0/1692 (0.00%)]\t\tLoss: 0.11621\n",
            "Training Progress: \tEpoch 98 [320/1692 (18.87%)]\t\tLoss: 0.21584\n",
            "Training Progress: \tEpoch 98 [640/1692 (37.74%)]\t\tLoss: 0.09108\n",
            "Training Progress: \tEpoch 98 [960/1692 (56.60%)]\t\tLoss: 0.23350\n",
            "Training Progress: \tEpoch 98 [1280/1692 (75.47%)]\t\tLoss: 0.32716\n",
            "Training Progress: \tEpoch 98 [1600/1692 (94.34%)]\t\tLoss: 0.48505\n",
            "\tTrain loss: 0.00064, Accuracy: 1687/1692 (99.70%)\n",
            "\tValidation loss: 0.00596, Accuracy: 113/423 (26.71%)\n",
            "\tTest loss: 0.00535, Accuracy: 131/443 (29.57%)\n",
            "\n",
            "Training Progress: \tEpoch 99 [0/1692 (0.00%)]\t\tLoss: 0.34428\n",
            "Training Progress: \tEpoch 99 [320/1692 (18.87%)]\t\tLoss: 0.20762\n",
            "Training Progress: \tEpoch 99 [640/1692 (37.74%)]\t\tLoss: 0.36778\n",
            "Training Progress: \tEpoch 99 [960/1692 (56.60%)]\t\tLoss: 0.09650\n",
            "Training Progress: \tEpoch 99 [1280/1692 (75.47%)]\t\tLoss: 0.17126\n",
            "Training Progress: \tEpoch 99 [1600/1692 (94.34%)]\t\tLoss: 0.23752\n",
            "\tTrain loss: 0.00060, Accuracy: 1686/1692 (99.65%)\n",
            "\tValidation loss: 0.00628, Accuracy: 112/423 (26.48%)\n",
            "\tTest loss: 0.00554, Accuracy: 131/443 (29.57%)\n",
            "\n",
            "Training Progress: \tEpoch 100 [0/1692 (0.00%)]\t\tLoss: 0.22280\n",
            "Training Progress: \tEpoch 100 [320/1692 (18.87%)]\t\tLoss: 0.22242\n",
            "Training Progress: \tEpoch 100 [640/1692 (37.74%)]\t\tLoss: 0.23046\n",
            "Training Progress: \tEpoch 100 [960/1692 (56.60%)]\t\tLoss: 0.16106\n",
            "Training Progress: \tEpoch 100 [1280/1692 (75.47%)]\t\tLoss: 0.19125\n",
            "Training Progress: \tEpoch 100 [1600/1692 (94.34%)]\t\tLoss: 0.20936\n",
            "\tTrain loss: 0.00055, Accuracy: 1690/1692 (99.88%)\n",
            "\tValidation loss: 0.00591, Accuracy: 124/423 (29.31%)\n",
            "\tTest loss: 0.00533, Accuracy: 133/443 (30.02%)\n",
            "\n",
            "[[tensor(0.0440), 0.2730496453900709], [tensor(0.0436), 0.2730496453900709], [tensor(0.0434), 0.26300236406619387], [tensor(0.0434), 0.2765957446808511], [tensor(0.0433), 0.27364066193853426], [tensor(0.0433), 0.26773049645390073], [tensor(0.0433), 0.2730496453900709], [tensor(0.0432), 0.2718676122931442], [tensor(0.0431), 0.28782505910165485], [tensor(0.0430), 0.30141843971631205], [tensor(0.0429), 0.28664302600472813], [tensor(0.0429), 0.30260047281323876], [tensor(0.0428), 0.299645390070922], [tensor(0.0427), 0.3102836879432624], [tensor(0.0426), 0.31678486997635935], [tensor(0.0426), 0.3108747044917258], [tensor(0.0428), 0.30319148936170215], [tensor(0.0428), 0.3055555555555556], [tensor(0.0427), 0.299645390070922], [tensor(0.0428), 0.30319148936170215], [tensor(0.0419), 0.3516548463356974], [tensor(0.0425), 0.32860520094562645], [tensor(0.0418), 0.3587470449172577], [tensor(0.0421), 0.35106382978723405], [tensor(0.0434), 0.3126477541371158], [tensor(0.0420), 0.33628841607565013], [tensor(0.0421), 0.34810874704491723], [tensor(0.0417), 0.34574468085106386], [tensor(0.0416), 0.3611111111111111], [tensor(0.0408), 0.38475177304964536], [tensor(0.0400), 0.40189125295508277], [tensor(0.0407), 0.38652482269503546], [tensor(0.0396), 0.4160756501182033], [tensor(0.0397), 0.4237588652482269], [tensor(0.0396), 0.42494089834515364], [tensor(0.0380), 0.46749408983451535], [tensor(0.0384), 0.4574468085106383], [tensor(0.0364), 0.514775413711584], [tensor(0.0363), 0.5023640661938534], [tensor(0.0364), 0.48699763593380613], [tensor(0.0344), 0.5419621749408984], [tensor(0.0334), 0.5514184397163121], [tensor(0.0332), 0.5632387706855791], [tensor(0.0313), 0.6294326241134752], [tensor(0.0308), 0.6187943262411347], [tensor(0.0287), 0.6796690307328606], [tensor(0.0271), 0.6973995271867612], [tensor(0.0269), 0.6985815602836879], [tensor(0.0261), 0.7121749408983451], [tensor(0.0253), 0.7174940898345153], [tensor(0.0221), 0.7742316784869976], [tensor(0.0213), 0.8002364066193853], [tensor(0.0212), 0.8026004728132388], [tensor(0.0191), 0.8286052009456265], [tensor(0.0184), 0.8291962174940898], [tensor(0.0176), 0.8427895981087471], [tensor(0.0162), 0.8534278959810875], [tensor(0.0144), 0.8882978723404256], [tensor(0.0142), 0.8782505910165485], [tensor(0.0135), 0.8888888888888888], [tensor(0.0116), 0.9213947990543735], [tensor(0.0108), 0.9190307328605201], [tensor(0.0091), 0.9355791962174941], [tensor(0.0098), 0.9261229314420804], [tensor(0.0067), 0.958628841607565], [tensor(0.0071), 0.9544917257683215], [tensor(0.0058), 0.9692671394799054], [tensor(0.0056), 0.9698581560283688], [tensor(0.0052), 0.9775413711583925], [tensor(0.0045), 0.9810874704491725], [tensor(0.0040), 0.9799054373522459], [tensor(0.0038), 0.9781323877068558], [tensor(0.0034), 0.9846335697399528], [tensor(0.0038), 0.9816784869976359], [tensor(0.0031), 0.9869976359338062], [tensor(0.0028), 0.9840425531914894], [tensor(0.0025), 0.9923167848699763], [tensor(0.0021), 0.9946808510638298], [tensor(0.0022), 0.9934988179669031], [tensor(0.0019), 0.9946808510638298], [tensor(0.0021), 0.9923167848699763], [tensor(0.0015), 0.9940898345153665], [tensor(0.0017), 0.9940898345153665], [tensor(0.0013), 0.9970449172576832], [tensor(0.0013), 0.9964539007092199], [tensor(0.0013), 0.9952718676122931], [tensor(0.0009), 0.9958628841607565], [tensor(0.0011), 0.9976359338061466], [tensor(0.0010), 0.9952718676122931], [tensor(0.0009), 0.99822695035461], [tensor(0.0007), 0.9988179669030733], [tensor(0.0008), 0.9994089834515366], [tensor(0.0006), 0.9994089834515366], [tensor(0.0008), 0.9964539007092199], [tensor(0.0007), 0.9970449172576832], [tensor(0.0005), 0.9994089834515366], [tensor(0.0006), 0.9994089834515366], [tensor(0.0006), 0.9970449172576832], [tensor(0.0006), 0.9964539007092199], [tensor(0.0006), 0.9988179669030733]]\n",
            "<class 'list'>\n",
            "[[tensor(0.0033), 0.2907801418439716], [tensor(0.0033), 0.30023640661938533], [tensor(0.0033), 0.2718676122931442], [tensor(0.0033), 0.25059101654846333], [tensor(0.0033), 0.24349881796690306], [tensor(0.0033), 0.26713947990543735], [tensor(0.0033), 0.27423167848699764], [tensor(0.0033), 0.29550827423167847], [tensor(0.0033), 0.27423167848699764], [tensor(0.0033), 0.24113475177304963], [tensor(0.0033), 0.2695035460992908], [tensor(0.0033), 0.26713947990543735], [tensor(0.0033), 0.26713947990543735], [tensor(0.0033), 0.29314420803782504], [tensor(0.0033), 0.29550827423167847], [tensor(0.0033), 0.29314420803782504], [tensor(0.0033), 0.27423167848699764], [tensor(0.0033), 0.2860520094562648], [tensor(0.0033), 0.2907801418439716], [tensor(0.0033), 0.28132387706855794], [tensor(0.0032), 0.30023640661938533], [tensor(0.0033), 0.28132387706855794], [tensor(0.0032), 0.32860520094562645], [tensor(0.0033), 0.2695035460992908], [tensor(0.0034), 0.2647754137115839], [tensor(0.0033), 0.29550827423167847], [tensor(0.0033), 0.2789598108747045], [tensor(0.0033), 0.2765957446808511], [tensor(0.0033), 0.30023640661938533], [tensor(0.0033), 0.2860520094562648], [tensor(0.0033), 0.30969267139479906], [tensor(0.0033), 0.3144208037825059], [tensor(0.0033), 0.3191489361702128], [tensor(0.0033), 0.3144208037825059], [tensor(0.0033), 0.3191489361702128], [tensor(0.0033), 0.34988179669030733], [tensor(0.0033), 0.30260047281323876], [tensor(0.0032), 0.34278959810874704], [tensor(0.0033), 0.34988179669030733], [tensor(0.0033), 0.3333333333333333], [tensor(0.0033), 0.3262411347517731], [tensor(0.0034), 0.32860520094562645], [tensor(0.0033), 0.3215130023640662], [tensor(0.0033), 0.29314420803782504], [tensor(0.0034), 0.34988179669030733], [tensor(0.0034), 0.3215130023640662], [tensor(0.0034), 0.32860520094562645], [tensor(0.0034), 0.3309692671394799], [tensor(0.0035), 0.28368794326241137], [tensor(0.0036), 0.3144208037825059], [tensor(0.0036), 0.3120567375886525], [tensor(0.0036), 0.30969267139479906], [tensor(0.0036), 0.3191489361702128], [tensor(0.0037), 0.33569739952718675], [tensor(0.0038), 0.29550827423167847], [tensor(0.0037), 0.32387706855791965], [tensor(0.0038), 0.3191489361702128], [tensor(0.0039), 0.31678486997635935], [tensor(0.0040), 0.3049645390070922], [tensor(0.0040), 0.2907801418439716], [tensor(0.0040), 0.3262411347517731], [tensor(0.0041), 0.2978723404255319], [tensor(0.0042), 0.3049645390070922], [tensor(0.0043), 0.29550827423167847], [tensor(0.0043), 0.3144208037825059], [tensor(0.0044), 0.30023640661938533], [tensor(0.0046), 0.3144208037825059], [tensor(0.0047), 0.29550827423167847], [tensor(0.0045), 0.2978723404255319], [tensor(0.0045), 0.29314420803782504], [tensor(0.0049), 0.3073286052009456], [tensor(0.0046), 0.32860520094562645], [tensor(0.0047), 0.2907801418439716], [tensor(0.0049), 0.2907801418439716], [tensor(0.0049), 0.30969267139479906], [tensor(0.0050), 0.3073286052009456], [tensor(0.0050), 0.31678486997635935], [tensor(0.0050), 0.30023640661938533], [tensor(0.0051), 0.30969267139479906], [tensor(0.0052), 0.31678486997635935], [tensor(0.0051), 0.34515366430260047], [tensor(0.0054), 0.30969267139479906], [tensor(0.0053), 0.2860520094562648], [tensor(0.0053), 0.3144208037825059], [tensor(0.0055), 0.3073286052009456], [tensor(0.0055), 0.29314420803782504], [tensor(0.0056), 0.30023640661938533], [tensor(0.0055), 0.29314420803782504], [tensor(0.0056), 0.3073286052009456], [tensor(0.0058), 0.28841607565011823], [tensor(0.0057), 0.3073286052009456], [tensor(0.0057), 0.29314420803782504], [tensor(0.0056), 0.3120567375886525], [tensor(0.0057), 0.2860520094562648], [tensor(0.0058), 0.28132387706855794], [tensor(0.0058), 0.3049645390070922], [tensor(0.0059), 0.2765957446808511], [tensor(0.0060), 0.26713947990543735], [tensor(0.0063), 0.2647754137115839], [tensor(0.0059), 0.29314420803782504]]\n",
            "<class 'list'>\n",
            "[[tensor(0.0031), 0.3002257336343115], [tensor(0.0031), 0.29571106094808125], [tensor(0.0031), 0.21670428893905191], [tensor(0.0031), 0.2505643340857788], [tensor(0.0031), 0.22799097065462753], [tensor(0.0031), 0.24604966139954854], [tensor(0.0031), 0.2528216704288939], [tensor(0.0031), 0.27313769751693], [tensor(0.0031), 0.2708803611738149], [tensor(0.0031), 0.27539503386004516], [tensor(0.0031), 0.2708803611738149], [tensor(0.0031), 0.2618510158013544], [tensor(0.0031), 0.2618510158013544], [tensor(0.0031), 0.27539503386004516], [tensor(0.0031), 0.2618510158013544], [tensor(0.0031), 0.28216704288939054], [tensor(0.0031), 0.2799097065462754], [tensor(0.0031), 0.291196388261851], [tensor(0.0031), 0.27765237020316025], [tensor(0.0032), 0.2708803611738149], [tensor(0.0031), 0.27539503386004516], [tensor(0.0031), 0.2708803611738149], [tensor(0.0031), 0.2866817155756208], [tensor(0.0032), 0.26410835214446954], [tensor(0.0033), 0.25733634311512416], [tensor(0.0032), 0.26410835214446954], [tensor(0.0032), 0.27539503386004516], [tensor(0.0032), 0.29345372460496616], [tensor(0.0032), 0.30699774266365687], [tensor(0.0031), 0.30699774266365687], [tensor(0.0031), 0.29571106094808125], [tensor(0.0032), 0.30248306997742663], [tensor(0.0031), 0.28893905191873587], [tensor(0.0032), 0.28893905191873587], [tensor(0.0032), 0.27765237020316025], [tensor(0.0031), 0.3227990970654628], [tensor(0.0032), 0.32054176072234764], [tensor(0.0031), 0.29345372460496616], [tensor(0.0031), 0.31376975169300225], [tensor(0.0032), 0.3227990970654628], [tensor(0.0032), 0.3340857787810384], [tensor(0.0032), 0.3182844243792325], [tensor(0.0032), 0.3160270880361174], [tensor(0.0032), 0.31376975169300225], [tensor(0.0033), 0.29571106094808125], [tensor(0.0032), 0.31376975169300225], [tensor(0.0032), 0.32054176072234764], [tensor(0.0032), 0.31376975169300225], [tensor(0.0033), 0.27765237020316025], [tensor(0.0033), 0.3160270880361174], [tensor(0.0034), 0.3002257336343115], [tensor(0.0033), 0.3227990970654628], [tensor(0.0034), 0.2979683972911964], [tensor(0.0034), 0.31376975169300225], [tensor(0.0036), 0.30699774266365687], [tensor(0.0034), 0.32054176072234764], [tensor(0.0035), 0.309255079006772], [tensor(0.0037), 0.28442437923250563], [tensor(0.0037), 0.29571106094808125], [tensor(0.0037), 0.309255079006772], [tensor(0.0037), 0.3295711060948081], [tensor(0.0037), 0.3047404063205418], [tensor(0.0038), 0.3002257336343115], [tensor(0.0039), 0.27539503386004516], [tensor(0.0040), 0.31376975169300225], [tensor(0.0040), 0.3002257336343115], [tensor(0.0040), 0.32505643340857787], [tensor(0.0040), 0.3182844243792325], [tensor(0.0041), 0.30699774266365687], [tensor(0.0042), 0.28893905191873587], [tensor(0.0044), 0.29571106094808125], [tensor(0.0043), 0.32505643340857787], [tensor(0.0041), 0.3295711060948081], [tensor(0.0043), 0.30699774266365687], [tensor(0.0044), 0.32054176072234764], [tensor(0.0044), 0.3227990970654628], [tensor(0.0043), 0.34085778781038373], [tensor(0.0043), 0.33182844243792325], [tensor(0.0044), 0.32054176072234764], [tensor(0.0045), 0.31376975169300225], [tensor(0.0045), 0.3047404063205418], [tensor(0.0047), 0.309255079006772], [tensor(0.0047), 0.29345372460496616], [tensor(0.0046), 0.3182844243792325], [tensor(0.0048), 0.30699774266365687], [tensor(0.0048), 0.327313769751693], [tensor(0.0049), 0.3047404063205418], [tensor(0.0049), 0.29345372460496616], [tensor(0.0049), 0.309255079006772], [tensor(0.0050), 0.3115124153498871], [tensor(0.0050), 0.3115124153498871], [tensor(0.0051), 0.30699774266365687], [tensor(0.0051), 0.30248306997742663], [tensor(0.0052), 0.28216704288939054], [tensor(0.0052), 0.2866817155756208], [tensor(0.0052), 0.291196388261851], [tensor(0.0053), 0.291196388261851], [tensor(0.0054), 0.29571106094808125], [tensor(0.0055), 0.29571106094808125], [tensor(0.0053), 0.3002257336343115]]\n",
            "<class 'list'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Best validation accuracy:\n",
        "0.3640\n",
        "\n",
        "Best test accuracy:\n",
        "0.3792\n",
        "\n",
        "## Plotting Metrics v/s Number of Epochs: \n",
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "TpT3EpcflUZO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_GG9B5Ce--K"
      },
      "source": [
        "## LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AKxCuSYb1i6",
        "outputId": "ce59374f-380b-4430-d2f9-ab3755b6b9eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = LSTM().cuda()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR, betas=BETAS, eps=EPS, weight_decay=DECAY)\n",
        "train_and_evaluate(model, optimizer, data_loaders, num_epochs=EPOCHS)\n",
        "del model"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Progress: \tEpoch 1 [0/1692 (0.00%)]\t\tLoss: 1.51417\n",
            "Training Progress: \tEpoch 1 [320/1692 (18.87%)]\t\tLoss: 1.41791\n",
            "Training Progress: \tEpoch 1 [640/1692 (37.74%)]\t\tLoss: 1.49884\n",
            "Training Progress: \tEpoch 1 [960/1692 (56.60%)]\t\tLoss: 1.45401\n",
            "Training Progress: \tEpoch 1 [1280/1692 (75.47%)]\t\tLoss: 1.48749\n",
            "Training Progress: \tEpoch 1 [1600/1692 (94.34%)]\t\tLoss: 1.44664\n",
            "\tTrain loss: 0.04316, Accuracy: 482/1692 (28.49%)\n",
            "\tValidation loss: 0.00329, Accuracy: 97/423 (22.93%)\n",
            "\tTest loss: 0.00314, Accuracy: 104/443 (23.48%)\n",
            "\n",
            "Training Progress: \tEpoch 2 [0/1692 (0.00%)]\t\tLoss: 1.42401\n",
            "Training Progress: \tEpoch 2 [320/1692 (18.87%)]\t\tLoss: 1.46840\n",
            "Training Progress: \tEpoch 2 [640/1692 (37.74%)]\t\tLoss: 1.33624\n",
            "Training Progress: \tEpoch 2 [960/1692 (56.60%)]\t\tLoss: 1.34534\n",
            "Training Progress: \tEpoch 2 [1280/1692 (75.47%)]\t\tLoss: 1.39990\n",
            "Training Progress: \tEpoch 2 [1600/1692 (94.34%)]\t\tLoss: 1.33316\n",
            "\tTrain loss: 0.04296, Accuracy: 524/1692 (30.97%)\n",
            "\tValidation loss: 0.00330, Accuracy: 109/423 (25.77%)\n",
            "\tTest loss: 0.00316, Accuracy: 110/443 (24.83%)\n",
            "\n",
            "Training Progress: \tEpoch 3 [0/1692 (0.00%)]\t\tLoss: 1.42871\n",
            "Training Progress: \tEpoch 3 [320/1692 (18.87%)]\t\tLoss: 1.38800\n",
            "Training Progress: \tEpoch 3 [640/1692 (37.74%)]\t\tLoss: 1.39415\n",
            "Training Progress: \tEpoch 3 [960/1692 (56.60%)]\t\tLoss: 1.32512\n",
            "Training Progress: \tEpoch 3 [1280/1692 (75.47%)]\t\tLoss: 1.40692\n",
            "Training Progress: \tEpoch 3 [1600/1692 (94.34%)]\t\tLoss: 1.37995\n",
            "\tTrain loss: 0.04280, Accuracy: 554/1692 (32.74%)\n",
            "\tValidation loss: 0.00330, Accuracy: 105/423 (24.82%)\n",
            "\tTest loss: 0.00314, Accuracy: 117/443 (26.41%)\n",
            "\n",
            "Training Progress: \tEpoch 4 [0/1692 (0.00%)]\t\tLoss: 1.41731\n",
            "Training Progress: \tEpoch 4 [320/1692 (18.87%)]\t\tLoss: 1.42922\n",
            "Training Progress: \tEpoch 4 [640/1692 (37.74%)]\t\tLoss: 1.33263\n",
            "Training Progress: \tEpoch 4 [960/1692 (56.60%)]\t\tLoss: 1.33383\n",
            "Training Progress: \tEpoch 4 [1280/1692 (75.47%)]\t\tLoss: 1.41090\n",
            "Training Progress: \tEpoch 4 [1600/1692 (94.34%)]\t\tLoss: 1.34470\n",
            "\tTrain loss: 0.04267, Accuracy: 548/1692 (32.39%)\n",
            "\tValidation loss: 0.00329, Accuracy: 98/423 (23.17%)\n",
            "\tTest loss: 0.00315, Accuracy: 109/443 (24.60%)\n",
            "\n",
            "Training Progress: \tEpoch 5 [0/1692 (0.00%)]\t\tLoss: 1.36888\n",
            "Training Progress: \tEpoch 5 [320/1692 (18.87%)]\t\tLoss: 1.43896\n",
            "Training Progress: \tEpoch 5 [640/1692 (37.74%)]\t\tLoss: 1.41460\n",
            "Training Progress: \tEpoch 5 [960/1692 (56.60%)]\t\tLoss: 1.38471\n",
            "Training Progress: \tEpoch 5 [1280/1692 (75.47%)]\t\tLoss: 1.41863\n",
            "Training Progress: \tEpoch 5 [1600/1692 (94.34%)]\t\tLoss: 1.39213\n",
            "\tTrain loss: 0.04259, Accuracy: 556/1692 (32.86%)\n",
            "\tValidation loss: 0.00331, Accuracy: 97/423 (22.93%)\n",
            "\tTest loss: 0.00315, Accuracy: 106/443 (23.93%)\n",
            "\n",
            "Training Progress: \tEpoch 6 [0/1692 (0.00%)]\t\tLoss: 1.40518\n",
            "Training Progress: \tEpoch 6 [320/1692 (18.87%)]\t\tLoss: 1.35231\n",
            "Training Progress: \tEpoch 6 [640/1692 (37.74%)]\t\tLoss: 1.36033\n",
            "Training Progress: \tEpoch 6 [960/1692 (56.60%)]\t\tLoss: 1.38496\n",
            "Training Progress: \tEpoch 6 [1280/1692 (75.47%)]\t\tLoss: 1.35161\n",
            "Training Progress: \tEpoch 6 [1600/1692 (94.34%)]\t\tLoss: 1.40427\n",
            "\tTrain loss: 0.04249, Accuracy: 572/1692 (33.81%)\n",
            "\tValidation loss: 0.00333, Accuracy: 91/423 (21.51%)\n",
            "\tTest loss: 0.00315, Accuracy: 110/443 (24.83%)\n",
            "\n",
            "Training Progress: \tEpoch 7 [0/1692 (0.00%)]\t\tLoss: 1.33826\n",
            "Training Progress: \tEpoch 7 [320/1692 (18.87%)]\t\tLoss: 1.42589\n",
            "Training Progress: \tEpoch 7 [640/1692 (37.74%)]\t\tLoss: 1.37431\n",
            "Training Progress: \tEpoch 7 [960/1692 (56.60%)]\t\tLoss: 1.33280\n",
            "Training Progress: \tEpoch 7 [1280/1692 (75.47%)]\t\tLoss: 1.37213\n",
            "Training Progress: \tEpoch 7 [1600/1692 (94.34%)]\t\tLoss: 1.35112\n",
            "\tTrain loss: 0.04236, Accuracy: 563/1692 (33.27%)\n",
            "\tValidation loss: 0.00332, Accuracy: 105/423 (24.82%)\n",
            "\tTest loss: 0.00316, Accuracy: 103/443 (23.25%)\n",
            "\n",
            "Training Progress: \tEpoch 8 [0/1692 (0.00%)]\t\tLoss: 1.39156\n",
            "Training Progress: \tEpoch 8 [320/1692 (18.87%)]\t\tLoss: 1.43370\n",
            "Training Progress: \tEpoch 8 [640/1692 (37.74%)]\t\tLoss: 1.36449\n",
            "Training Progress: \tEpoch 8 [960/1692 (56.60%)]\t\tLoss: 1.36958\n",
            "Training Progress: \tEpoch 8 [1280/1692 (75.47%)]\t\tLoss: 1.39856\n",
            "Training Progress: \tEpoch 8 [1600/1692 (94.34%)]\t\tLoss: 1.38701\n",
            "\tTrain loss: 0.04221, Accuracy: 571/1692 (33.75%)\n",
            "\tValidation loss: 0.00335, Accuracy: 103/423 (24.35%)\n",
            "\tTest loss: 0.00316, Accuracy: 114/443 (25.73%)\n",
            "\n",
            "Training Progress: \tEpoch 9 [0/1692 (0.00%)]\t\tLoss: 1.39067\n",
            "Training Progress: \tEpoch 9 [320/1692 (18.87%)]\t\tLoss: 1.37606\n",
            "Training Progress: \tEpoch 9 [640/1692 (37.74%)]\t\tLoss: 1.33487\n",
            "Training Progress: \tEpoch 9 [960/1692 (56.60%)]\t\tLoss: 1.34374\n",
            "Training Progress: \tEpoch 9 [1280/1692 (75.47%)]\t\tLoss: 1.45041\n",
            "Training Progress: \tEpoch 9 [1600/1692 (94.34%)]\t\tLoss: 1.33055\n",
            "\tTrain loss: 0.04208, Accuracy: 596/1692 (35.22%)\n",
            "\tValidation loss: 0.00334, Accuracy: 102/423 (24.11%)\n",
            "\tTest loss: 0.00315, Accuracy: 114/443 (25.73%)\n",
            "\n",
            "Training Progress: \tEpoch 10 [0/1692 (0.00%)]\t\tLoss: 1.33997\n",
            "Training Progress: \tEpoch 10 [320/1692 (18.87%)]\t\tLoss: 1.34844\n",
            "Training Progress: \tEpoch 10 [640/1692 (37.74%)]\t\tLoss: 1.30951\n",
            "Training Progress: \tEpoch 10 [960/1692 (56.60%)]\t\tLoss: 1.38810\n",
            "Training Progress: \tEpoch 10 [1280/1692 (75.47%)]\t\tLoss: 1.39697\n",
            "Training Progress: \tEpoch 10 [1600/1692 (94.34%)]\t\tLoss: 1.38796\n",
            "\tTrain loss: 0.04188, Accuracy: 617/1692 (36.47%)\n",
            "\tValidation loss: 0.00335, Accuracy: 99/423 (23.40%)\n",
            "\tTest loss: 0.00317, Accuracy: 119/443 (26.86%)\n",
            "\n",
            "Training Progress: \tEpoch 11 [0/1692 (0.00%)]\t\tLoss: 1.33069\n",
            "Training Progress: \tEpoch 11 [320/1692 (18.87%)]\t\tLoss: 1.40936\n",
            "Training Progress: \tEpoch 11 [640/1692 (37.74%)]\t\tLoss: 1.27639\n",
            "Training Progress: \tEpoch 11 [960/1692 (56.60%)]\t\tLoss: 1.34513\n",
            "Training Progress: \tEpoch 11 [1280/1692 (75.47%)]\t\tLoss: 1.39398\n",
            "Training Progress: \tEpoch 11 [1600/1692 (94.34%)]\t\tLoss: 1.33966\n",
            "\tTrain loss: 0.04168, Accuracy: 626/1692 (37.00%)\n",
            "\tValidation loss: 0.00335, Accuracy: 111/423 (26.24%)\n",
            "\tTest loss: 0.00317, Accuracy: 121/443 (27.31%)\n",
            "\n",
            "Training Progress: \tEpoch 12 [0/1692 (0.00%)]\t\tLoss: 1.34334\n",
            "Training Progress: \tEpoch 12 [320/1692 (18.87%)]\t\tLoss: 1.36572\n",
            "Training Progress: \tEpoch 12 [640/1692 (37.74%)]\t\tLoss: 1.33657\n",
            "Training Progress: \tEpoch 12 [960/1692 (56.60%)]\t\tLoss: 1.37022\n",
            "Training Progress: \tEpoch 12 [1280/1692 (75.47%)]\t\tLoss: 1.34832\n",
            "Training Progress: \tEpoch 12 [1600/1692 (94.34%)]\t\tLoss: 1.31511\n",
            "\tTrain loss: 0.04137, Accuracy: 652/1692 (38.53%)\n",
            "\tValidation loss: 0.00338, Accuracy: 112/423 (26.48%)\n",
            "\tTest loss: 0.00317, Accuracy: 118/443 (26.64%)\n",
            "\n",
            "Training Progress: \tEpoch 13 [0/1692 (0.00%)]\t\tLoss: 1.33181\n",
            "Training Progress: \tEpoch 13 [320/1692 (18.87%)]\t\tLoss: 1.37680\n",
            "Training Progress: \tEpoch 13 [640/1692 (37.74%)]\t\tLoss: 1.30021\n",
            "Training Progress: \tEpoch 13 [960/1692 (56.60%)]\t\tLoss: 1.32868\n",
            "Training Progress: \tEpoch 13 [1280/1692 (75.47%)]\t\tLoss: 1.32084\n",
            "Training Progress: \tEpoch 13 [1600/1692 (94.34%)]\t\tLoss: 1.29085\n",
            "\tTrain loss: 0.04114, Accuracy: 655/1692 (38.71%)\n",
            "\tValidation loss: 0.00337, Accuracy: 106/423 (25.06%)\n",
            "\tTest loss: 0.00318, Accuracy: 109/443 (24.60%)\n",
            "\n",
            "Training Progress: \tEpoch 14 [0/1692 (0.00%)]\t\tLoss: 1.32663\n",
            "Training Progress: \tEpoch 14 [320/1692 (18.87%)]\t\tLoss: 1.38335\n",
            "Training Progress: \tEpoch 14 [640/1692 (37.74%)]\t\tLoss: 1.30451\n",
            "Training Progress: \tEpoch 14 [960/1692 (56.60%)]\t\tLoss: 1.36008\n",
            "Training Progress: \tEpoch 14 [1280/1692 (75.47%)]\t\tLoss: 1.39736\n",
            "Training Progress: \tEpoch 14 [1600/1692 (94.34%)]\t\tLoss: 1.33408\n",
            "\tTrain loss: 0.04106, Accuracy: 663/1692 (39.18%)\n",
            "\tValidation loss: 0.00340, Accuracy: 106/423 (25.06%)\n",
            "\tTest loss: 0.00319, Accuracy: 120/443 (27.09%)\n",
            "\n",
            "Training Progress: \tEpoch 15 [0/1692 (0.00%)]\t\tLoss: 1.25120\n",
            "Training Progress: \tEpoch 15 [320/1692 (18.87%)]\t\tLoss: 1.36248\n",
            "Training Progress: \tEpoch 15 [640/1692 (37.74%)]\t\tLoss: 1.28967\n",
            "Training Progress: \tEpoch 15 [960/1692 (56.60%)]\t\tLoss: 1.28428\n",
            "Training Progress: \tEpoch 15 [1280/1692 (75.47%)]\t\tLoss: 1.35155\n",
            "Training Progress: \tEpoch 15 [1600/1692 (94.34%)]\t\tLoss: 1.31917\n",
            "\tTrain loss: 0.04069, Accuracy: 684/1692 (40.43%)\n",
            "\tValidation loss: 0.00340, Accuracy: 113/423 (26.71%)\n",
            "\tTest loss: 0.00317, Accuracy: 121/443 (27.31%)\n",
            "\n",
            "Training Progress: \tEpoch 16 [0/1692 (0.00%)]\t\tLoss: 1.31441\n",
            "Training Progress: \tEpoch 16 [320/1692 (18.87%)]\t\tLoss: 1.37160\n",
            "Training Progress: \tEpoch 16 [640/1692 (37.74%)]\t\tLoss: 1.25899\n",
            "Training Progress: \tEpoch 16 [960/1692 (56.60%)]\t\tLoss: 1.30644\n",
            "Training Progress: \tEpoch 16 [1280/1692 (75.47%)]\t\tLoss: 1.37342\n",
            "Training Progress: \tEpoch 16 [1600/1692 (94.34%)]\t\tLoss: 1.33832\n",
            "\tTrain loss: 0.04042, Accuracy: 705/1692 (41.67%)\n",
            "\tValidation loss: 0.00342, Accuracy: 108/423 (25.53%)\n",
            "\tTest loss: 0.00320, Accuracy: 114/443 (25.73%)\n",
            "\n",
            "Training Progress: \tEpoch 17 [0/1692 (0.00%)]\t\tLoss: 1.26665\n",
            "Training Progress: \tEpoch 17 [320/1692 (18.87%)]\t\tLoss: 1.36076\n",
            "Training Progress: \tEpoch 17 [640/1692 (37.74%)]\t\tLoss: 1.30547\n",
            "Training Progress: \tEpoch 17 [960/1692 (56.60%)]\t\tLoss: 1.28331\n",
            "Training Progress: \tEpoch 17 [1280/1692 (75.47%)]\t\tLoss: 1.32803\n",
            "Training Progress: \tEpoch 17 [1600/1692 (94.34%)]\t\tLoss: 1.40666\n",
            "\tTrain loss: 0.04005, Accuracy: 714/1692 (42.20%)\n",
            "\tValidation loss: 0.00347, Accuracy: 103/423 (24.35%)\n",
            "\tTest loss: 0.00320, Accuracy: 114/443 (25.73%)\n",
            "\n",
            "Training Progress: \tEpoch 18 [0/1692 (0.00%)]\t\tLoss: 1.26529\n",
            "Training Progress: \tEpoch 18 [320/1692 (18.87%)]\t\tLoss: 1.33520\n",
            "Training Progress: \tEpoch 18 [640/1692 (37.74%)]\t\tLoss: 1.19369\n",
            "Training Progress: \tEpoch 18 [960/1692 (56.60%)]\t\tLoss: 1.23314\n",
            "Training Progress: \tEpoch 18 [1280/1692 (75.47%)]\t\tLoss: 1.37495\n",
            "Training Progress: \tEpoch 18 [1600/1692 (94.34%)]\t\tLoss: 1.28705\n",
            "\tTrain loss: 0.03983, Accuracy: 709/1692 (41.90%)\n",
            "\tValidation loss: 0.00346, Accuracy: 101/423 (23.88%)\n",
            "\tTest loss: 0.00321, Accuracy: 114/443 (25.73%)\n",
            "\n",
            "Training Progress: \tEpoch 19 [0/1692 (0.00%)]\t\tLoss: 1.27816\n",
            "Training Progress: \tEpoch 19 [320/1692 (18.87%)]\t\tLoss: 1.38422\n",
            "Training Progress: \tEpoch 19 [640/1692 (37.74%)]\t\tLoss: 1.34131\n",
            "Training Progress: \tEpoch 19 [960/1692 (56.60%)]\t\tLoss: 1.28478\n",
            "Training Progress: \tEpoch 19 [1280/1692 (75.47%)]\t\tLoss: 1.31976\n",
            "Training Progress: \tEpoch 19 [1600/1692 (94.34%)]\t\tLoss: 1.32400\n",
            "\tTrain loss: 0.03963, Accuracy: 742/1692 (43.85%)\n",
            "\tValidation loss: 0.00344, Accuracy: 105/423 (24.82%)\n",
            "\tTest loss: 0.00320, Accuracy: 117/443 (26.41%)\n",
            "\n",
            "Training Progress: \tEpoch 20 [0/1692 (0.00%)]\t\tLoss: 1.20274\n",
            "Training Progress: \tEpoch 20 [320/1692 (18.87%)]\t\tLoss: 1.46463\n",
            "Training Progress: \tEpoch 20 [640/1692 (37.74%)]\t\tLoss: 1.21883\n",
            "Training Progress: \tEpoch 20 [960/1692 (56.60%)]\t\tLoss: 1.27635\n",
            "Training Progress: \tEpoch 20 [1280/1692 (75.47%)]\t\tLoss: 1.35491\n",
            "Training Progress: \tEpoch 20 [1600/1692 (94.34%)]\t\tLoss: 1.36607\n",
            "\tTrain loss: 0.03916, Accuracy: 785/1692 (46.39%)\n",
            "\tValidation loss: 0.00347, Accuracy: 110/423 (26.00%)\n",
            "\tTest loss: 0.00322, Accuracy: 110/443 (24.83%)\n",
            "\n",
            "Training Progress: \tEpoch 21 [0/1692 (0.00%)]\t\tLoss: 1.25942\n",
            "Training Progress: \tEpoch 21 [320/1692 (18.87%)]\t\tLoss: 1.40787\n",
            "Training Progress: \tEpoch 21 [640/1692 (37.74%)]\t\tLoss: 1.28691\n",
            "Training Progress: \tEpoch 21 [960/1692 (56.60%)]\t\tLoss: 1.25395\n",
            "Training Progress: \tEpoch 21 [1280/1692 (75.47%)]\t\tLoss: 1.40413\n",
            "Training Progress: \tEpoch 21 [1600/1692 (94.34%)]\t\tLoss: 1.38376\n",
            "\tTrain loss: 0.03902, Accuracy: 743/1692 (43.91%)\n",
            "\tValidation loss: 0.00348, Accuracy: 110/423 (26.00%)\n",
            "\tTest loss: 0.00322, Accuracy: 117/443 (26.41%)\n",
            "\n",
            "Training Progress: \tEpoch 22 [0/1692 (0.00%)]\t\tLoss: 1.29894\n",
            "Training Progress: \tEpoch 22 [320/1692 (18.87%)]\t\tLoss: 1.35495\n",
            "Training Progress: \tEpoch 22 [640/1692 (37.74%)]\t\tLoss: 1.19722\n",
            "Training Progress: \tEpoch 22 [960/1692 (56.60%)]\t\tLoss: 1.15429\n",
            "Training Progress: \tEpoch 22 [1280/1692 (75.47%)]\t\tLoss: 1.18224\n",
            "Training Progress: \tEpoch 22 [1600/1692 (94.34%)]\t\tLoss: 1.29345\n",
            "\tTrain loss: 0.03844, Accuracy: 769/1692 (45.45%)\n",
            "\tValidation loss: 0.00351, Accuracy: 106/423 (25.06%)\n",
            "\tTest loss: 0.00324, Accuracy: 109/443 (24.60%)\n",
            "\n",
            "Training Progress: \tEpoch 23 [0/1692 (0.00%)]\t\tLoss: 1.15460\n",
            "Training Progress: \tEpoch 23 [320/1692 (18.87%)]\t\tLoss: 1.35123\n",
            "Training Progress: \tEpoch 23 [640/1692 (37.74%)]\t\tLoss: 1.20187\n",
            "Training Progress: \tEpoch 23 [960/1692 (56.60%)]\t\tLoss: 1.17966\n",
            "Training Progress: \tEpoch 23 [1280/1692 (75.47%)]\t\tLoss: 1.19516\n",
            "Training Progress: \tEpoch 23 [1600/1692 (94.34%)]\t\tLoss: 1.20292\n",
            "\tTrain loss: 0.03797, Accuracy: 796/1692 (47.04%)\n",
            "\tValidation loss: 0.00353, Accuracy: 104/423 (24.59%)\n",
            "\tTest loss: 0.00327, Accuracy: 107/443 (24.15%)\n",
            "\n",
            "Training Progress: \tEpoch 24 [0/1692 (0.00%)]\t\tLoss: 1.20375\n",
            "Training Progress: \tEpoch 24 [320/1692 (18.87%)]\t\tLoss: 1.37955\n",
            "Training Progress: \tEpoch 24 [640/1692 (37.74%)]\t\tLoss: 1.17369\n",
            "Training Progress: \tEpoch 24 [960/1692 (56.60%)]\t\tLoss: 1.27655\n",
            "Training Progress: \tEpoch 24 [1280/1692 (75.47%)]\t\tLoss: 1.29229\n",
            "Training Progress: \tEpoch 24 [1600/1692 (94.34%)]\t\tLoss: 1.28661\n",
            "\tTrain loss: 0.03784, Accuracy: 793/1692 (46.87%)\n",
            "\tValidation loss: 0.00351, Accuracy: 101/423 (23.88%)\n",
            "\tTest loss: 0.00327, Accuracy: 105/443 (23.70%)\n",
            "\n",
            "Training Progress: \tEpoch 25 [0/1692 (0.00%)]\t\tLoss: 1.21372\n",
            "Training Progress: \tEpoch 25 [320/1692 (18.87%)]\t\tLoss: 1.38627\n",
            "Training Progress: \tEpoch 25 [640/1692 (37.74%)]\t\tLoss: 1.25822\n",
            "Training Progress: \tEpoch 25 [960/1692 (56.60%)]\t\tLoss: 1.17362\n",
            "Training Progress: \tEpoch 25 [1280/1692 (75.47%)]\t\tLoss: 1.28277\n",
            "Training Progress: \tEpoch 25 [1600/1692 (94.34%)]\t\tLoss: 1.25994\n",
            "\tTrain loss: 0.03750, Accuracy: 818/1692 (48.35%)\n",
            "\tValidation loss: 0.00360, Accuracy: 99/423 (23.40%)\n",
            "\tTest loss: 0.00330, Accuracy: 122/443 (27.54%)\n",
            "\n",
            "Training Progress: \tEpoch 26 [0/1692 (0.00%)]\t\tLoss: 1.23914\n",
            "Training Progress: \tEpoch 26 [320/1692 (18.87%)]\t\tLoss: 1.32873\n",
            "Training Progress: \tEpoch 26 [640/1692 (37.74%)]\t\tLoss: 1.25328\n",
            "Training Progress: \tEpoch 26 [960/1692 (56.60%)]\t\tLoss: 1.14285\n",
            "Training Progress: \tEpoch 26 [1280/1692 (75.47%)]\t\tLoss: 1.19718\n",
            "Training Progress: \tEpoch 26 [1600/1692 (94.34%)]\t\tLoss: 1.18408\n",
            "\tTrain loss: 0.03704, Accuracy: 822/1692 (48.58%)\n",
            "\tValidation loss: 0.00357, Accuracy: 95/423 (22.46%)\n",
            "\tTest loss: 0.00332, Accuracy: 108/443 (24.38%)\n",
            "\n",
            "Training Progress: \tEpoch 27 [0/1692 (0.00%)]\t\tLoss: 1.19548\n",
            "Training Progress: \tEpoch 27 [320/1692 (18.87%)]\t\tLoss: 1.43088\n",
            "Training Progress: \tEpoch 27 [640/1692 (37.74%)]\t\tLoss: 1.13639\n",
            "Training Progress: \tEpoch 27 [960/1692 (56.60%)]\t\tLoss: 1.14663\n",
            "Training Progress: \tEpoch 27 [1280/1692 (75.47%)]\t\tLoss: 1.19040\n",
            "Training Progress: \tEpoch 27 [1600/1692 (94.34%)]\t\tLoss: 1.23805\n",
            "\tTrain loss: 0.03649, Accuracy: 856/1692 (50.59%)\n",
            "\tValidation loss: 0.00363, Accuracy: 99/423 (23.40%)\n",
            "\tTest loss: 0.00334, Accuracy: 111/443 (25.06%)\n",
            "\n",
            "Training Progress: \tEpoch 28 [0/1692 (0.00%)]\t\tLoss: 1.17948\n",
            "Training Progress: \tEpoch 28 [320/1692 (18.87%)]\t\tLoss: 1.34246\n",
            "Training Progress: \tEpoch 28 [640/1692 (37.74%)]\t\tLoss: 1.12066\n",
            "Training Progress: \tEpoch 28 [960/1692 (56.60%)]\t\tLoss: 1.10616\n",
            "Training Progress: \tEpoch 28 [1280/1692 (75.47%)]\t\tLoss: 1.18961\n",
            "Training Progress: \tEpoch 28 [1600/1692 (94.34%)]\t\tLoss: 1.25339\n",
            "\tTrain loss: 0.03656, Accuracy: 852/1692 (50.35%)\n",
            "\tValidation loss: 0.00359, Accuracy: 96/423 (22.70%)\n",
            "\tTest loss: 0.00332, Accuracy: 110/443 (24.83%)\n",
            "\n",
            "Training Progress: \tEpoch 29 [0/1692 (0.00%)]\t\tLoss: 1.25791\n",
            "Training Progress: \tEpoch 29 [320/1692 (18.87%)]\t\tLoss: 1.32801\n",
            "Training Progress: \tEpoch 29 [640/1692 (37.74%)]\t\tLoss: 1.14333\n",
            "Training Progress: \tEpoch 29 [960/1692 (56.60%)]\t\tLoss: 1.12447\n",
            "Training Progress: \tEpoch 29 [1280/1692 (75.47%)]\t\tLoss: 1.20032\n",
            "Training Progress: \tEpoch 29 [1600/1692 (94.34%)]\t\tLoss: 1.28028\n",
            "\tTrain loss: 0.03577, Accuracy: 862/1692 (50.95%)\n",
            "\tValidation loss: 0.00369, Accuracy: 97/423 (22.93%)\n",
            "\tTest loss: 0.00341, Accuracy: 104/443 (23.48%)\n",
            "\n",
            "Training Progress: \tEpoch 30 [0/1692 (0.00%)]\t\tLoss: 1.24914\n",
            "Training Progress: \tEpoch 30 [320/1692 (18.87%)]\t\tLoss: 1.42974\n",
            "Training Progress: \tEpoch 30 [640/1692 (37.74%)]\t\tLoss: 1.24092\n",
            "Training Progress: \tEpoch 30 [960/1692 (56.60%)]\t\tLoss: 1.13438\n",
            "Training Progress: \tEpoch 30 [1280/1692 (75.47%)]\t\tLoss: 1.38287\n",
            "Training Progress: \tEpoch 30 [1600/1692 (94.34%)]\t\tLoss: 1.21070\n",
            "\tTrain loss: 0.03550, Accuracy: 867/1692 (51.24%)\n",
            "\tValidation loss: 0.00370, Accuracy: 90/423 (21.28%)\n",
            "\tTest loss: 0.00341, Accuracy: 108/443 (24.38%)\n",
            "\n",
            "Training Progress: \tEpoch 31 [0/1692 (0.00%)]\t\tLoss: 1.09580\n",
            "Training Progress: \tEpoch 31 [320/1692 (18.87%)]\t\tLoss: 1.23981\n",
            "Training Progress: \tEpoch 31 [640/1692 (37.74%)]\t\tLoss: 1.15754\n",
            "Training Progress: \tEpoch 31 [960/1692 (56.60%)]\t\tLoss: 1.16579\n",
            "Training Progress: \tEpoch 31 [1280/1692 (75.47%)]\t\tLoss: 1.17154\n",
            "Training Progress: \tEpoch 31 [1600/1692 (94.34%)]\t\tLoss: 1.29215\n",
            "\tTrain loss: 0.03499, Accuracy: 902/1692 (53.31%)\n",
            "\tValidation loss: 0.00366, Accuracy: 95/423 (22.46%)\n",
            "\tTest loss: 0.00339, Accuracy: 115/443 (25.96%)\n",
            "\n",
            "Training Progress: \tEpoch 32 [0/1692 (0.00%)]\t\tLoss: 1.08717\n",
            "Training Progress: \tEpoch 32 [320/1692 (18.87%)]\t\tLoss: 1.29260\n",
            "Training Progress: \tEpoch 32 [640/1692 (37.74%)]\t\tLoss: 1.24278\n",
            "Training Progress: \tEpoch 32 [960/1692 (56.60%)]\t\tLoss: 1.10956\n",
            "Training Progress: \tEpoch 32 [1280/1692 (75.47%)]\t\tLoss: 1.23331\n",
            "Training Progress: \tEpoch 32 [1600/1692 (94.34%)]\t\tLoss: 1.21822\n",
            "\tTrain loss: 0.03477, Accuracy: 884/1692 (52.25%)\n",
            "\tValidation loss: 0.00376, Accuracy: 89/423 (21.04%)\n",
            "\tTest loss: 0.00347, Accuracy: 115/443 (25.96%)\n",
            "\n",
            "Training Progress: \tEpoch 33 [0/1692 (0.00%)]\t\tLoss: 1.16177\n",
            "Training Progress: \tEpoch 33 [320/1692 (18.87%)]\t\tLoss: 1.25686\n",
            "Training Progress: \tEpoch 33 [640/1692 (37.74%)]\t\tLoss: 1.20424\n",
            "Training Progress: \tEpoch 33 [960/1692 (56.60%)]\t\tLoss: 0.96546\n",
            "Training Progress: \tEpoch 33 [1280/1692 (75.47%)]\t\tLoss: 1.24703\n",
            "Training Progress: \tEpoch 33 [1600/1692 (94.34%)]\t\tLoss: 1.23004\n",
            "\tTrain loss: 0.03436, Accuracy: 914/1692 (54.02%)\n",
            "\tValidation loss: 0.00374, Accuracy: 93/423 (21.99%)\n",
            "\tTest loss: 0.00350, Accuracy: 110/443 (24.83%)\n",
            "\n",
            "Training Progress: \tEpoch 34 [0/1692 (0.00%)]\t\tLoss: 1.14790\n",
            "Training Progress: \tEpoch 34 [320/1692 (18.87%)]\t\tLoss: 1.29896\n",
            "Training Progress: \tEpoch 34 [640/1692 (37.74%)]\t\tLoss: 1.17809\n",
            "Training Progress: \tEpoch 34 [960/1692 (56.60%)]\t\tLoss: 1.09019\n",
            "Training Progress: \tEpoch 34 [1280/1692 (75.47%)]\t\tLoss: 1.14561\n",
            "Training Progress: \tEpoch 34 [1600/1692 (94.34%)]\t\tLoss: 1.13680\n",
            "\tTrain loss: 0.03353, Accuracy: 925/1692 (54.67%)\n",
            "\tValidation loss: 0.00380, Accuracy: 85/423 (20.09%)\n",
            "\tTest loss: 0.00350, Accuracy: 109/443 (24.60%)\n",
            "\n",
            "Training Progress: \tEpoch 35 [0/1692 (0.00%)]\t\tLoss: 1.11597\n",
            "Training Progress: \tEpoch 35 [320/1692 (18.87%)]\t\tLoss: 1.29073\n",
            "Training Progress: \tEpoch 35 [640/1692 (37.74%)]\t\tLoss: 1.14725\n",
            "Training Progress: \tEpoch 35 [960/1692 (56.60%)]\t\tLoss: 1.09853\n",
            "Training Progress: \tEpoch 35 [1280/1692 (75.47%)]\t\tLoss: 1.08036\n",
            "Training Progress: \tEpoch 35 [1600/1692 (94.34%)]\t\tLoss: 1.20337\n",
            "\tTrain loss: 0.03290, Accuracy: 937/1692 (55.38%)\n",
            "\tValidation loss: 0.00388, Accuracy: 90/423 (21.28%)\n",
            "\tTest loss: 0.00356, Accuracy: 113/443 (25.51%)\n",
            "\n",
            "Training Progress: \tEpoch 36 [0/1692 (0.00%)]\t\tLoss: 1.03574\n",
            "Training Progress: \tEpoch 36 [320/1692 (18.87%)]\t\tLoss: 1.24308\n",
            "Training Progress: \tEpoch 36 [640/1692 (37.74%)]\t\tLoss: 1.05899\n",
            "Training Progress: \tEpoch 36 [960/1692 (56.60%)]\t\tLoss: 1.17417\n",
            "Training Progress: \tEpoch 36 [1280/1692 (75.47%)]\t\tLoss: 1.13030\n",
            "Training Progress: \tEpoch 36 [1600/1692 (94.34%)]\t\tLoss: 1.13691\n",
            "\tTrain loss: 0.03267, Accuracy: 949/1692 (56.09%)\n",
            "\tValidation loss: 0.00393, Accuracy: 89/423 (21.04%)\n",
            "\tTest loss: 0.00363, Accuracy: 116/443 (26.19%)\n",
            "\n",
            "Training Progress: \tEpoch 37 [0/1692 (0.00%)]\t\tLoss: 1.10343\n",
            "Training Progress: \tEpoch 37 [320/1692 (18.87%)]\t\tLoss: 1.29090\n",
            "Training Progress: \tEpoch 37 [640/1692 (37.74%)]\t\tLoss: 1.07926\n",
            "Training Progress: \tEpoch 37 [960/1692 (56.60%)]\t\tLoss: 1.01068\n",
            "Training Progress: \tEpoch 37 [1280/1692 (75.47%)]\t\tLoss: 1.09148\n",
            "Training Progress: \tEpoch 37 [1600/1692 (94.34%)]\t\tLoss: 1.08821\n",
            "\tTrain loss: 0.03191, Accuracy: 989/1692 (58.45%)\n",
            "\tValidation loss: 0.00393, Accuracy: 91/423 (21.51%)\n",
            "\tTest loss: 0.00368, Accuracy: 107/443 (24.15%)\n",
            "\n",
            "Training Progress: \tEpoch 38 [0/1692 (0.00%)]\t\tLoss: 1.11108\n",
            "Training Progress: \tEpoch 38 [320/1692 (18.87%)]\t\tLoss: 1.23615\n",
            "Training Progress: \tEpoch 38 [640/1692 (37.74%)]\t\tLoss: 1.02113\n",
            "Training Progress: \tEpoch 38 [960/1692 (56.60%)]\t\tLoss: 0.98763\n",
            "Training Progress: \tEpoch 38 [1280/1692 (75.47%)]\t\tLoss: 1.12810\n",
            "Training Progress: \tEpoch 38 [1600/1692 (94.34%)]\t\tLoss: 1.06954\n",
            "\tTrain loss: 0.03179, Accuracy: 990/1692 (58.51%)\n",
            "\tValidation loss: 0.00396, Accuracy: 86/423 (20.33%)\n",
            "\tTest loss: 0.00370, Accuracy: 106/443 (23.93%)\n",
            "\n",
            "Training Progress: \tEpoch 39 [0/1692 (0.00%)]\t\tLoss: 1.02601\n",
            "Training Progress: \tEpoch 39 [320/1692 (18.87%)]\t\tLoss: 1.18135\n",
            "Training Progress: \tEpoch 39 [640/1692 (37.74%)]\t\tLoss: 1.01831\n",
            "Training Progress: \tEpoch 39 [960/1692 (56.60%)]\t\tLoss: 1.01021\n",
            "Training Progress: \tEpoch 39 [1280/1692 (75.47%)]\t\tLoss: 1.08849\n",
            "Training Progress: \tEpoch 39 [1600/1692 (94.34%)]\t\tLoss: 1.17781\n",
            "\tTrain loss: 0.03105, Accuracy: 1031/1692 (60.93%)\n",
            "\tValidation loss: 0.00396, Accuracy: 84/423 (19.86%)\n",
            "\tTest loss: 0.00366, Accuracy: 114/443 (25.73%)\n",
            "\n",
            "Training Progress: \tEpoch 40 [0/1692 (0.00%)]\t\tLoss: 1.05011\n",
            "Training Progress: \tEpoch 40 [320/1692 (18.87%)]\t\tLoss: 1.19814\n",
            "Training Progress: \tEpoch 40 [640/1692 (37.74%)]\t\tLoss: 0.99915\n",
            "Training Progress: \tEpoch 40 [960/1692 (56.60%)]\t\tLoss: 0.98630\n",
            "Training Progress: \tEpoch 40 [1280/1692 (75.47%)]\t\tLoss: 0.98255\n",
            "Training Progress: \tEpoch 40 [1600/1692 (94.34%)]\t\tLoss: 1.09492\n",
            "\tTrain loss: 0.03075, Accuracy: 1041/1692 (61.52%)\n",
            "\tValidation loss: 0.00399, Accuracy: 94/423 (22.22%)\n",
            "\tTest loss: 0.00368, Accuracy: 117/443 (26.41%)\n",
            "\n",
            "Training Progress: \tEpoch 41 [0/1692 (0.00%)]\t\tLoss: 0.94065\n",
            "Training Progress: \tEpoch 41 [320/1692 (18.87%)]\t\tLoss: 1.26554\n",
            "Training Progress: \tEpoch 41 [640/1692 (37.74%)]\t\tLoss: 0.98500\n",
            "Training Progress: \tEpoch 41 [960/1692 (56.60%)]\t\tLoss: 1.03012\n",
            "Training Progress: \tEpoch 41 [1280/1692 (75.47%)]\t\tLoss: 0.87886\n",
            "Training Progress: \tEpoch 41 [1600/1692 (94.34%)]\t\tLoss: 1.09062\n",
            "\tTrain loss: 0.02995, Accuracy: 1055/1692 (62.35%)\n",
            "\tValidation loss: 0.00411, Accuracy: 85/423 (20.09%)\n",
            "\tTest loss: 0.00384, Accuracy: 108/443 (24.38%)\n",
            "\n",
            "Training Progress: \tEpoch 42 [0/1692 (0.00%)]\t\tLoss: 1.06279\n",
            "Training Progress: \tEpoch 42 [320/1692 (18.87%)]\t\tLoss: 1.20091\n",
            "Training Progress: \tEpoch 42 [640/1692 (37.74%)]\t\tLoss: 1.10162\n",
            "Training Progress: \tEpoch 42 [960/1692 (56.60%)]\t\tLoss: 0.95420\n",
            "Training Progress: \tEpoch 42 [1280/1692 (75.47%)]\t\tLoss: 0.90018\n",
            "Training Progress: \tEpoch 42 [1600/1692 (94.34%)]\t\tLoss: 0.99200\n",
            "\tTrain loss: 0.02983, Accuracy: 1040/1692 (61.47%)\n",
            "\tValidation loss: 0.00418, Accuracy: 92/423 (21.75%)\n",
            "\tTest loss: 0.00393, Accuracy: 104/443 (23.48%)\n",
            "\n",
            "Training Progress: \tEpoch 43 [0/1692 (0.00%)]\t\tLoss: 0.90263\n",
            "Training Progress: \tEpoch 43 [320/1692 (18.87%)]\t\tLoss: 1.19571\n",
            "Training Progress: \tEpoch 43 [640/1692 (37.74%)]\t\tLoss: 1.00771\n",
            "Training Progress: \tEpoch 43 [960/1692 (56.60%)]\t\tLoss: 0.99011\n",
            "Training Progress: \tEpoch 43 [1280/1692 (75.47%)]\t\tLoss: 1.04672\n",
            "Training Progress: \tEpoch 43 [1600/1692 (94.34%)]\t\tLoss: 1.00686\n",
            "\tTrain loss: 0.02932, Accuracy: 1054/1692 (62.29%)\n",
            "\tValidation loss: 0.00413, Accuracy: 83/423 (19.62%)\n",
            "\tTest loss: 0.00386, Accuracy: 105/443 (23.70%)\n",
            "\n",
            "Training Progress: \tEpoch 44 [0/1692 (0.00%)]\t\tLoss: 0.94565\n",
            "Training Progress: \tEpoch 44 [320/1692 (18.87%)]\t\tLoss: 1.25165\n",
            "Training Progress: \tEpoch 44 [640/1692 (37.74%)]\t\tLoss: 0.97873\n",
            "Training Progress: \tEpoch 44 [960/1692 (56.60%)]\t\tLoss: 1.00815\n",
            "Training Progress: \tEpoch 44 [1280/1692 (75.47%)]\t\tLoss: 0.94710\n",
            "Training Progress: \tEpoch 44 [1600/1692 (94.34%)]\t\tLoss: 1.02109\n",
            "\tTrain loss: 0.02895, Accuracy: 1082/1692 (63.95%)\n",
            "\tValidation loss: 0.00421, Accuracy: 85/423 (20.09%)\n",
            "\tTest loss: 0.00391, Accuracy: 104/443 (23.48%)\n",
            "\n",
            "Training Progress: \tEpoch 45 [0/1692 (0.00%)]\t\tLoss: 1.03539\n",
            "Training Progress: \tEpoch 45 [320/1692 (18.87%)]\t\tLoss: 1.11138\n",
            "Training Progress: \tEpoch 45 [640/1692 (37.74%)]\t\tLoss: 0.89801\n",
            "Training Progress: \tEpoch 45 [960/1692 (56.60%)]\t\tLoss: 0.94774\n",
            "Training Progress: \tEpoch 45 [1280/1692 (75.47%)]\t\tLoss: 0.92869\n",
            "Training Progress: \tEpoch 45 [1600/1692 (94.34%)]\t\tLoss: 1.12313\n",
            "\tTrain loss: 0.02880, Accuracy: 1097/1692 (64.83%)\n",
            "\tValidation loss: 0.00418, Accuracy: 81/423 (19.15%)\n",
            "\tTest loss: 0.00385, Accuracy: 111/443 (25.06%)\n",
            "\n",
            "Training Progress: \tEpoch 46 [0/1692 (0.00%)]\t\tLoss: 1.07042\n",
            "Training Progress: \tEpoch 46 [320/1692 (18.87%)]\t\tLoss: 1.17734\n",
            "Training Progress: \tEpoch 46 [640/1692 (37.74%)]\t\tLoss: 0.96485\n",
            "Training Progress: \tEpoch 46 [960/1692 (56.60%)]\t\tLoss: 0.88469\n",
            "Training Progress: \tEpoch 46 [1280/1692 (75.47%)]\t\tLoss: 0.91315\n",
            "Training Progress: \tEpoch 46 [1600/1692 (94.34%)]\t\tLoss: 0.92876\n",
            "\tTrain loss: 0.02764, Accuracy: 1093/1692 (64.60%)\n",
            "\tValidation loss: 0.00427, Accuracy: 89/423 (21.04%)\n",
            "\tTest loss: 0.00398, Accuracy: 113/443 (25.51%)\n",
            "\n",
            "Training Progress: \tEpoch 47 [0/1692 (0.00%)]\t\tLoss: 0.99417\n",
            "Training Progress: \tEpoch 47 [320/1692 (18.87%)]\t\tLoss: 1.08689\n",
            "Training Progress: \tEpoch 47 [640/1692 (37.74%)]\t\tLoss: 0.98837\n",
            "Training Progress: \tEpoch 47 [960/1692 (56.60%)]\t\tLoss: 0.85659\n",
            "Training Progress: \tEpoch 47 [1280/1692 (75.47%)]\t\tLoss: 0.79527\n",
            "Training Progress: \tEpoch 47 [1600/1692 (94.34%)]\t\tLoss: 0.97487\n",
            "\tTrain loss: 0.02651, Accuracy: 1146/1692 (67.73%)\n",
            "\tValidation loss: 0.00437, Accuracy: 89/423 (21.04%)\n",
            "\tTest loss: 0.00407, Accuracy: 106/443 (23.93%)\n",
            "\n",
            "Training Progress: \tEpoch 48 [0/1692 (0.00%)]\t\tLoss: 0.99307\n",
            "Training Progress: \tEpoch 48 [320/1692 (18.87%)]\t\tLoss: 0.96562\n",
            "Training Progress: \tEpoch 48 [640/1692 (37.74%)]\t\tLoss: 0.92894\n",
            "Training Progress: \tEpoch 48 [960/1692 (56.60%)]\t\tLoss: 0.99307\n",
            "Training Progress: \tEpoch 48 [1280/1692 (75.47%)]\t\tLoss: 0.78122\n",
            "Training Progress: \tEpoch 48 [1600/1692 (94.34%)]\t\tLoss: 1.02014\n",
            "\tTrain loss: 0.02643, Accuracy: 1117/1692 (66.02%)\n",
            "\tValidation loss: 0.00439, Accuracy: 87/423 (20.57%)\n",
            "\tTest loss: 0.00411, Accuracy: 105/443 (23.70%)\n",
            "\n",
            "Training Progress: \tEpoch 49 [0/1692 (0.00%)]\t\tLoss: 1.02832\n",
            "Training Progress: \tEpoch 49 [320/1692 (18.87%)]\t\tLoss: 1.09436\n",
            "Training Progress: \tEpoch 49 [640/1692 (37.74%)]\t\tLoss: 0.90498\n",
            "Training Progress: \tEpoch 49 [960/1692 (56.60%)]\t\tLoss: 0.95343\n",
            "Training Progress: \tEpoch 49 [1280/1692 (75.47%)]\t\tLoss: 1.10213\n",
            "Training Progress: \tEpoch 49 [1600/1692 (94.34%)]\t\tLoss: 1.00859\n",
            "\tTrain loss: 0.02615, Accuracy: 1147/1692 (67.79%)\n",
            "\tValidation loss: 0.00435, Accuracy: 91/423 (21.51%)\n",
            "\tTest loss: 0.00406, Accuracy: 111/443 (25.06%)\n",
            "\n",
            "Training Progress: \tEpoch 50 [0/1692 (0.00%)]\t\tLoss: 1.08562\n",
            "Training Progress: \tEpoch 50 [320/1692 (18.87%)]\t\tLoss: 1.18831\n",
            "Training Progress: \tEpoch 50 [640/1692 (37.74%)]\t\tLoss: 1.08177\n",
            "Training Progress: \tEpoch 50 [960/1692 (56.60%)]\t\tLoss: 0.98704\n",
            "Training Progress: \tEpoch 50 [1280/1692 (75.47%)]\t\tLoss: 0.88657\n",
            "Training Progress: \tEpoch 50 [1600/1692 (94.34%)]\t\tLoss: 0.89454\n",
            "\tTrain loss: 0.02556, Accuracy: 1155/1692 (68.26%)\n",
            "\tValidation loss: 0.00444, Accuracy: 91/423 (21.51%)\n",
            "\tTest loss: 0.00421, Accuracy: 100/443 (22.57%)\n",
            "\n",
            "Training Progress: \tEpoch 51 [0/1692 (0.00%)]\t\tLoss: 0.99805\n",
            "Training Progress: \tEpoch 51 [320/1692 (18.87%)]\t\tLoss: 0.95438\n",
            "Training Progress: \tEpoch 51 [640/1692 (37.74%)]\t\tLoss: 0.80008\n",
            "Training Progress: \tEpoch 51 [960/1692 (56.60%)]\t\tLoss: 0.88824\n",
            "Training Progress: \tEpoch 51 [1280/1692 (75.47%)]\t\tLoss: 0.80608\n",
            "Training Progress: \tEpoch 51 [1600/1692 (94.34%)]\t\tLoss: 0.83837\n",
            "\tTrain loss: 0.02534, Accuracy: 1172/1692 (69.27%)\n",
            "\tValidation loss: 0.00454, Accuracy: 90/423 (21.28%)\n",
            "\tTest loss: 0.00424, Accuracy: 106/443 (23.93%)\n",
            "\n",
            "Training Progress: \tEpoch 52 [0/1692 (0.00%)]\t\tLoss: 0.99453\n",
            "Training Progress: \tEpoch 52 [320/1692 (18.87%)]\t\tLoss: 1.10760\n",
            "Training Progress: \tEpoch 52 [640/1692 (37.74%)]\t\tLoss: 0.88031\n",
            "Training Progress: \tEpoch 52 [960/1692 (56.60%)]\t\tLoss: 0.87879\n",
            "Training Progress: \tEpoch 52 [1280/1692 (75.47%)]\t\tLoss: 0.75107\n",
            "Training Progress: \tEpoch 52 [1600/1692 (94.34%)]\t\tLoss: 1.20717\n",
            "\tTrain loss: 0.02446, Accuracy: 1205/1692 (71.22%)\n",
            "\tValidation loss: 0.00454, Accuracy: 88/423 (20.80%)\n",
            "\tTest loss: 0.00428, Accuracy: 115/443 (25.96%)\n",
            "\n",
            "Training Progress: \tEpoch 53 [0/1692 (0.00%)]\t\tLoss: 1.00892\n",
            "Training Progress: \tEpoch 53 [320/1692 (18.87%)]\t\tLoss: 0.91487\n",
            "Training Progress: \tEpoch 53 [640/1692 (37.74%)]\t\tLoss: 0.84056\n",
            "Training Progress: \tEpoch 53 [960/1692 (56.60%)]\t\tLoss: 0.83776\n",
            "Training Progress: \tEpoch 53 [1280/1692 (75.47%)]\t\tLoss: 0.78429\n",
            "Training Progress: \tEpoch 53 [1600/1692 (94.34%)]\t\tLoss: 0.75739\n",
            "\tTrain loss: 0.02448, Accuracy: 1187/1692 (70.15%)\n",
            "\tValidation loss: 0.00454, Accuracy: 99/423 (23.40%)\n",
            "\tTest loss: 0.00435, Accuracy: 99/443 (22.35%)\n",
            "\n",
            "Training Progress: \tEpoch 54 [0/1692 (0.00%)]\t\tLoss: 0.96708\n",
            "Training Progress: \tEpoch 54 [320/1692 (18.87%)]\t\tLoss: 1.05885\n",
            "Training Progress: \tEpoch 54 [640/1692 (37.74%)]\t\tLoss: 0.83913\n",
            "Training Progress: \tEpoch 54 [960/1692 (56.60%)]\t\tLoss: 0.69487\n",
            "Training Progress: \tEpoch 54 [1280/1692 (75.47%)]\t\tLoss: 0.76089\n",
            "Training Progress: \tEpoch 54 [1600/1692 (94.34%)]\t\tLoss: 0.93771\n",
            "\tTrain loss: 0.02378, Accuracy: 1217/1692 (71.93%)\n",
            "\tValidation loss: 0.00463, Accuracy: 98/423 (23.17%)\n",
            "\tTest loss: 0.00438, Accuracy: 103/443 (23.25%)\n",
            "\n",
            "Training Progress: \tEpoch 55 [0/1692 (0.00%)]\t\tLoss: 0.90554\n",
            "Training Progress: \tEpoch 55 [320/1692 (18.87%)]\t\tLoss: 0.82623\n",
            "Training Progress: \tEpoch 55 [640/1692 (37.74%)]\t\tLoss: 0.73369\n",
            "Training Progress: \tEpoch 55 [960/1692 (56.60%)]\t\tLoss: 0.95039\n",
            "Training Progress: \tEpoch 55 [1280/1692 (75.47%)]\t\tLoss: 0.91449\n",
            "Training Progress: \tEpoch 55 [1600/1692 (94.34%)]\t\tLoss: 0.83761\n",
            "\tTrain loss: 0.02312, Accuracy: 1227/1692 (72.52%)\n",
            "\tValidation loss: 0.00483, Accuracy: 92/423 (21.75%)\n",
            "\tTest loss: 0.00453, Accuracy: 111/443 (25.06%)\n",
            "\n",
            "Training Progress: \tEpoch 56 [0/1692 (0.00%)]\t\tLoss: 0.92389\n",
            "Training Progress: \tEpoch 56 [320/1692 (18.87%)]\t\tLoss: 0.91474\n",
            "Training Progress: \tEpoch 56 [640/1692 (37.74%)]\t\tLoss: 0.97726\n",
            "Training Progress: \tEpoch 56 [960/1692 (56.60%)]\t\tLoss: 0.77377\n",
            "Training Progress: \tEpoch 56 [1280/1692 (75.47%)]\t\tLoss: 0.70184\n",
            "Training Progress: \tEpoch 56 [1600/1692 (94.34%)]\t\tLoss: 0.84116\n",
            "\tTrain loss: 0.02270, Accuracy: 1225/1692 (72.40%)\n",
            "\tValidation loss: 0.00483, Accuracy: 94/423 (22.22%)\n",
            "\tTest loss: 0.00468, Accuracy: 114/443 (25.73%)\n",
            "\n",
            "Training Progress: \tEpoch 57 [0/1692 (0.00%)]\t\tLoss: 0.96432\n",
            "Training Progress: \tEpoch 57 [320/1692 (18.87%)]\t\tLoss: 1.11698\n",
            "Training Progress: \tEpoch 57 [640/1692 (37.74%)]\t\tLoss: 0.82973\n",
            "Training Progress: \tEpoch 57 [960/1692 (56.60%)]\t\tLoss: 0.73599\n",
            "Training Progress: \tEpoch 57 [1280/1692 (75.47%)]\t\tLoss: 0.68314\n",
            "Training Progress: \tEpoch 57 [1600/1692 (94.34%)]\t\tLoss: 0.95510\n",
            "\tTrain loss: 0.02365, Accuracy: 1193/1692 (70.51%)\n",
            "\tValidation loss: 0.00475, Accuracy: 91/423 (21.51%)\n",
            "\tTest loss: 0.00445, Accuracy: 106/443 (23.93%)\n",
            "\n",
            "Training Progress: \tEpoch 58 [0/1692 (0.00%)]\t\tLoss: 0.98412\n",
            "Training Progress: \tEpoch 58 [320/1692 (18.87%)]\t\tLoss: 0.66157\n",
            "Training Progress: \tEpoch 58 [640/1692 (37.74%)]\t\tLoss: 0.75902\n",
            "Training Progress: \tEpoch 58 [960/1692 (56.60%)]\t\tLoss: 0.88590\n",
            "Training Progress: \tEpoch 58 [1280/1692 (75.47%)]\t\tLoss: 0.90367\n",
            "Training Progress: \tEpoch 58 [1600/1692 (94.34%)]\t\tLoss: 0.77801\n",
            "\tTrain loss: 0.02221, Accuracy: 1254/1692 (74.11%)\n",
            "\tValidation loss: 0.00468, Accuracy: 92/423 (21.75%)\n",
            "\tTest loss: 0.00447, Accuracy: 103/443 (23.25%)\n",
            "\n",
            "Training Progress: \tEpoch 59 [0/1692 (0.00%)]\t\tLoss: 1.05407\n",
            "Training Progress: \tEpoch 59 [320/1692 (18.87%)]\t\tLoss: 0.81839\n",
            "Training Progress: \tEpoch 59 [640/1692 (37.74%)]\t\tLoss: 0.93187\n",
            "Training Progress: \tEpoch 59 [960/1692 (56.60%)]\t\tLoss: 0.85851\n",
            "Training Progress: \tEpoch 59 [1280/1692 (75.47%)]\t\tLoss: 0.68456\n",
            "Training Progress: \tEpoch 59 [1600/1692 (94.34%)]\t\tLoss: 0.74614\n",
            "\tTrain loss: 0.02155, Accuracy: 1265/1692 (74.76%)\n",
            "\tValidation loss: 0.00489, Accuracy: 95/423 (22.46%)\n",
            "\tTest loss: 0.00464, Accuracy: 109/443 (24.60%)\n",
            "\n",
            "Training Progress: \tEpoch 60 [0/1692 (0.00%)]\t\tLoss: 0.86013\n",
            "Training Progress: \tEpoch 60 [320/1692 (18.87%)]\t\tLoss: 0.98989\n",
            "Training Progress: \tEpoch 60 [640/1692 (37.74%)]\t\tLoss: 0.77199\n",
            "Training Progress: \tEpoch 60 [960/1692 (56.60%)]\t\tLoss: 0.88554\n",
            "Training Progress: \tEpoch 60 [1280/1692 (75.47%)]\t\tLoss: 0.56207\n",
            "Training Progress: \tEpoch 60 [1600/1692 (94.34%)]\t\tLoss: 1.03495\n",
            "\tTrain loss: 0.02133, Accuracy: 1271/1692 (75.12%)\n",
            "\tValidation loss: 0.00504, Accuracy: 92/423 (21.75%)\n",
            "\tTest loss: 0.00460, Accuracy: 104/443 (23.48%)\n",
            "\n",
            "Training Progress: \tEpoch 61 [0/1692 (0.00%)]\t\tLoss: 0.86235\n",
            "Training Progress: \tEpoch 61 [320/1692 (18.87%)]\t\tLoss: 0.96075\n",
            "Training Progress: \tEpoch 61 [640/1692 (37.74%)]\t\tLoss: 0.75942\n",
            "Training Progress: \tEpoch 61 [960/1692 (56.60%)]\t\tLoss: 0.96867\n",
            "Training Progress: \tEpoch 61 [1280/1692 (75.47%)]\t\tLoss: 0.69264\n",
            "Training Progress: \tEpoch 61 [1600/1692 (94.34%)]\t\tLoss: 0.73290\n",
            "\tTrain loss: 0.02109, Accuracy: 1253/1692 (74.05%)\n",
            "\tValidation loss: 0.00508, Accuracy: 85/423 (20.09%)\n",
            "\tTest loss: 0.00480, Accuracy: 104/443 (23.48%)\n",
            "\n",
            "Training Progress: \tEpoch 62 [0/1692 (0.00%)]\t\tLoss: 0.72943\n",
            "Training Progress: \tEpoch 62 [320/1692 (18.87%)]\t\tLoss: 0.91750\n",
            "Training Progress: \tEpoch 62 [640/1692 (37.74%)]\t\tLoss: 0.64452\n",
            "Training Progress: \tEpoch 62 [960/1692 (56.60%)]\t\tLoss: 0.89288\n",
            "Training Progress: \tEpoch 62 [1280/1692 (75.47%)]\t\tLoss: 0.88738\n",
            "Training Progress: \tEpoch 62 [1600/1692 (94.34%)]\t\tLoss: 0.78763\n",
            "\tTrain loss: 0.02053, Accuracy: 1301/1692 (76.89%)\n",
            "\tValidation loss: 0.00523, Accuracy: 89/423 (21.04%)\n",
            "\tTest loss: 0.00499, Accuracy: 101/443 (22.80%)\n",
            "\n",
            "Training Progress: \tEpoch 63 [0/1692 (0.00%)]\t\tLoss: 0.90662\n",
            "Training Progress: \tEpoch 63 [320/1692 (18.87%)]\t\tLoss: 0.93062\n",
            "Training Progress: \tEpoch 63 [640/1692 (37.74%)]\t\tLoss: 0.79653\n",
            "Training Progress: \tEpoch 63 [960/1692 (56.60%)]\t\tLoss: 0.78680\n",
            "Training Progress: \tEpoch 63 [1280/1692 (75.47%)]\t\tLoss: 0.56836\n",
            "Training Progress: \tEpoch 63 [1600/1692 (94.34%)]\t\tLoss: 0.72551\n",
            "\tTrain loss: 0.01998, Accuracy: 1290/1692 (76.24%)\n",
            "\tValidation loss: 0.00519, Accuracy: 88/423 (20.80%)\n",
            "\tTest loss: 0.00493, Accuracy: 106/443 (23.93%)\n",
            "\n",
            "Training Progress: \tEpoch 64 [0/1692 (0.00%)]\t\tLoss: 0.83671\n",
            "Training Progress: \tEpoch 64 [320/1692 (18.87%)]\t\tLoss: 0.72216\n",
            "Training Progress: \tEpoch 64 [640/1692 (37.74%)]\t\tLoss: 0.97304\n",
            "Training Progress: \tEpoch 64 [960/1692 (56.60%)]\t\tLoss: 0.82857\n",
            "Training Progress: \tEpoch 64 [1280/1692 (75.47%)]\t\tLoss: 0.70582\n",
            "Training Progress: \tEpoch 64 [1600/1692 (94.34%)]\t\tLoss: 0.60728\n",
            "\tTrain loss: 0.01925, Accuracy: 1324/1692 (78.25%)\n",
            "\tValidation loss: 0.00515, Accuracy: 103/423 (24.35%)\n",
            "\tTest loss: 0.00492, Accuracy: 105/443 (23.70%)\n",
            "\n",
            "Training Progress: \tEpoch 65 [0/1692 (0.00%)]\t\tLoss: 0.68821\n",
            "Training Progress: \tEpoch 65 [320/1692 (18.87%)]\t\tLoss: 0.65769\n",
            "Training Progress: \tEpoch 65 [640/1692 (37.74%)]\t\tLoss: 0.79451\n",
            "Training Progress: \tEpoch 65 [960/1692 (56.60%)]\t\tLoss: 0.75168\n",
            "Training Progress: \tEpoch 65 [1280/1692 (75.47%)]\t\tLoss: 0.61211\n",
            "Training Progress: \tEpoch 65 [1600/1692 (94.34%)]\t\tLoss: 0.84028\n",
            "\tTrain loss: 0.01994, Accuracy: 1287/1692 (76.06%)\n",
            "\tValidation loss: 0.00522, Accuracy: 93/423 (21.99%)\n",
            "\tTest loss: 0.00493, Accuracy: 107/443 (24.15%)\n",
            "\n",
            "Training Progress: \tEpoch 66 [0/1692 (0.00%)]\t\tLoss: 1.03666\n",
            "Training Progress: \tEpoch 66 [320/1692 (18.87%)]\t\tLoss: 0.63966\n",
            "Training Progress: \tEpoch 66 [640/1692 (37.74%)]\t\tLoss: 0.64259\n",
            "Training Progress: \tEpoch 66 [960/1692 (56.60%)]\t\tLoss: 0.73905\n",
            "Training Progress: \tEpoch 66 [1280/1692 (75.47%)]\t\tLoss: 0.64936\n",
            "Training Progress: \tEpoch 66 [1600/1692 (94.34%)]\t\tLoss: 0.69146\n",
            "\tTrain loss: 0.01857, Accuracy: 1335/1692 (78.90%)\n",
            "\tValidation loss: 0.00530, Accuracy: 93/423 (21.99%)\n",
            "\tTest loss: 0.00520, Accuracy: 102/443 (23.02%)\n",
            "\n",
            "Training Progress: \tEpoch 67 [0/1692 (0.00%)]\t\tLoss: 0.59843\n",
            "Training Progress: \tEpoch 67 [320/1692 (18.87%)]\t\tLoss: 0.82405\n",
            "Training Progress: \tEpoch 67 [640/1692 (37.74%)]\t\tLoss: 0.64323\n",
            "Training Progress: \tEpoch 67 [960/1692 (56.60%)]\t\tLoss: 0.60767\n",
            "Training Progress: \tEpoch 67 [1280/1692 (75.47%)]\t\tLoss: 0.55504\n",
            "Training Progress: \tEpoch 67 [1600/1692 (94.34%)]\t\tLoss: 0.63762\n",
            "\tTrain loss: 0.01819, Accuracy: 1335/1692 (78.90%)\n",
            "\tValidation loss: 0.00533, Accuracy: 94/423 (22.22%)\n",
            "\tTest loss: 0.00497, Accuracy: 103/443 (23.25%)\n",
            "\n",
            "Training Progress: \tEpoch 68 [0/1692 (0.00%)]\t\tLoss: 1.01796\n",
            "Training Progress: \tEpoch 68 [320/1692 (18.87%)]\t\tLoss: 0.90369\n",
            "Training Progress: \tEpoch 68 [640/1692 (37.74%)]\t\tLoss: 0.76354\n",
            "Training Progress: \tEpoch 68 [960/1692 (56.60%)]\t\tLoss: 0.78635\n",
            "Training Progress: \tEpoch 68 [1280/1692 (75.47%)]\t\tLoss: 0.42889\n",
            "Training Progress: \tEpoch 68 [1600/1692 (94.34%)]\t\tLoss: 0.65381\n",
            "\tTrain loss: 0.01826, Accuracy: 1349/1692 (79.73%)\n",
            "\tValidation loss: 0.00545, Accuracy: 95/423 (22.46%)\n",
            "\tTest loss: 0.00502, Accuracy: 97/443 (21.90%)\n",
            "\n",
            "Training Progress: \tEpoch 69 [0/1692 (0.00%)]\t\tLoss: 0.82069\n",
            "Training Progress: \tEpoch 69 [320/1692 (18.87%)]\t\tLoss: 1.03344\n",
            "Training Progress: \tEpoch 69 [640/1692 (37.74%)]\t\tLoss: 0.70650\n",
            "Training Progress: \tEpoch 69 [960/1692 (56.60%)]\t\tLoss: 0.60662\n",
            "Training Progress: \tEpoch 69 [1280/1692 (75.47%)]\t\tLoss: 0.66420\n",
            "Training Progress: \tEpoch 69 [1600/1692 (94.34%)]\t\tLoss: 0.61551\n",
            "\tTrain loss: 0.01751, Accuracy: 1326/1692 (78.37%)\n",
            "\tValidation loss: 0.00560, Accuracy: 98/423 (23.17%)\n",
            "\tTest loss: 0.00531, Accuracy: 99/443 (22.35%)\n",
            "\n",
            "Training Progress: \tEpoch 70 [0/1692 (0.00%)]\t\tLoss: 0.74492\n",
            "Training Progress: \tEpoch 70 [320/1692 (18.87%)]\t\tLoss: 0.79379\n",
            "Training Progress: \tEpoch 70 [640/1692 (37.74%)]\t\tLoss: 0.64952\n",
            "Training Progress: \tEpoch 70 [960/1692 (56.60%)]\t\tLoss: 0.65741\n",
            "Training Progress: \tEpoch 70 [1280/1692 (75.47%)]\t\tLoss: 0.58028\n",
            "Training Progress: \tEpoch 70 [1600/1692 (94.34%)]\t\tLoss: 0.58941\n",
            "\tTrain loss: 0.01670, Accuracy: 1375/1692 (81.26%)\n",
            "\tValidation loss: 0.00539, Accuracy: 89/423 (21.04%)\n",
            "\tTest loss: 0.00519, Accuracy: 110/443 (24.83%)\n",
            "\n",
            "Training Progress: \tEpoch 71 [0/1692 (0.00%)]\t\tLoss: 0.75607\n",
            "Training Progress: \tEpoch 71 [320/1692 (18.87%)]\t\tLoss: 0.72558\n",
            "Training Progress: \tEpoch 71 [640/1692 (37.74%)]\t\tLoss: 0.91218\n",
            "Training Progress: \tEpoch 71 [960/1692 (56.60%)]\t\tLoss: 0.64701\n",
            "Training Progress: \tEpoch 71 [1280/1692 (75.47%)]\t\tLoss: 0.60003\n",
            "Training Progress: \tEpoch 71 [1600/1692 (94.34%)]\t\tLoss: 0.54869\n",
            "\tTrain loss: 0.01779, Accuracy: 1330/1692 (78.61%)\n",
            "\tValidation loss: 0.00545, Accuracy: 100/423 (23.64%)\n",
            "\tTest loss: 0.00513, Accuracy: 105/443 (23.70%)\n",
            "\n",
            "Training Progress: \tEpoch 72 [0/1692 (0.00%)]\t\tLoss: 0.89846\n",
            "Training Progress: \tEpoch 72 [320/1692 (18.87%)]\t\tLoss: 0.78823\n",
            "Training Progress: \tEpoch 72 [640/1692 (37.74%)]\t\tLoss: 0.69054\n",
            "Training Progress: \tEpoch 72 [960/1692 (56.60%)]\t\tLoss: 0.76807\n",
            "Training Progress: \tEpoch 72 [1280/1692 (75.47%)]\t\tLoss: 0.61835\n",
            "Training Progress: \tEpoch 72 [1600/1692 (94.34%)]\t\tLoss: 0.74507\n",
            "\tTrain loss: 0.01761, Accuracy: 1341/1692 (79.26%)\n",
            "\tValidation loss: 0.00542, Accuracy: 95/423 (22.46%)\n",
            "\tTest loss: 0.00507, Accuracy: 99/443 (22.35%)\n",
            "\n",
            "Training Progress: \tEpoch 73 [0/1692 (0.00%)]\t\tLoss: 0.69132\n",
            "Training Progress: \tEpoch 73 [320/1692 (18.87%)]\t\tLoss: 0.69769\n",
            "Training Progress: \tEpoch 73 [640/1692 (37.74%)]\t\tLoss: 0.49881\n",
            "Training Progress: \tEpoch 73 [960/1692 (56.60%)]\t\tLoss: 0.61599\n",
            "Training Progress: \tEpoch 73 [1280/1692 (75.47%)]\t\tLoss: 0.74610\n",
            "Training Progress: \tEpoch 73 [1600/1692 (94.34%)]\t\tLoss: 0.88453\n",
            "\tTrain loss: 0.01652, Accuracy: 1372/1692 (81.09%)\n",
            "\tValidation loss: 0.00561, Accuracy: 86/423 (20.33%)\n",
            "\tTest loss: 0.00538, Accuracy: 104/443 (23.48%)\n",
            "\n",
            "Training Progress: \tEpoch 74 [0/1692 (0.00%)]\t\tLoss: 0.88999\n",
            "Training Progress: \tEpoch 74 [320/1692 (18.87%)]\t\tLoss: 0.54742\n",
            "Training Progress: \tEpoch 74 [640/1692 (37.74%)]\t\tLoss: 0.60296\n",
            "Training Progress: \tEpoch 74 [960/1692 (56.60%)]\t\tLoss: 0.62778\n",
            "Training Progress: \tEpoch 74 [1280/1692 (75.47%)]\t\tLoss: 0.55368\n",
            "Training Progress: \tEpoch 74 [1600/1692 (94.34%)]\t\tLoss: 0.55357\n",
            "\tTrain loss: 0.01589, Accuracy: 1392/1692 (82.27%)\n",
            "\tValidation loss: 0.00581, Accuracy: 86/423 (20.33%)\n",
            "\tTest loss: 0.00560, Accuracy: 94/443 (21.22%)\n",
            "\n",
            "Training Progress: \tEpoch 75 [0/1692 (0.00%)]\t\tLoss: 0.65288\n",
            "Training Progress: \tEpoch 75 [320/1692 (18.87%)]\t\tLoss: 0.82721\n",
            "Training Progress: \tEpoch 75 [640/1692 (37.74%)]\t\tLoss: 0.52720\n",
            "Training Progress: \tEpoch 75 [960/1692 (56.60%)]\t\tLoss: 0.71861\n",
            "Training Progress: \tEpoch 75 [1280/1692 (75.47%)]\t\tLoss: 0.64065\n",
            "Training Progress: \tEpoch 75 [1600/1692 (94.34%)]\t\tLoss: 0.67621\n",
            "\tTrain loss: 0.01590, Accuracy: 1385/1692 (81.86%)\n",
            "\tValidation loss: 0.00572, Accuracy: 99/423 (23.40%)\n",
            "\tTest loss: 0.00554, Accuracy: 100/443 (22.57%)\n",
            "\n",
            "Training Progress: \tEpoch 76 [0/1692 (0.00%)]\t\tLoss: 0.63626\n",
            "Training Progress: \tEpoch 76 [320/1692 (18.87%)]\t\tLoss: 0.99198\n",
            "Training Progress: \tEpoch 76 [640/1692 (37.74%)]\t\tLoss: 0.64789\n",
            "Training Progress: \tEpoch 76 [960/1692 (56.60%)]\t\tLoss: 0.66867\n",
            "Training Progress: \tEpoch 76 [1280/1692 (75.47%)]\t\tLoss: 0.56715\n",
            "Training Progress: \tEpoch 76 [1600/1692 (94.34%)]\t\tLoss: 0.65033\n",
            "\tTrain loss: 0.01565, Accuracy: 1399/1692 (82.68%)\n",
            "\tValidation loss: 0.00564, Accuracy: 91/423 (21.51%)\n",
            "\tTest loss: 0.00543, Accuracy: 103/443 (23.25%)\n",
            "\n",
            "Training Progress: \tEpoch 77 [0/1692 (0.00%)]\t\tLoss: 0.58311\n",
            "Training Progress: \tEpoch 77 [320/1692 (18.87%)]\t\tLoss: 0.62253\n",
            "Training Progress: \tEpoch 77 [640/1692 (37.74%)]\t\tLoss: 0.75814\n",
            "Training Progress: \tEpoch 77 [960/1692 (56.60%)]\t\tLoss: 0.69773\n",
            "Training Progress: \tEpoch 77 [1280/1692 (75.47%)]\t\tLoss: 0.53253\n",
            "Training Progress: \tEpoch 77 [1600/1692 (94.34%)]\t\tLoss: 0.50669\n",
            "\tTrain loss: 0.01526, Accuracy: 1394/1692 (82.39%)\n",
            "\tValidation loss: 0.00599, Accuracy: 93/423 (21.99%)\n",
            "\tTest loss: 0.00565, Accuracy: 98/443 (22.12%)\n",
            "\n",
            "Training Progress: \tEpoch 78 [0/1692 (0.00%)]\t\tLoss: 0.72317\n",
            "Training Progress: \tEpoch 78 [320/1692 (18.87%)]\t\tLoss: 0.69859\n",
            "Training Progress: \tEpoch 78 [640/1692 (37.74%)]\t\tLoss: 0.48579\n",
            "Training Progress: \tEpoch 78 [960/1692 (56.60%)]\t\tLoss: 0.61355\n",
            "Training Progress: \tEpoch 78 [1280/1692 (75.47%)]\t\tLoss: 0.76581\n",
            "Training Progress: \tEpoch 78 [1600/1692 (94.34%)]\t\tLoss: 0.53584\n",
            "\tTrain loss: 0.01540, Accuracy: 1386/1692 (81.91%)\n",
            "\tValidation loss: 0.00586, Accuracy: 89/423 (21.04%)\n",
            "\tTest loss: 0.00545, Accuracy: 111/443 (25.06%)\n",
            "\n",
            "Training Progress: \tEpoch 79 [0/1692 (0.00%)]\t\tLoss: 0.78081\n",
            "Training Progress: \tEpoch 79 [320/1692 (18.87%)]\t\tLoss: 0.64927\n",
            "Training Progress: \tEpoch 79 [640/1692 (37.74%)]\t\tLoss: 0.50876\n",
            "Training Progress: \tEpoch 79 [960/1692 (56.60%)]\t\tLoss: 0.74949\n",
            "Training Progress: \tEpoch 79 [1280/1692 (75.47%)]\t\tLoss: 0.48811\n",
            "Training Progress: \tEpoch 79 [1600/1692 (94.34%)]\t\tLoss: 0.57289\n",
            "\tTrain loss: 0.01504, Accuracy: 1416/1692 (83.69%)\n",
            "\tValidation loss: 0.00578, Accuracy: 100/423 (23.64%)\n",
            "\tTest loss: 0.00556, Accuracy: 95/443 (21.44%)\n",
            "\n",
            "Training Progress: \tEpoch 80 [0/1692 (0.00%)]\t\tLoss: 0.77581\n",
            "Training Progress: \tEpoch 80 [320/1692 (18.87%)]\t\tLoss: 0.73851\n",
            "Training Progress: \tEpoch 80 [640/1692 (37.74%)]\t\tLoss: 0.57590\n",
            "Training Progress: \tEpoch 80 [960/1692 (56.60%)]\t\tLoss: 0.56251\n",
            "Training Progress: \tEpoch 80 [1280/1692 (75.47%)]\t\tLoss: 0.47599\n",
            "Training Progress: \tEpoch 80 [1600/1692 (94.34%)]\t\tLoss: 0.76292\n",
            "\tTrain loss: 0.01376, Accuracy: 1441/1692 (85.17%)\n",
            "\tValidation loss: 0.00606, Accuracy: 96/423 (22.70%)\n",
            "\tTest loss: 0.00569, Accuracy: 97/443 (21.90%)\n",
            "\n",
            "Training Progress: \tEpoch 81 [0/1692 (0.00%)]\t\tLoss: 0.78151\n",
            "Training Progress: \tEpoch 81 [320/1692 (18.87%)]\t\tLoss: 0.88428\n",
            "Training Progress: \tEpoch 81 [640/1692 (37.74%)]\t\tLoss: 0.77889\n",
            "Training Progress: \tEpoch 81 [960/1692 (56.60%)]\t\tLoss: 0.76201\n",
            "Training Progress: \tEpoch 81 [1280/1692 (75.47%)]\t\tLoss: 0.47120\n",
            "Training Progress: \tEpoch 81 [1600/1692 (94.34%)]\t\tLoss: 0.53279\n",
            "\tTrain loss: 0.01432, Accuracy: 1420/1692 (83.92%)\n",
            "\tValidation loss: 0.00597, Accuracy: 87/423 (20.57%)\n",
            "\tTest loss: 0.00559, Accuracy: 104/443 (23.48%)\n",
            "\n",
            "Training Progress: \tEpoch 82 [0/1692 (0.00%)]\t\tLoss: 0.70950\n",
            "Training Progress: \tEpoch 82 [320/1692 (18.87%)]\t\tLoss: 0.56089\n",
            "Training Progress: \tEpoch 82 [640/1692 (37.74%)]\t\tLoss: 0.73665\n",
            "Training Progress: \tEpoch 82 [960/1692 (56.60%)]\t\tLoss: 0.57687\n",
            "Training Progress: \tEpoch 82 [1280/1692 (75.47%)]\t\tLoss: 0.49567\n",
            "Training Progress: \tEpoch 82 [1600/1692 (94.34%)]\t\tLoss: 0.57652\n",
            "\tTrain loss: 0.01321, Accuracy: 1444/1692 (85.34%)\n",
            "\tValidation loss: 0.00596, Accuracy: 81/423 (19.15%)\n",
            "\tTest loss: 0.00570, Accuracy: 101/443 (22.80%)\n",
            "\n",
            "Training Progress: \tEpoch 83 [0/1692 (0.00%)]\t\tLoss: 0.58191\n",
            "Training Progress: \tEpoch 83 [320/1692 (18.87%)]\t\tLoss: 0.95867\n",
            "Training Progress: \tEpoch 83 [640/1692 (37.74%)]\t\tLoss: 0.68114\n",
            "Training Progress: \tEpoch 83 [960/1692 (56.60%)]\t\tLoss: 0.76927\n",
            "Training Progress: \tEpoch 83 [1280/1692 (75.47%)]\t\tLoss: 0.36854\n",
            "Training Progress: \tEpoch 83 [1600/1692 (94.34%)]\t\tLoss: 0.40315\n",
            "\tTrain loss: 0.01397, Accuracy: 1443/1692 (85.28%)\n",
            "\tValidation loss: 0.00617, Accuracy: 91/423 (21.51%)\n",
            "\tTest loss: 0.00580, Accuracy: 99/443 (22.35%)\n",
            "\n",
            "Training Progress: \tEpoch 84 [0/1692 (0.00%)]\t\tLoss: 1.02411\n",
            "Training Progress: \tEpoch 84 [320/1692 (18.87%)]\t\tLoss: 0.69971\n",
            "Training Progress: \tEpoch 84 [640/1692 (37.74%)]\t\tLoss: 0.50624\n",
            "Training Progress: \tEpoch 84 [960/1692 (56.60%)]\t\tLoss: 0.49102\n",
            "Training Progress: \tEpoch 84 [1280/1692 (75.47%)]\t\tLoss: 0.53132\n",
            "Training Progress: \tEpoch 84 [1600/1692 (94.34%)]\t\tLoss: 0.48089\n",
            "\tTrain loss: 0.01325, Accuracy: 1438/1692 (84.99%)\n",
            "\tValidation loss: 0.00608, Accuracy: 95/423 (22.46%)\n",
            "\tTest loss: 0.00569, Accuracy: 105/443 (23.70%)\n",
            "\n",
            "Training Progress: \tEpoch 85 [0/1692 (0.00%)]\t\tLoss: 0.76663\n",
            "Training Progress: \tEpoch 85 [320/1692 (18.87%)]\t\tLoss: 0.49393\n",
            "Training Progress: \tEpoch 85 [640/1692 (37.74%)]\t\tLoss: 0.54653\n",
            "Training Progress: \tEpoch 85 [960/1692 (56.60%)]\t\tLoss: 0.53490\n",
            "Training Progress: \tEpoch 85 [1280/1692 (75.47%)]\t\tLoss: 0.33933\n",
            "Training Progress: \tEpoch 85 [1600/1692 (94.34%)]\t\tLoss: 0.33336\n",
            "\tTrain loss: 0.01338, Accuracy: 1434/1692 (84.75%)\n",
            "\tValidation loss: 0.00614, Accuracy: 91/423 (21.51%)\n",
            "\tTest loss: 0.00575, Accuracy: 107/443 (24.15%)\n",
            "\n",
            "Training Progress: \tEpoch 86 [0/1692 (0.00%)]\t\tLoss: 0.53842\n",
            "Training Progress: \tEpoch 86 [320/1692 (18.87%)]\t\tLoss: 0.77109\n",
            "Training Progress: \tEpoch 86 [640/1692 (37.74%)]\t\tLoss: 0.42488\n",
            "Training Progress: \tEpoch 86 [960/1692 (56.60%)]\t\tLoss: 0.51318\n",
            "Training Progress: \tEpoch 86 [1280/1692 (75.47%)]\t\tLoss: 0.31785\n",
            "Training Progress: \tEpoch 86 [1600/1692 (94.34%)]\t\tLoss: 0.46870\n",
            "\tTrain loss: 0.01315, Accuracy: 1443/1692 (85.28%)\n",
            "\tValidation loss: 0.00639, Accuracy: 88/423 (20.80%)\n",
            "\tTest loss: 0.00599, Accuracy: 99/443 (22.35%)\n",
            "\n",
            "Training Progress: \tEpoch 87 [0/1692 (0.00%)]\t\tLoss: 0.56489\n",
            "Training Progress: \tEpoch 87 [320/1692 (18.87%)]\t\tLoss: 0.70386\n",
            "Training Progress: \tEpoch 87 [640/1692 (37.74%)]\t\tLoss: 0.70865\n",
            "Training Progress: \tEpoch 87 [960/1692 (56.60%)]\t\tLoss: 0.53675\n",
            "Training Progress: \tEpoch 87 [1280/1692 (75.47%)]\t\tLoss: 0.29863\n",
            "Training Progress: \tEpoch 87 [1600/1692 (94.34%)]\t\tLoss: 0.38759\n",
            "\tTrain loss: 0.01263, Accuracy: 1445/1692 (85.40%)\n",
            "\tValidation loss: 0.00639, Accuracy: 89/423 (21.04%)\n",
            "\tTest loss: 0.00587, Accuracy: 109/443 (24.60%)\n",
            "\n",
            "Training Progress: \tEpoch 88 [0/1692 (0.00%)]\t\tLoss: 0.41373\n",
            "Training Progress: \tEpoch 88 [320/1692 (18.87%)]\t\tLoss: 0.56334\n",
            "Training Progress: \tEpoch 88 [640/1692 (37.74%)]\t\tLoss: 0.39305\n",
            "Training Progress: \tEpoch 88 [960/1692 (56.60%)]\t\tLoss: 0.77124\n",
            "Training Progress: \tEpoch 88 [1280/1692 (75.47%)]\t\tLoss: 0.48079\n",
            "Training Progress: \tEpoch 88 [1600/1692 (94.34%)]\t\tLoss: 0.47145\n",
            "\tTrain loss: 0.01152, Accuracy: 1481/1692 (87.53%)\n",
            "\tValidation loss: 0.00641, Accuracy: 93/423 (21.99%)\n",
            "\tTest loss: 0.00592, Accuracy: 111/443 (25.06%)\n",
            "\n",
            "Training Progress: \tEpoch 89 [0/1692 (0.00%)]\t\tLoss: 0.50094\n",
            "Training Progress: \tEpoch 89 [320/1692 (18.87%)]\t\tLoss: 0.60647\n",
            "Training Progress: \tEpoch 89 [640/1692 (37.74%)]\t\tLoss: 0.47068\n",
            "Training Progress: \tEpoch 89 [960/1692 (56.60%)]\t\tLoss: 0.68090\n",
            "Training Progress: \tEpoch 89 [1280/1692 (75.47%)]\t\tLoss: 0.45833\n",
            "Training Progress: \tEpoch 89 [1600/1692 (94.34%)]\t\tLoss: 0.44609\n",
            "\tTrain loss: 0.01182, Accuracy: 1470/1692 (86.88%)\n",
            "\tValidation loss: 0.00658, Accuracy: 93/423 (21.99%)\n",
            "\tTest loss: 0.00590, Accuracy: 111/443 (25.06%)\n",
            "\n",
            "Training Progress: \tEpoch 90 [0/1692 (0.00%)]\t\tLoss: 0.54690\n",
            "Training Progress: \tEpoch 90 [320/1692 (18.87%)]\t\tLoss: 0.56783\n",
            "Training Progress: \tEpoch 90 [640/1692 (37.74%)]\t\tLoss: 0.45947\n",
            "Training Progress: \tEpoch 90 [960/1692 (56.60%)]\t\tLoss: 0.69735\n",
            "Training Progress: \tEpoch 90 [1280/1692 (75.47%)]\t\tLoss: 0.41570\n",
            "Training Progress: \tEpoch 90 [1600/1692 (94.34%)]\t\tLoss: 0.43087\n",
            "\tTrain loss: 0.01231, Accuracy: 1457/1692 (86.11%)\n",
            "\tValidation loss: 0.00652, Accuracy: 91/423 (21.51%)\n",
            "\tTest loss: 0.00612, Accuracy: 103/443 (23.25%)\n",
            "\n",
            "Training Progress: \tEpoch 91 [0/1692 (0.00%)]\t\tLoss: 0.50772\n",
            "Training Progress: \tEpoch 91 [320/1692 (18.87%)]\t\tLoss: 0.60367\n",
            "Training Progress: \tEpoch 91 [640/1692 (37.74%)]\t\tLoss: 0.61755\n",
            "Training Progress: \tEpoch 91 [960/1692 (56.60%)]\t\tLoss: 0.59637\n",
            "Training Progress: \tEpoch 91 [1280/1692 (75.47%)]\t\tLoss: 0.40121\n",
            "Training Progress: \tEpoch 91 [1600/1692 (94.34%)]\t\tLoss: 0.41856\n",
            "\tTrain loss: 0.01167, Accuracy: 1475/1692 (87.17%)\n",
            "\tValidation loss: 0.00635, Accuracy: 97/423 (22.93%)\n",
            "\tTest loss: 0.00600, Accuracy: 102/443 (23.02%)\n",
            "\n",
            "Training Progress: \tEpoch 92 [0/1692 (0.00%)]\t\tLoss: 0.37202\n",
            "Training Progress: \tEpoch 92 [320/1692 (18.87%)]\t\tLoss: 0.62673\n",
            "Training Progress: \tEpoch 92 [640/1692 (37.74%)]\t\tLoss: 0.35722\n",
            "Training Progress: \tEpoch 92 [960/1692 (56.60%)]\t\tLoss: 0.41768\n",
            "Training Progress: \tEpoch 92 [1280/1692 (75.47%)]\t\tLoss: 0.41854\n",
            "Training Progress: \tEpoch 92 [1600/1692 (94.34%)]\t\tLoss: 0.43423\n",
            "\tTrain loss: 0.01151, Accuracy: 1486/1692 (87.83%)\n",
            "\tValidation loss: 0.00640, Accuracy: 101/423 (23.88%)\n",
            "\tTest loss: 0.00617, Accuracy: 99/443 (22.35%)\n",
            "\n",
            "Training Progress: \tEpoch 93 [0/1692 (0.00%)]\t\tLoss: 0.66824\n",
            "Training Progress: \tEpoch 93 [320/1692 (18.87%)]\t\tLoss: 0.51991\n",
            "Training Progress: \tEpoch 93 [640/1692 (37.74%)]\t\tLoss: 0.62412\n",
            "Training Progress: \tEpoch 93 [960/1692 (56.60%)]\t\tLoss: 0.72117\n",
            "Training Progress: \tEpoch 93 [1280/1692 (75.47%)]\t\tLoss: 0.51718\n",
            "Training Progress: \tEpoch 93 [1600/1692 (94.34%)]\t\tLoss: 0.53061\n",
            "\tTrain loss: 0.01024, Accuracy: 1515/1692 (89.54%)\n",
            "\tValidation loss: 0.00643, Accuracy: 91/423 (21.51%)\n",
            "\tTest loss: 0.00619, Accuracy: 104/443 (23.48%)\n",
            "\n",
            "Training Progress: \tEpoch 94 [0/1692 (0.00%)]\t\tLoss: 0.64781\n",
            "Training Progress: \tEpoch 94 [320/1692 (18.87%)]\t\tLoss: 0.56311\n",
            "Training Progress: \tEpoch 94 [640/1692 (37.74%)]\t\tLoss: 0.51796\n",
            "Training Progress: \tEpoch 94 [960/1692 (56.60%)]\t\tLoss: 0.67904\n",
            "Training Progress: \tEpoch 94 [1280/1692 (75.47%)]\t\tLoss: 0.24269\n",
            "Training Progress: \tEpoch 94 [1600/1692 (94.34%)]\t\tLoss: 0.48247\n",
            "\tTrain loss: 0.01095, Accuracy: 1496/1692 (88.42%)\n",
            "\tValidation loss: 0.00642, Accuracy: 92/423 (21.75%)\n",
            "\tTest loss: 0.00608, Accuracy: 105/443 (23.70%)\n",
            "\n",
            "Training Progress: \tEpoch 95 [0/1692 (0.00%)]\t\tLoss: 0.70651\n",
            "Training Progress: \tEpoch 95 [320/1692 (18.87%)]\t\tLoss: 0.74749\n",
            "Training Progress: \tEpoch 95 [640/1692 (37.74%)]\t\tLoss: 0.34706\n",
            "Training Progress: \tEpoch 95 [960/1692 (56.60%)]\t\tLoss: 0.55591\n",
            "Training Progress: \tEpoch 95 [1280/1692 (75.47%)]\t\tLoss: 0.53402\n",
            "Training Progress: \tEpoch 95 [1600/1692 (94.34%)]\t\tLoss: 0.38989\n",
            "\tTrain loss: 0.01191, Accuracy: 1475/1692 (87.17%)\n",
            "\tValidation loss: 0.00656, Accuracy: 91/423 (21.51%)\n",
            "\tTest loss: 0.00633, Accuracy: 104/443 (23.48%)\n",
            "\n",
            "Training Progress: \tEpoch 96 [0/1692 (0.00%)]\t\tLoss: 0.54124\n",
            "Training Progress: \tEpoch 96 [320/1692 (18.87%)]\t\tLoss: 0.49125\n",
            "Training Progress: \tEpoch 96 [640/1692 (37.74%)]\t\tLoss: 0.52939\n",
            "Training Progress: \tEpoch 96 [960/1692 (56.60%)]\t\tLoss: 0.60257\n",
            "Training Progress: \tEpoch 96 [1280/1692 (75.47%)]\t\tLoss: 0.22654\n",
            "Training Progress: \tEpoch 96 [1600/1692 (94.34%)]\t\tLoss: 0.45402\n",
            "\tTrain loss: 0.00991, Accuracy: 1509/1692 (89.18%)\n",
            "\tValidation loss: 0.00673, Accuracy: 88/423 (20.80%)\n",
            "\tTest loss: 0.00657, Accuracy: 107/443 (24.15%)\n",
            "\n",
            "Training Progress: \tEpoch 97 [0/1692 (0.00%)]\t\tLoss: 0.74537\n",
            "Training Progress: \tEpoch 97 [320/1692 (18.87%)]\t\tLoss: 0.51065\n",
            "Training Progress: \tEpoch 97 [640/1692 (37.74%)]\t\tLoss: 0.55128\n",
            "Training Progress: \tEpoch 97 [960/1692 (56.60%)]\t\tLoss: 0.45789\n",
            "Training Progress: \tEpoch 97 [1280/1692 (75.47%)]\t\tLoss: 0.23796\n",
            "Training Progress: \tEpoch 97 [1600/1692 (94.34%)]\t\tLoss: 0.38746\n",
            "\tTrain loss: 0.01068, Accuracy: 1494/1692 (88.30%)\n",
            "\tValidation loss: 0.00658, Accuracy: 92/423 (21.75%)\n",
            "\tTest loss: 0.00632, Accuracy: 102/443 (23.02%)\n",
            "\n",
            "Training Progress: \tEpoch 98 [0/1692 (0.00%)]\t\tLoss: 0.68309\n",
            "Training Progress: \tEpoch 98 [320/1692 (18.87%)]\t\tLoss: 0.52305\n",
            "Training Progress: \tEpoch 98 [640/1692 (37.74%)]\t\tLoss: 0.66401\n",
            "Training Progress: \tEpoch 98 [960/1692 (56.60%)]\t\tLoss: 0.48252\n",
            "Training Progress: \tEpoch 98 [1280/1692 (75.47%)]\t\tLoss: 0.40440\n",
            "Training Progress: \tEpoch 98 [1600/1692 (94.34%)]\t\tLoss: 0.41853\n",
            "\tTrain loss: 0.00999, Accuracy: 1508/1692 (89.13%)\n",
            "\tValidation loss: 0.00693, Accuracy: 98/423 (23.17%)\n",
            "\tTest loss: 0.00663, Accuracy: 101/443 (22.80%)\n",
            "\n",
            "Training Progress: \tEpoch 99 [0/1692 (0.00%)]\t\tLoss: 0.46019\n",
            "Training Progress: \tEpoch 99 [320/1692 (18.87%)]\t\tLoss: 0.50679\n",
            "Training Progress: \tEpoch 99 [640/1692 (37.74%)]\t\tLoss: 0.30388\n",
            "Training Progress: \tEpoch 99 [960/1692 (56.60%)]\t\tLoss: 0.70428\n",
            "Training Progress: \tEpoch 99 [1280/1692 (75.47%)]\t\tLoss: 0.28566\n",
            "Training Progress: \tEpoch 99 [1600/1692 (94.34%)]\t\tLoss: 0.58687\n",
            "\tTrain loss: 0.00953, Accuracy: 1520/1692 (89.83%)\n",
            "\tValidation loss: 0.00697, Accuracy: 87/423 (20.57%)\n",
            "\tTest loss: 0.00650, Accuracy: 108/443 (24.38%)\n",
            "\n",
            "Training Progress: \tEpoch 100 [0/1692 (0.00%)]\t\tLoss: 0.41837\n",
            "Training Progress: \tEpoch 100 [320/1692 (18.87%)]\t\tLoss: 0.52584\n",
            "Training Progress: \tEpoch 100 [640/1692 (37.74%)]\t\tLoss: 0.62034\n",
            "Training Progress: \tEpoch 100 [960/1692 (56.60%)]\t\tLoss: 0.45587\n",
            "Training Progress: \tEpoch 100 [1280/1692 (75.47%)]\t\tLoss: 0.34882\n",
            "Training Progress: \tEpoch 100 [1600/1692 (94.34%)]\t\tLoss: 0.34536\n",
            "\tTrain loss: 0.01008, Accuracy: 1510/1692 (89.24%)\n",
            "\tValidation loss: 0.00681, Accuracy: 98/423 (23.17%)\n",
            "\tTest loss: 0.00654, Accuracy: 106/443 (23.93%)\n",
            "\n",
            "[[tensor(0.0432), 0.2848699763593381], [tensor(0.0430), 0.30969267139479906], [tensor(0.0428), 0.32742316784869974], [tensor(0.0427), 0.32387706855791965], [tensor(0.0426), 0.32860520094562645], [tensor(0.0425), 0.3380614657210402], [tensor(0.0424), 0.33274231678487], [tensor(0.0422), 0.33747044917257685], [tensor(0.0421), 0.35224586288416077], [tensor(0.0419), 0.36465721040189125], [tensor(0.0417), 0.36997635933806144], [tensor(0.0414), 0.38534278959810875], [tensor(0.0411), 0.3871158392434988], [tensor(0.0411), 0.39184397163120566], [tensor(0.0407), 0.40425531914893614], [tensor(0.0404), 0.4166666666666667], [tensor(0.0400), 0.4219858156028369], [tensor(0.0398), 0.4190307328605201], [tensor(0.0396), 0.4385342789598109], [tensor(0.0392), 0.4639479905437352], [tensor(0.0390), 0.4391252955082742], [tensor(0.0384), 0.45449172576832153], [tensor(0.0380), 0.47044917257683216], [tensor(0.0378), 0.46867612293144206], [tensor(0.0375), 0.483451536643026], [tensor(0.0370), 0.4858156028368794], [tensor(0.0365), 0.5059101654846335], [tensor(0.0366), 0.5035460992907801], [tensor(0.0358), 0.5094562647754137], [tensor(0.0355), 0.5124113475177305], [tensor(0.0350), 0.533096926713948], [tensor(0.0348), 0.5224586288416075], [tensor(0.0344), 0.5401891252955082], [tensor(0.0335), 0.5466903073286052], [tensor(0.0329), 0.5537825059101655], [tensor(0.0327), 0.5608747044917257], [tensor(0.0319), 0.58451536643026], [tensor(0.0318), 0.5851063829787234], [tensor(0.0311), 0.609338061465721], [tensor(0.0307), 0.6152482269503546], [tensor(0.0300), 0.6235224586288416], [tensor(0.0298), 0.6146572104018913], [tensor(0.0293), 0.6229314420803782], [tensor(0.0290), 0.6394799054373522], [tensor(0.0288), 0.6483451536643026], [tensor(0.0276), 0.6459810874704491], [tensor(0.0265), 0.6773049645390071], [tensor(0.0264), 0.6601654846335697], [tensor(0.0262), 0.6778959810874704], [tensor(0.0256), 0.6826241134751773], [tensor(0.0253), 0.6926713947990544], [tensor(0.0245), 0.7121749408983451], [tensor(0.0245), 0.7015366430260047], [tensor(0.0238), 0.7192671394799054], [tensor(0.0231), 0.725177304964539], [tensor(0.0227), 0.7239952718676123], [tensor(0.0236), 0.7050827423167849], [tensor(0.0222), 0.7411347517730497], [tensor(0.0216), 0.7476359338061466], [tensor(0.0213), 0.7511820330969267], [tensor(0.0211), 0.7405437352245863], [tensor(0.0205), 0.7689125295508275], [tensor(0.0200), 0.7624113475177305], [tensor(0.0192), 0.7825059101654847], [tensor(0.0199), 0.7606382978723404], [tensor(0.0186), 0.7890070921985816], [tensor(0.0182), 0.7890070921985816], [tensor(0.0183), 0.7972813238770685], [tensor(0.0175), 0.7836879432624113], [tensor(0.0167), 0.8126477541371159], [tensor(0.0178), 0.7860520094562647], [tensor(0.0176), 0.7925531914893617], [tensor(0.0165), 0.8108747044917257], [tensor(0.0159), 0.8226950354609929], [tensor(0.0159), 0.8185579196217494], [tensor(0.0157), 0.8268321513002365], [tensor(0.0153), 0.8238770685579196], [tensor(0.0154), 0.8191489361702128], [tensor(0.0150), 0.8368794326241135], [tensor(0.0138), 0.8516548463356974], [tensor(0.0143), 0.8392434988179669], [tensor(0.0132), 0.8534278959810875], [tensor(0.0140), 0.8528368794326241], [tensor(0.0133), 0.8498817966903073], [tensor(0.0134), 0.8475177304964538], [tensor(0.0132), 0.8528368794326241], [tensor(0.0126), 0.8540189125295509], [tensor(0.0115), 0.8752955082742316], [tensor(0.0118), 0.8687943262411347], [tensor(0.0123), 0.8611111111111112], [tensor(0.0117), 0.8717494089834515], [tensor(0.0115), 0.8782505910165485], [tensor(0.0102), 0.8953900709219859], [tensor(0.0110), 0.8841607565011821], [tensor(0.0119), 0.8717494089834515], [tensor(0.0099), 0.8918439716312057], [tensor(0.0107), 0.8829787234042553], [tensor(0.0100), 0.8912529550827423], [tensor(0.0095), 0.8983451536643026], [tensor(0.0101), 0.892434988179669]]\n",
            "<class 'list'>\n",
            "[[tensor(0.0033), 0.2293144208037825], [tensor(0.0033), 0.2576832151300236], [tensor(0.0033), 0.24822695035460993], [tensor(0.0033), 0.23167848699763594], [tensor(0.0033), 0.2293144208037825], [tensor(0.0033), 0.21513002364066194], [tensor(0.0033), 0.24822695035460993], [tensor(0.0034), 0.24349881796690306], [tensor(0.0033), 0.24113475177304963], [tensor(0.0034), 0.23404255319148937], [tensor(0.0034), 0.2624113475177305], [tensor(0.0034), 0.2647754137115839], [tensor(0.0034), 0.25059101654846333], [tensor(0.0034), 0.25059101654846333], [tensor(0.0034), 0.26713947990543735], [tensor(0.0034), 0.2553191489361702], [tensor(0.0035), 0.24349881796690306], [tensor(0.0035), 0.23877068557919623], [tensor(0.0034), 0.24822695035460993], [tensor(0.0035), 0.26004728132387706], [tensor(0.0035), 0.26004728132387706], [tensor(0.0035), 0.25059101654846333], [tensor(0.0035), 0.2458628841607565], [tensor(0.0035), 0.23877068557919623], [tensor(0.0036), 0.23404255319148937], [tensor(0.0036), 0.22458628841607564], [tensor(0.0036), 0.23404255319148937], [tensor(0.0036), 0.22695035460992907], [tensor(0.0037), 0.2293144208037825], [tensor(0.0037), 0.2127659574468085], [tensor(0.0037), 0.22458628841607564], [tensor(0.0038), 0.21040189125295508], [tensor(0.0037), 0.2198581560283688], [tensor(0.0038), 0.20094562647754138], [tensor(0.0039), 0.2127659574468085], [tensor(0.0039), 0.21040189125295508], [tensor(0.0039), 0.21513002364066194], [tensor(0.0040), 0.2033096926713948], [tensor(0.0040), 0.19858156028368795], [tensor(0.0040), 0.2222222222222222], [tensor(0.0041), 0.20094562647754138], [tensor(0.0042), 0.21749408983451538], [tensor(0.0041), 0.19621749408983452], [tensor(0.0042), 0.20094562647754138], [tensor(0.0042), 0.19148936170212766], [tensor(0.0043), 0.21040189125295508], [tensor(0.0044), 0.21040189125295508], [tensor(0.0044), 0.20567375886524822], [tensor(0.0044), 0.21513002364066194], [tensor(0.0044), 0.21513002364066194], [tensor(0.0045), 0.2127659574468085], [tensor(0.0045), 0.20803782505910165], [tensor(0.0045), 0.23404255319148937], [tensor(0.0046), 0.23167848699763594], [tensor(0.0048), 0.21749408983451538], [tensor(0.0048), 0.2222222222222222], [tensor(0.0047), 0.21513002364066194], [tensor(0.0047), 0.21749408983451538], [tensor(0.0049), 0.22458628841607564], [tensor(0.0050), 0.21749408983451538], [tensor(0.0051), 0.20094562647754138], [tensor(0.0052), 0.21040189125295508], [tensor(0.0052), 0.20803782505910165], [tensor(0.0052), 0.24349881796690306], [tensor(0.0052), 0.2198581560283688], [tensor(0.0053), 0.2198581560283688], [tensor(0.0053), 0.2222222222222222], [tensor(0.0054), 0.22458628841607564], [tensor(0.0056), 0.23167848699763594], [tensor(0.0054), 0.21040189125295508], [tensor(0.0055), 0.2364066193853428], [tensor(0.0054), 0.22458628841607564], [tensor(0.0056), 0.2033096926713948], [tensor(0.0058), 0.2033096926713948], [tensor(0.0057), 0.23404255319148937], [tensor(0.0056), 0.21513002364066194], [tensor(0.0060), 0.2198581560283688], [tensor(0.0059), 0.21040189125295508], [tensor(0.0058), 0.2364066193853428], [tensor(0.0061), 0.22695035460992907], [tensor(0.0060), 0.20567375886524822], [tensor(0.0060), 0.19148936170212766], [tensor(0.0062), 0.21513002364066194], [tensor(0.0061), 0.22458628841607564], [tensor(0.0061), 0.21513002364066194], [tensor(0.0064), 0.20803782505910165], [tensor(0.0064), 0.21040189125295508], [tensor(0.0064), 0.2198581560283688], [tensor(0.0066), 0.2198581560283688], [tensor(0.0065), 0.21513002364066194], [tensor(0.0063), 0.2293144208037825], [tensor(0.0064), 0.23877068557919623], [tensor(0.0064), 0.21513002364066194], [tensor(0.0064), 0.21749408983451538], [tensor(0.0066), 0.21513002364066194], [tensor(0.0067), 0.20803782505910165], [tensor(0.0066), 0.21749408983451538], [tensor(0.0069), 0.23167848699763594], [tensor(0.0070), 0.20567375886524822], [tensor(0.0068), 0.23167848699763594]]\n",
            "<class 'list'>\n",
            "[[tensor(0.0031), 0.23476297968397292], [tensor(0.0032), 0.24830699774266365], [tensor(0.0031), 0.26410835214446954], [tensor(0.0031), 0.24604966139954854], [tensor(0.0031), 0.23927765237020315], [tensor(0.0031), 0.24830699774266365], [tensor(0.0032), 0.2325056433408578], [tensor(0.0032), 0.25733634311512416], [tensor(0.0032), 0.25733634311512416], [tensor(0.0032), 0.2686230248306998], [tensor(0.0032), 0.27313769751693], [tensor(0.0032), 0.26636568848758463], [tensor(0.0032), 0.24604966139954854], [tensor(0.0032), 0.2708803611738149], [tensor(0.0032), 0.27313769751693], [tensor(0.0032), 0.25733634311512416], [tensor(0.0032), 0.25733634311512416], [tensor(0.0032), 0.25733634311512416], [tensor(0.0032), 0.26410835214446954], [tensor(0.0032), 0.24830699774266365], [tensor(0.0032), 0.26410835214446954], [tensor(0.0032), 0.24604966139954854], [tensor(0.0033), 0.24153498871331827], [tensor(0.0033), 0.23702031602708803], [tensor(0.0033), 0.27539503386004516], [tensor(0.0033), 0.24379232505643342], [tensor(0.0033), 0.2505643340857788], [tensor(0.0033), 0.24830699774266365], [tensor(0.0034), 0.23476297968397292], [tensor(0.0034), 0.24379232505643342], [tensor(0.0034), 0.2595936794582393], [tensor(0.0035), 0.2595936794582393], [tensor(0.0035), 0.24830699774266365], [tensor(0.0035), 0.24604966139954854], [tensor(0.0036), 0.255079006772009], [tensor(0.0036), 0.2618510158013544], [tensor(0.0037), 0.24153498871331827], [tensor(0.0037), 0.23927765237020315], [tensor(0.0037), 0.25733634311512416], [tensor(0.0037), 0.26410835214446954], [tensor(0.0038), 0.24379232505643342], [tensor(0.0039), 0.23476297968397292], [tensor(0.0039), 0.23702031602708803], [tensor(0.0039), 0.23476297968397292], [tensor(0.0039), 0.2505643340857788], [tensor(0.0040), 0.255079006772009], [tensor(0.0041), 0.23927765237020315], [tensor(0.0041), 0.23702031602708803], [tensor(0.0041), 0.2505643340857788], [tensor(0.0042), 0.22573363431151242], [tensor(0.0042), 0.23927765237020315], [tensor(0.0043), 0.2595936794582393], [tensor(0.0043), 0.2234762979683973], [tensor(0.0044), 0.2325056433408578], [tensor(0.0045), 0.2505643340857788], [tensor(0.0047), 0.25733634311512416], [tensor(0.0044), 0.23927765237020315], [tensor(0.0045), 0.2325056433408578], [tensor(0.0046), 0.24604966139954854], [tensor(0.0046), 0.23476297968397292], [tensor(0.0048), 0.23476297968397292], [tensor(0.0050), 0.22799097065462753], [tensor(0.0049), 0.23927765237020315], [tensor(0.0049), 0.23702031602708803], [tensor(0.0049), 0.24153498871331827], [tensor(0.0052), 0.23024830699774265], [tensor(0.0050), 0.2325056433408578], [tensor(0.0050), 0.21896162528216703], [tensor(0.0053), 0.2234762979683973], [tensor(0.0052), 0.24830699774266365], [tensor(0.0051), 0.23702031602708803], [tensor(0.0051), 0.2234762979683973], [tensor(0.0054), 0.23476297968397292], [tensor(0.0056), 0.21218961625282168], [tensor(0.0055), 0.22573363431151242], [tensor(0.0054), 0.2325056433408578], [tensor(0.0057), 0.22121896162528218], [tensor(0.0054), 0.2505643340857788], [tensor(0.0056), 0.2144469525959368], [tensor(0.0057), 0.21896162528216703], [tensor(0.0056), 0.23476297968397292], [tensor(0.0057), 0.22799097065462753], [tensor(0.0058), 0.2234762979683973], [tensor(0.0057), 0.23702031602708803], [tensor(0.0057), 0.24153498871331827], [tensor(0.0060), 0.2234762979683973], [tensor(0.0059), 0.24604966139954854], [tensor(0.0059), 0.2505643340857788], [tensor(0.0059), 0.2505643340857788], [tensor(0.0061), 0.2325056433408578], [tensor(0.0060), 0.23024830699774265], [tensor(0.0062), 0.2234762979683973], [tensor(0.0062), 0.23476297968397292], [tensor(0.0061), 0.23702031602708803], [tensor(0.0063), 0.23476297968397292], [tensor(0.0066), 0.24153498871331827], [tensor(0.0063), 0.23024830699774265], [tensor(0.0066), 0.22799097065462753], [tensor(0.0065), 0.24379232505643342], [tensor(0.0065), 0.23927765237020315]]\n",
            "<class 'list'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Best validation accuracy:\n",
        "0.2955\n",
        "\n",
        "Best test accuracy:\n",
        "0.2957\n",
        "\n",
        "## Plotting Metrics v/s Number of Epochs: \n",
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "l-zlXV5_lfTK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dkb7FVAfA7e"
      },
      "source": [
        "## CNN + LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTUyv1HLfrm2",
        "outputId": "44e7f742-911b-4793-cebe-9d652d067a2c",
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = CNN_LSTM().cuda()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR, betas=BETAS, eps=EPS, weight_decay=DECAY)\n",
        "train_and_evaluate(model, optimizer, data_loaders, num_epochs=EPOCHS)\n",
        "del model"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Progress: \tEpoch 1 [0/1692 (0.00%)]\t\tLoss: 1.42300\n",
            "Training Progress: \tEpoch 1 [320/1692 (18.87%)]\t\tLoss: 1.37810\n",
            "Training Progress: \tEpoch 1 [640/1692 (37.74%)]\t\tLoss: 1.38590\n",
            "Training Progress: \tEpoch 1 [960/1692 (56.60%)]\t\tLoss: 1.38306\n",
            "Training Progress: \tEpoch 1 [1280/1692 (75.47%)]\t\tLoss: 1.39080\n",
            "Training Progress: \tEpoch 1 [1600/1692 (94.34%)]\t\tLoss: 1.38300\n",
            "\tTrain loss: 0.04341, Accuracy: 421/1692 (24.88%)\n",
            "\tValidation loss: 0.00328, Accuracy: 109/423 (25.77%)\n",
            "\tTest loss: 0.00313, Accuracy: 111/443 (25.06%)\n",
            "\n",
            "Training Progress: \tEpoch 2 [0/1692 (0.00%)]\t\tLoss: 1.39463\n",
            "Training Progress: \tEpoch 2 [320/1692 (18.87%)]\t\tLoss: 1.38857\n",
            "Training Progress: \tEpoch 2 [640/1692 (37.74%)]\t\tLoss: 1.38414\n",
            "Training Progress: \tEpoch 2 [960/1692 (56.60%)]\t\tLoss: 1.38351\n",
            "Training Progress: \tEpoch 2 [1280/1692 (75.47%)]\t\tLoss: 1.39523\n",
            "Training Progress: \tEpoch 2 [1600/1692 (94.34%)]\t\tLoss: 1.36347\n",
            "\tTrain loss: 0.04293, Accuracy: 561/1692 (33.16%)\n",
            "\tValidation loss: 0.00325, Accuracy: 143/423 (33.81%)\n",
            "\tTest loss: 0.00311, Accuracy: 139/443 (31.38%)\n",
            "\n",
            "Training Progress: \tEpoch 3 [0/1692 (0.00%)]\t\tLoss: 1.35059\n",
            "Training Progress: \tEpoch 3 [320/1692 (18.87%)]\t\tLoss: 1.29225\n",
            "Training Progress: \tEpoch 3 [640/1692 (37.74%)]\t\tLoss: 1.35741\n",
            "Training Progress: \tEpoch 3 [960/1692 (56.60%)]\t\tLoss: 1.31107\n",
            "Training Progress: \tEpoch 3 [1280/1692 (75.47%)]\t\tLoss: 1.45410\n",
            "Training Progress: \tEpoch 3 [1600/1692 (94.34%)]\t\tLoss: 1.33252\n",
            "\tTrain loss: 0.04162, Accuracy: 590/1692 (34.87%)\n",
            "\tValidation loss: 0.00314, Accuracy: 156/423 (36.88%)\n",
            "\tTest loss: 0.00301, Accuracy: 156/443 (35.21%)\n",
            "\n",
            "Training Progress: \tEpoch 4 [0/1692 (0.00%)]\t\tLoss: 1.33921\n",
            "Training Progress: \tEpoch 4 [320/1692 (18.87%)]\t\tLoss: 1.28590\n",
            "Training Progress: \tEpoch 4 [640/1692 (37.74%)]\t\tLoss: 1.29964\n",
            "Training Progress: \tEpoch 4 [960/1692 (56.60%)]\t\tLoss: 1.27420\n",
            "Training Progress: \tEpoch 4 [1280/1692 (75.47%)]\t\tLoss: 1.45306\n",
            "Training Progress: \tEpoch 4 [1600/1692 (94.34%)]\t\tLoss: 1.27782\n",
            "\tTrain loss: 0.04129, Accuracy: 595/1692 (35.17%)\n",
            "\tValidation loss: 0.00314, Accuracy: 156/423 (36.88%)\n",
            "\tTest loss: 0.00300, Accuracy: 158/443 (35.67%)\n",
            "\n",
            "Training Progress: \tEpoch 5 [0/1692 (0.00%)]\t\tLoss: 1.34065\n",
            "Training Progress: \tEpoch 5 [320/1692 (18.87%)]\t\tLoss: 1.21920\n",
            "Training Progress: \tEpoch 5 [640/1692 (37.74%)]\t\tLoss: 1.28317\n",
            "Training Progress: \tEpoch 5 [960/1692 (56.60%)]\t\tLoss: 1.20482\n",
            "Training Progress: \tEpoch 5 [1280/1692 (75.47%)]\t\tLoss: 1.48564\n",
            "Training Progress: \tEpoch 5 [1600/1692 (94.34%)]\t\tLoss: 1.23008\n",
            "\tTrain loss: 0.04053, Accuracy: 637/1692 (37.65%)\n",
            "\tValidation loss: 0.00309, Accuracy: 159/423 (37.59%)\n",
            "\tTest loss: 0.00292, Accuracy: 154/443 (34.76%)\n",
            "\n",
            "Training Progress: \tEpoch 6 [0/1692 (0.00%)]\t\tLoss: 1.32838\n",
            "Training Progress: \tEpoch 6 [320/1692 (18.87%)]\t\tLoss: 1.30243\n",
            "Training Progress: \tEpoch 6 [640/1692 (37.74%)]\t\tLoss: 1.25284\n",
            "Training Progress: \tEpoch 6 [960/1692 (56.60%)]\t\tLoss: 1.22372\n",
            "Training Progress: \tEpoch 6 [1280/1692 (75.47%)]\t\tLoss: 1.39313\n",
            "Training Progress: \tEpoch 6 [1600/1692 (94.34%)]\t\tLoss: 1.24913\n",
            "\tTrain loss: 0.04026, Accuracy: 618/1692 (36.52%)\n",
            "\tValidation loss: 0.00307, Accuracy: 154/423 (36.41%)\n",
            "\tTest loss: 0.00293, Accuracy: 155/443 (34.99%)\n",
            "\n",
            "Training Progress: \tEpoch 7 [0/1692 (0.00%)]\t\tLoss: 1.28093\n",
            "Training Progress: \tEpoch 7 [320/1692 (18.87%)]\t\tLoss: 1.22984\n",
            "Training Progress: \tEpoch 7 [640/1692 (37.74%)]\t\tLoss: 1.24617\n",
            "Training Progress: \tEpoch 7 [960/1692 (56.60%)]\t\tLoss: 1.18955\n",
            "Training Progress: \tEpoch 7 [1280/1692 (75.47%)]\t\tLoss: 1.41699\n",
            "Training Progress: \tEpoch 7 [1600/1692 (94.34%)]\t\tLoss: 1.25407\n",
            "\tTrain loss: 0.03949, Accuracy: 703/1692 (41.55%)\n",
            "\tValidation loss: 0.00303, Accuracy: 158/423 (37.35%)\n",
            "\tTest loss: 0.00292, Accuracy: 154/443 (34.76%)\n",
            "\n",
            "Training Progress: \tEpoch 8 [0/1692 (0.00%)]\t\tLoss: 1.29898\n",
            "Training Progress: \tEpoch 8 [320/1692 (18.87%)]\t\tLoss: 1.18304\n",
            "Training Progress: \tEpoch 8 [640/1692 (37.74%)]\t\tLoss: 1.15973\n",
            "Training Progress: \tEpoch 8 [960/1692 (56.60%)]\t\tLoss: 1.19986\n",
            "Training Progress: \tEpoch 8 [1280/1692 (75.47%)]\t\tLoss: 1.30421\n",
            "Training Progress: \tEpoch 8 [1600/1692 (94.34%)]\t\tLoss: 1.20373\n",
            "\tTrain loss: 0.03847, Accuracy: 729/1692 (43.09%)\n",
            "\tValidation loss: 0.00299, Accuracy: 164/423 (38.77%)\n",
            "\tTest loss: 0.00288, Accuracy: 170/443 (38.37%)\n",
            "\n",
            "Training Progress: \tEpoch 9 [0/1692 (0.00%)]\t\tLoss: 1.28578\n",
            "Training Progress: \tEpoch 9 [320/1692 (18.87%)]\t\tLoss: 1.12305\n",
            "Training Progress: \tEpoch 9 [640/1692 (37.74%)]\t\tLoss: 1.17310\n",
            "Training Progress: \tEpoch 9 [960/1692 (56.60%)]\t\tLoss: 1.08720\n",
            "Training Progress: \tEpoch 9 [1280/1692 (75.47%)]\t\tLoss: 1.30166\n",
            "Training Progress: \tEpoch 9 [1600/1692 (94.34%)]\t\tLoss: 1.24736\n",
            "\tTrain loss: 0.03784, Accuracy: 741/1692 (43.79%)\n",
            "\tValidation loss: 0.00292, Accuracy: 172/423 (40.66%)\n",
            "\tTest loss: 0.00282, Accuracy: 174/443 (39.28%)\n",
            "\n",
            "Training Progress: \tEpoch 10 [0/1692 (0.00%)]\t\tLoss: 1.31687\n",
            "Training Progress: \tEpoch 10 [320/1692 (18.87%)]\t\tLoss: 1.19684\n",
            "Training Progress: \tEpoch 10 [640/1692 (37.74%)]\t\tLoss: 1.16244\n",
            "Training Progress: \tEpoch 10 [960/1692 (56.60%)]\t\tLoss: 1.16579\n",
            "Training Progress: \tEpoch 10 [1280/1692 (75.47%)]\t\tLoss: 1.26250\n",
            "Training Progress: \tEpoch 10 [1600/1692 (94.34%)]\t\tLoss: 1.18254\n",
            "\tTrain loss: 0.03748, Accuracy: 734/1692 (43.38%)\n",
            "\tValidation loss: 0.00290, Accuracy: 178/423 (42.08%)\n",
            "\tTest loss: 0.00283, Accuracy: 181/443 (40.86%)\n",
            "\n",
            "Training Progress: \tEpoch 11 [0/1692 (0.00%)]\t\tLoss: 1.25609\n",
            "Training Progress: \tEpoch 11 [320/1692 (18.87%)]\t\tLoss: 1.22955\n",
            "Training Progress: \tEpoch 11 [640/1692 (37.74%)]\t\tLoss: 1.14787\n",
            "Training Progress: \tEpoch 11 [960/1692 (56.60%)]\t\tLoss: 1.18579\n",
            "Training Progress: \tEpoch 11 [1280/1692 (75.47%)]\t\tLoss: 1.20910\n",
            "Training Progress: \tEpoch 11 [1600/1692 (94.34%)]\t\tLoss: 1.14440\n",
            "\tTrain loss: 0.03766, Accuracy: 724/1692 (42.79%)\n",
            "\tValidation loss: 0.00298, Accuracy: 176/423 (41.61%)\n",
            "\tTest loss: 0.00289, Accuracy: 175/443 (39.50%)\n",
            "\n",
            "Training Progress: \tEpoch 12 [0/1692 (0.00%)]\t\tLoss: 1.37425\n",
            "Training Progress: \tEpoch 12 [320/1692 (18.87%)]\t\tLoss: 1.15794\n",
            "Training Progress: \tEpoch 12 [640/1692 (37.74%)]\t\tLoss: 1.12888\n",
            "Training Progress: \tEpoch 12 [960/1692 (56.60%)]\t\tLoss: 1.19950\n",
            "Training Progress: \tEpoch 12 [1280/1692 (75.47%)]\t\tLoss: 1.27526\n",
            "Training Progress: \tEpoch 12 [1600/1692 (94.34%)]\t\tLoss: 1.16064\n",
            "\tTrain loss: 0.03638, Accuracy: 783/1692 (46.28%)\n",
            "\tValidation loss: 0.00292, Accuracy: 181/423 (42.79%)\n",
            "\tTest loss: 0.00282, Accuracy: 180/443 (40.63%)\n",
            "\n",
            "Training Progress: \tEpoch 13 [0/1692 (0.00%)]\t\tLoss: 1.24201\n",
            "Training Progress: \tEpoch 13 [320/1692 (18.87%)]\t\tLoss: 1.15503\n",
            "Training Progress: \tEpoch 13 [640/1692 (37.74%)]\t\tLoss: 1.09053\n",
            "Training Progress: \tEpoch 13 [960/1692 (56.60%)]\t\tLoss: 1.09532\n",
            "Training Progress: \tEpoch 13 [1280/1692 (75.47%)]\t\tLoss: 1.32166\n",
            "Training Progress: \tEpoch 13 [1600/1692 (94.34%)]\t\tLoss: 1.16581\n",
            "\tTrain loss: 0.03568, Accuracy: 839/1692 (49.59%)\n",
            "\tValidation loss: 0.00282, Accuracy: 187/423 (44.21%)\n",
            "\tTest loss: 0.00277, Accuracy: 182/443 (41.08%)\n",
            "\n",
            "Training Progress: \tEpoch 14 [0/1692 (0.00%)]\t\tLoss: 1.22336\n",
            "Training Progress: \tEpoch 14 [320/1692 (18.87%)]\t\tLoss: 1.10523\n",
            "Training Progress: \tEpoch 14 [640/1692 (37.74%)]\t\tLoss: 1.09377\n",
            "Training Progress: \tEpoch 14 [960/1692 (56.60%)]\t\tLoss: 1.06726\n",
            "Training Progress: \tEpoch 14 [1280/1692 (75.47%)]\t\tLoss: 1.27896\n",
            "Training Progress: \tEpoch 14 [1600/1692 (94.34%)]\t\tLoss: 1.19325\n",
            "\tTrain loss: 0.03634, Accuracy: 797/1692 (47.10%)\n",
            "\tValidation loss: 0.00290, Accuracy: 190/423 (44.92%)\n",
            "\tTest loss: 0.00281, Accuracy: 184/443 (41.53%)\n",
            "\n",
            "Training Progress: \tEpoch 15 [0/1692 (0.00%)]\t\tLoss: 1.26645\n",
            "Training Progress: \tEpoch 15 [320/1692 (18.87%)]\t\tLoss: 1.20667\n",
            "Training Progress: \tEpoch 15 [640/1692 (37.74%)]\t\tLoss: 1.17237\n",
            "Training Progress: \tEpoch 15 [960/1692 (56.60%)]\t\tLoss: 1.04299\n",
            "Training Progress: \tEpoch 15 [1280/1692 (75.47%)]\t\tLoss: 1.30811\n",
            "Training Progress: \tEpoch 15 [1600/1692 (94.34%)]\t\tLoss: 1.07519\n",
            "\tTrain loss: 0.03460, Accuracy: 866/1692 (51.18%)\n",
            "\tValidation loss: 0.00285, Accuracy: 184/423 (43.50%)\n",
            "\tTest loss: 0.00269, Accuracy: 203/443 (45.82%)\n",
            "\n",
            "Training Progress: \tEpoch 16 [0/1692 (0.00%)]\t\tLoss: 1.35076\n",
            "Training Progress: \tEpoch 16 [320/1692 (18.87%)]\t\tLoss: 0.97419\n",
            "Training Progress: \tEpoch 16 [640/1692 (37.74%)]\t\tLoss: 1.10104\n",
            "Training Progress: \tEpoch 16 [960/1692 (56.60%)]\t\tLoss: 1.06925\n",
            "Training Progress: \tEpoch 16 [1280/1692 (75.47%)]\t\tLoss: 1.13181\n",
            "Training Progress: \tEpoch 16 [1600/1692 (94.34%)]\t\tLoss: 1.09355\n",
            "\tTrain loss: 0.03358, Accuracy: 917/1692 (54.20%)\n",
            "\tValidation loss: 0.00280, Accuracy: 200/423 (47.28%)\n",
            "\tTest loss: 0.00265, Accuracy: 209/443 (47.18%)\n",
            "\n",
            "Training Progress: \tEpoch 17 [0/1692 (0.00%)]\t\tLoss: 1.37229\n",
            "Training Progress: \tEpoch 17 [320/1692 (18.87%)]\t\tLoss: 1.08489\n",
            "Training Progress: \tEpoch 17 [640/1692 (37.74%)]\t\tLoss: 0.99076\n",
            "Training Progress: \tEpoch 17 [960/1692 (56.60%)]\t\tLoss: 1.03449\n",
            "Training Progress: \tEpoch 17 [1280/1692 (75.47%)]\t\tLoss: 1.28168\n",
            "Training Progress: \tEpoch 17 [1600/1692 (94.34%)]\t\tLoss: 1.07820\n",
            "\tTrain loss: 0.03309, Accuracy: 899/1692 (53.13%)\n",
            "\tValidation loss: 0.00276, Accuracy: 196/423 (46.34%)\n",
            "\tTest loss: 0.00264, Accuracy: 208/443 (46.95%)\n",
            "\n",
            "Training Progress: \tEpoch 18 [0/1692 (0.00%)]\t\tLoss: 1.20271\n",
            "Training Progress: \tEpoch 18 [320/1692 (18.87%)]\t\tLoss: 1.26738\n",
            "Training Progress: \tEpoch 18 [640/1692 (37.74%)]\t\tLoss: 1.02309\n",
            "Training Progress: \tEpoch 18 [960/1692 (56.60%)]\t\tLoss: 0.94124\n",
            "Training Progress: \tEpoch 18 [1280/1692 (75.47%)]\t\tLoss: 1.15134\n",
            "Training Progress: \tEpoch 18 [1600/1692 (94.34%)]\t\tLoss: 1.19509\n",
            "\tTrain loss: 0.03179, Accuracy: 951/1692 (56.21%)\n",
            "\tValidation loss: 0.00273, Accuracy: 203/423 (47.99%)\n",
            "\tTest loss: 0.00260, Accuracy: 207/443 (46.73%)\n",
            "\n",
            "Training Progress: \tEpoch 19 [0/1692 (0.00%)]\t\tLoss: 1.25687\n",
            "Training Progress: \tEpoch 19 [320/1692 (18.87%)]\t\tLoss: 1.07663\n",
            "Training Progress: \tEpoch 19 [640/1692 (37.74%)]\t\tLoss: 1.14827\n",
            "Training Progress: \tEpoch 19 [960/1692 (56.60%)]\t\tLoss: 0.87125\n",
            "Training Progress: \tEpoch 19 [1280/1692 (75.47%)]\t\tLoss: 1.14833\n",
            "Training Progress: \tEpoch 19 [1600/1692 (94.34%)]\t\tLoss: 0.99449\n",
            "\tTrain loss: 0.03228, Accuracy: 924/1692 (54.61%)\n",
            "\tValidation loss: 0.00276, Accuracy: 199/423 (47.04%)\n",
            "\tTest loss: 0.00263, Accuracy: 204/443 (46.05%)\n",
            "\n",
            "Training Progress: \tEpoch 20 [0/1692 (0.00%)]\t\tLoss: 1.31942\n",
            "Training Progress: \tEpoch 20 [320/1692 (18.87%)]\t\tLoss: 0.93765\n",
            "Training Progress: \tEpoch 20 [640/1692 (37.74%)]\t\tLoss: 0.99674\n",
            "Training Progress: \tEpoch 20 [960/1692 (56.60%)]\t\tLoss: 0.90472\n",
            "Training Progress: \tEpoch 20 [1280/1692 (75.47%)]\t\tLoss: 1.26189\n",
            "Training Progress: \tEpoch 20 [1600/1692 (94.34%)]\t\tLoss: 1.13348\n",
            "\tTrain loss: 0.03214, Accuracy: 918/1692 (54.26%)\n",
            "\tValidation loss: 0.00273, Accuracy: 200/423 (47.28%)\n",
            "\tTest loss: 0.00258, Accuracy: 217/443 (48.98%)\n",
            "\n",
            "Training Progress: \tEpoch 21 [0/1692 (0.00%)]\t\tLoss: 1.22555\n",
            "Training Progress: \tEpoch 21 [320/1692 (18.87%)]\t\tLoss: 1.03266\n",
            "Training Progress: \tEpoch 21 [640/1692 (37.74%)]\t\tLoss: 1.05958\n",
            "Training Progress: \tEpoch 21 [960/1692 (56.60%)]\t\tLoss: 0.92847\n",
            "Training Progress: \tEpoch 21 [1280/1692 (75.47%)]\t\tLoss: 1.18409\n",
            "Training Progress: \tEpoch 21 [1600/1692 (94.34%)]\t\tLoss: 0.92586\n",
            "\tTrain loss: 0.02966, Accuracy: 1009/1692 (59.63%)\n",
            "\tValidation loss: 0.00258, Accuracy: 221/423 (52.25%)\n",
            "\tTest loss: 0.00242, Accuracy: 235/443 (53.05%)\n",
            "\n",
            "Training Progress: \tEpoch 22 [0/1692 (0.00%)]\t\tLoss: 1.25062\n",
            "Training Progress: \tEpoch 22 [320/1692 (18.87%)]\t\tLoss: 1.02433\n",
            "Training Progress: \tEpoch 22 [640/1692 (37.74%)]\t\tLoss: 1.08335\n",
            "Training Progress: \tEpoch 22 [960/1692 (56.60%)]\t\tLoss: 0.85415\n",
            "Training Progress: \tEpoch 22 [1280/1692 (75.47%)]\t\tLoss: 0.96784\n",
            "Training Progress: \tEpoch 22 [1600/1692 (94.34%)]\t\tLoss: 0.92287\n",
            "\tTrain loss: 0.03013, Accuracy: 988/1692 (58.39%)\n",
            "\tValidation loss: 0.00276, Accuracy: 204/423 (48.23%)\n",
            "\tTest loss: 0.00264, Accuracy: 202/443 (45.60%)\n",
            "\n",
            "Training Progress: \tEpoch 23 [0/1692 (0.00%)]\t\tLoss: 1.20834\n",
            "Training Progress: \tEpoch 23 [320/1692 (18.87%)]\t\tLoss: 0.91083\n",
            "Training Progress: \tEpoch 23 [640/1692 (37.74%)]\t\tLoss: 1.04020\n",
            "Training Progress: \tEpoch 23 [960/1692 (56.60%)]\t\tLoss: 0.94401\n",
            "Training Progress: \tEpoch 23 [1280/1692 (75.47%)]\t\tLoss: 1.08175\n",
            "Training Progress: \tEpoch 23 [1600/1692 (94.34%)]\t\tLoss: 0.88441\n",
            "\tTrain loss: 0.03026, Accuracy: 982/1692 (58.04%)\n",
            "\tValidation loss: 0.00273, Accuracy: 202/423 (47.75%)\n",
            "\tTest loss: 0.00262, Accuracy: 211/443 (47.63%)\n",
            "\n",
            "Training Progress: \tEpoch 24 [0/1692 (0.00%)]\t\tLoss: 1.17882\n",
            "Training Progress: \tEpoch 24 [320/1692 (18.87%)]\t\tLoss: 0.96991\n",
            "Training Progress: \tEpoch 24 [640/1692 (37.74%)]\t\tLoss: 0.92058\n",
            "Training Progress: \tEpoch 24 [960/1692 (56.60%)]\t\tLoss: 0.90578\n",
            "Training Progress: \tEpoch 24 [1280/1692 (75.47%)]\t\tLoss: 1.10192\n",
            "Training Progress: \tEpoch 24 [1600/1692 (94.34%)]\t\tLoss: 0.69852\n",
            "\tTrain loss: 0.03347, Accuracy: 908/1692 (53.66%)\n",
            "\tValidation loss: 0.00304, Accuracy: 174/423 (41.13%)\n",
            "\tTest loss: 0.00290, Accuracy: 196/443 (44.24%)\n",
            "\n",
            "Training Progress: \tEpoch 25 [0/1692 (0.00%)]\t\tLoss: 1.34580\n",
            "Training Progress: \tEpoch 25 [320/1692 (18.87%)]\t\tLoss: 0.97693\n",
            "Training Progress: \tEpoch 25 [640/1692 (37.74%)]\t\tLoss: 0.99717\n",
            "Training Progress: \tEpoch 25 [960/1692 (56.60%)]\t\tLoss: 0.85297\n",
            "Training Progress: \tEpoch 25 [1280/1692 (75.47%)]\t\tLoss: 0.98877\n",
            "Training Progress: \tEpoch 25 [1600/1692 (94.34%)]\t\tLoss: 0.81753\n",
            "\tTrain loss: 0.02877, Accuracy: 1012/1692 (59.81%)\n",
            "\tValidation loss: 0.00273, Accuracy: 203/423 (47.99%)\n",
            "\tTest loss: 0.00262, Accuracy: 222/443 (50.11%)\n",
            "\n",
            "Training Progress: \tEpoch 26 [0/1692 (0.00%)]\t\tLoss: 1.16036\n",
            "Training Progress: \tEpoch 26 [320/1692 (18.87%)]\t\tLoss: 0.96323\n",
            "Training Progress: \tEpoch 26 [640/1692 (37.74%)]\t\tLoss: 0.98782\n",
            "Training Progress: \tEpoch 26 [960/1692 (56.60%)]\t\tLoss: 0.90842\n",
            "Training Progress: \tEpoch 26 [1280/1692 (75.47%)]\t\tLoss: 1.07767\n",
            "Training Progress: \tEpoch 26 [1600/1692 (94.34%)]\t\tLoss: 0.77759\n",
            "\tTrain loss: 0.02905, Accuracy: 1017/1692 (60.11%)\n",
            "\tValidation loss: 0.00284, Accuracy: 202/423 (47.75%)\n",
            "\tTest loss: 0.00262, Accuracy: 215/443 (48.53%)\n",
            "\n",
            "Training Progress: \tEpoch 27 [0/1692 (0.00%)]\t\tLoss: 1.19566\n",
            "Training Progress: \tEpoch 27 [320/1692 (18.87%)]\t\tLoss: 0.98697\n",
            "Training Progress: \tEpoch 27 [640/1692 (37.74%)]\t\tLoss: 0.91034\n",
            "Training Progress: \tEpoch 27 [960/1692 (56.60%)]\t\tLoss: 0.93984\n",
            "Training Progress: \tEpoch 27 [1280/1692 (75.47%)]\t\tLoss: 1.06938\n",
            "Training Progress: \tEpoch 27 [1600/1692 (94.34%)]\t\tLoss: 0.80440\n",
            "\tTrain loss: 0.02911, Accuracy: 1030/1692 (60.87%)\n",
            "\tValidation loss: 0.00285, Accuracy: 199/423 (47.04%)\n",
            "\tTest loss: 0.00272, Accuracy: 211/443 (47.63%)\n",
            "\n",
            "Training Progress: \tEpoch 28 [0/1692 (0.00%)]\t\tLoss: 1.15145\n",
            "Training Progress: \tEpoch 28 [320/1692 (18.87%)]\t\tLoss: 0.97199\n",
            "Training Progress: \tEpoch 28 [640/1692 (37.74%)]\t\tLoss: 0.87511\n",
            "Training Progress: \tEpoch 28 [960/1692 (56.60%)]\t\tLoss: 0.86529\n",
            "Training Progress: \tEpoch 28 [1280/1692 (75.47%)]\t\tLoss: 0.98437\n",
            "Training Progress: \tEpoch 28 [1600/1692 (94.34%)]\t\tLoss: 0.72380\n",
            "\tTrain loss: 0.02535, Accuracy: 1113/1692 (65.78%)\n",
            "\tValidation loss: 0.00257, Accuracy: 214/423 (50.59%)\n",
            "\tTest loss: 0.00253, Accuracy: 228/443 (51.47%)\n",
            "\n",
            "Training Progress: \tEpoch 29 [0/1692 (0.00%)]\t\tLoss: 1.13958\n",
            "Training Progress: \tEpoch 29 [320/1692 (18.87%)]\t\tLoss: 1.11357\n",
            "Training Progress: \tEpoch 29 [640/1692 (37.74%)]\t\tLoss: 0.88744\n",
            "Training Progress: \tEpoch 29 [960/1692 (56.60%)]\t\tLoss: 0.82622\n",
            "Training Progress: \tEpoch 29 [1280/1692 (75.47%)]\t\tLoss: 1.07125\n",
            "Training Progress: \tEpoch 29 [1600/1692 (94.34%)]\t\tLoss: 0.84797\n",
            "\tTrain loss: 0.02687, Accuracy: 1087/1692 (64.24%)\n",
            "\tValidation loss: 0.00270, Accuracy: 193/423 (45.63%)\n",
            "\tTest loss: 0.00263, Accuracy: 218/443 (49.21%)\n",
            "\n",
            "Training Progress: \tEpoch 30 [0/1692 (0.00%)]\t\tLoss: 1.14186\n",
            "Training Progress: \tEpoch 30 [320/1692 (18.87%)]\t\tLoss: 0.92355\n",
            "Training Progress: \tEpoch 30 [640/1692 (37.74%)]\t\tLoss: 0.81495\n",
            "Training Progress: \tEpoch 30 [960/1692 (56.60%)]\t\tLoss: 0.97525\n",
            "Training Progress: \tEpoch 30 [1280/1692 (75.47%)]\t\tLoss: 0.89160\n",
            "Training Progress: \tEpoch 30 [1600/1692 (94.34%)]\t\tLoss: 0.81169\n",
            "\tTrain loss: 0.02786, Accuracy: 1059/1692 (62.59%)\n",
            "\tValidation loss: 0.00287, Accuracy: 203/423 (47.99%)\n",
            "\tTest loss: 0.00273, Accuracy: 212/443 (47.86%)\n",
            "\n",
            "Training Progress: \tEpoch 31 [0/1692 (0.00%)]\t\tLoss: 1.17527\n",
            "Training Progress: \tEpoch 31 [320/1692 (18.87%)]\t\tLoss: 1.01057\n",
            "Training Progress: \tEpoch 31 [640/1692 (37.74%)]\t\tLoss: 0.87367\n",
            "Training Progress: \tEpoch 31 [960/1692 (56.60%)]\t\tLoss: 0.86877\n",
            "Training Progress: \tEpoch 31 [1280/1692 (75.47%)]\t\tLoss: 0.94601\n",
            "Training Progress: \tEpoch 31 [1600/1692 (94.34%)]\t\tLoss: 0.76758\n",
            "\tTrain loss: 0.02515, Accuracy: 1113/1692 (65.78%)\n",
            "\tValidation loss: 0.00260, Accuracy: 222/423 (52.48%)\n",
            "\tTest loss: 0.00253, Accuracy: 226/443 (51.02%)\n",
            "\n",
            "Training Progress: \tEpoch 32 [0/1692 (0.00%)]\t\tLoss: 1.02282\n",
            "Training Progress: \tEpoch 32 [320/1692 (18.87%)]\t\tLoss: 0.94160\n",
            "Training Progress: \tEpoch 32 [640/1692 (37.74%)]\t\tLoss: 1.03981\n",
            "Training Progress: \tEpoch 32 [960/1692 (56.60%)]\t\tLoss: 0.98000\n",
            "Training Progress: \tEpoch 32 [1280/1692 (75.47%)]\t\tLoss: 1.10460\n",
            "Training Progress: \tEpoch 32 [1600/1692 (94.34%)]\t\tLoss: 0.81318\n",
            "\tTrain loss: 0.02894, Accuracy: 1030/1692 (60.87%)\n",
            "\tValidation loss: 0.00298, Accuracy: 187/423 (44.21%)\n",
            "\tTest loss: 0.00280, Accuracy: 218/443 (49.21%)\n",
            "\n",
            "Training Progress: \tEpoch 33 [0/1692 (0.00%)]\t\tLoss: 1.19226\n",
            "Training Progress: \tEpoch 33 [320/1692 (18.87%)]\t\tLoss: 0.75602\n",
            "Training Progress: \tEpoch 33 [640/1692 (37.74%)]\t\tLoss: 0.75896\n",
            "Training Progress: \tEpoch 33 [960/1692 (56.60%)]\t\tLoss: 0.78187\n",
            "Training Progress: \tEpoch 33 [1280/1692 (75.47%)]\t\tLoss: 0.93198\n",
            "Training Progress: \tEpoch 33 [1600/1692 (94.34%)]\t\tLoss: 0.70358\n",
            "\tTrain loss: 0.02616, Accuracy: 1096/1692 (64.78%)\n",
            "\tValidation loss: 0.00297, Accuracy: 188/423 (44.44%)\n",
            "\tTest loss: 0.00276, Accuracy: 215/443 (48.53%)\n",
            "\n",
            "Training Progress: \tEpoch 34 [0/1692 (0.00%)]\t\tLoss: 1.20865\n",
            "Training Progress: \tEpoch 34 [320/1692 (18.87%)]\t\tLoss: 1.04707\n",
            "Training Progress: \tEpoch 34 [640/1692 (37.74%)]\t\tLoss: 0.74078\n",
            "Training Progress: \tEpoch 34 [960/1692 (56.60%)]\t\tLoss: 0.92208\n",
            "Training Progress: \tEpoch 34 [1280/1692 (75.47%)]\t\tLoss: 0.85060\n",
            "Training Progress: \tEpoch 34 [1600/1692 (94.34%)]\t\tLoss: 0.65044\n",
            "\tTrain loss: 0.02565, Accuracy: 1106/1692 (65.37%)\n",
            "\tValidation loss: 0.00287, Accuracy: 211/423 (49.88%)\n",
            "\tTest loss: 0.00271, Accuracy: 222/443 (50.11%)\n",
            "\n",
            "Training Progress: \tEpoch 35 [0/1692 (0.00%)]\t\tLoss: 1.06421\n",
            "Training Progress: \tEpoch 35 [320/1692 (18.87%)]\t\tLoss: 0.83638\n",
            "Training Progress: \tEpoch 35 [640/1692 (37.74%)]\t\tLoss: 0.72267\n",
            "Training Progress: \tEpoch 35 [960/1692 (56.60%)]\t\tLoss: 0.89783\n",
            "Training Progress: \tEpoch 35 [1280/1692 (75.47%)]\t\tLoss: 0.95615\n",
            "Training Progress: \tEpoch 35 [1600/1692 (94.34%)]\t\tLoss: 0.84757\n",
            "\tTrain loss: 0.02312, Accuracy: 1176/1692 (69.50%)\n",
            "\tValidation loss: 0.00273, Accuracy: 207/423 (48.94%)\n",
            "\tTest loss: 0.00265, Accuracy: 225/443 (50.79%)\n",
            "\n",
            "Training Progress: \tEpoch 36 [0/1692 (0.00%)]\t\tLoss: 1.04929\n",
            "Training Progress: \tEpoch 36 [320/1692 (18.87%)]\t\tLoss: 0.89117\n",
            "Training Progress: \tEpoch 36 [640/1692 (37.74%)]\t\tLoss: 0.86391\n",
            "Training Progress: \tEpoch 36 [960/1692 (56.60%)]\t\tLoss: 0.85511\n",
            "Training Progress: \tEpoch 36 [1280/1692 (75.47%)]\t\tLoss: 1.15347\n",
            "Training Progress: \tEpoch 36 [1600/1692 (94.34%)]\t\tLoss: 0.85741\n",
            "\tTrain loss: 0.02372, Accuracy: 1164/1692 (68.79%)\n",
            "\tValidation loss: 0.00277, Accuracy: 214/423 (50.59%)\n",
            "\tTest loss: 0.00264, Accuracy: 223/443 (50.34%)\n",
            "\n",
            "Training Progress: \tEpoch 37 [0/1692 (0.00%)]\t\tLoss: 1.01904\n",
            "Training Progress: \tEpoch 37 [320/1692 (18.87%)]\t\tLoss: 0.94444\n",
            "Training Progress: \tEpoch 37 [640/1692 (37.74%)]\t\tLoss: 0.80163\n",
            "Training Progress: \tEpoch 37 [960/1692 (56.60%)]\t\tLoss: 0.67201\n",
            "Training Progress: \tEpoch 37 [1280/1692 (75.47%)]\t\tLoss: 0.90949\n",
            "Training Progress: \tEpoch 37 [1600/1692 (94.34%)]\t\tLoss: 0.68916\n",
            "\tTrain loss: 0.02498, Accuracy: 1160/1692 (68.56%)\n",
            "\tValidation loss: 0.00287, Accuracy: 212/423 (50.12%)\n",
            "\tTest loss: 0.00264, Accuracy: 232/443 (52.37%)\n",
            "\n",
            "Training Progress: \tEpoch 38 [0/1692 (0.00%)]\t\tLoss: 1.15121\n",
            "Training Progress: \tEpoch 38 [320/1692 (18.87%)]\t\tLoss: 0.90655\n",
            "Training Progress: \tEpoch 38 [640/1692 (37.74%)]\t\tLoss: 0.76321\n",
            "Training Progress: \tEpoch 38 [960/1692 (56.60%)]\t\tLoss: 0.67677\n",
            "Training Progress: \tEpoch 38 [1280/1692 (75.47%)]\t\tLoss: 0.67952\n",
            "Training Progress: \tEpoch 38 [1600/1692 (94.34%)]\t\tLoss: 0.81047\n",
            "\tTrain loss: 0.02461, Accuracy: 1108/1692 (65.48%)\n",
            "\tValidation loss: 0.00289, Accuracy: 206/423 (48.70%)\n",
            "\tTest loss: 0.00268, Accuracy: 224/443 (50.56%)\n",
            "\n",
            "Training Progress: \tEpoch 39 [0/1692 (0.00%)]\t\tLoss: 1.01941\n",
            "Training Progress: \tEpoch 39 [320/1692 (18.87%)]\t\tLoss: 0.70738\n",
            "Training Progress: \tEpoch 39 [640/1692 (37.74%)]\t\tLoss: 0.85830\n",
            "Training Progress: \tEpoch 39 [960/1692 (56.60%)]\t\tLoss: 0.85688\n",
            "Training Progress: \tEpoch 39 [1280/1692 (75.47%)]\t\tLoss: 1.00349\n",
            "Training Progress: \tEpoch 39 [1600/1692 (94.34%)]\t\tLoss: 0.66488\n",
            "\tTrain loss: 0.02186, Accuracy: 1219/1692 (72.04%)\n",
            "\tValidation loss: 0.00284, Accuracy: 216/423 (51.06%)\n",
            "\tTest loss: 0.00275, Accuracy: 218/443 (49.21%)\n",
            "\n",
            "Training Progress: \tEpoch 40 [0/1692 (0.00%)]\t\tLoss: 0.88451\n",
            "Training Progress: \tEpoch 40 [320/1692 (18.87%)]\t\tLoss: 0.96772\n",
            "Training Progress: \tEpoch 40 [640/1692 (37.74%)]\t\tLoss: 0.76851\n",
            "Training Progress: \tEpoch 40 [960/1692 (56.60%)]\t\tLoss: 0.74761\n",
            "Training Progress: \tEpoch 40 [1280/1692 (75.47%)]\t\tLoss: 0.93105\n",
            "Training Progress: \tEpoch 40 [1600/1692 (94.34%)]\t\tLoss: 0.65182\n",
            "\tTrain loss: 0.02405, Accuracy: 1152/1692 (68.09%)\n",
            "\tValidation loss: 0.00311, Accuracy: 194/423 (45.86%)\n",
            "\tTest loss: 0.00285, Accuracy: 226/443 (51.02%)\n",
            "\n",
            "Training Progress: \tEpoch 41 [0/1692 (0.00%)]\t\tLoss: 1.16311\n",
            "Training Progress: \tEpoch 41 [320/1692 (18.87%)]\t\tLoss: 1.18504\n",
            "Training Progress: \tEpoch 41 [640/1692 (37.74%)]\t\tLoss: 0.87399\n",
            "Training Progress: \tEpoch 41 [960/1692 (56.60%)]\t\tLoss: 0.76629\n",
            "Training Progress: \tEpoch 41 [1280/1692 (75.47%)]\t\tLoss: 0.67302\n",
            "Training Progress: \tEpoch 41 [1600/1692 (94.34%)]\t\tLoss: 0.64242\n",
            "\tTrain loss: 0.02278, Accuracy: 1194/1692 (70.57%)\n",
            "\tValidation loss: 0.00302, Accuracy: 195/423 (46.10%)\n",
            "\tTest loss: 0.00288, Accuracy: 215/443 (48.53%)\n",
            "\n",
            "Training Progress: \tEpoch 42 [0/1692 (0.00%)]\t\tLoss: 1.19917\n",
            "Training Progress: \tEpoch 42 [320/1692 (18.87%)]\t\tLoss: 0.78071\n",
            "Training Progress: \tEpoch 42 [640/1692 (37.74%)]\t\tLoss: 0.67657\n",
            "Training Progress: \tEpoch 42 [960/1692 (56.60%)]\t\tLoss: 0.63938\n",
            "Training Progress: \tEpoch 42 [1280/1692 (75.47%)]\t\tLoss: 1.04149\n",
            "Training Progress: \tEpoch 42 [1600/1692 (94.34%)]\t\tLoss: 0.51712\n",
            "\tTrain loss: 0.02281, Accuracy: 1195/1692 (70.63%)\n",
            "\tValidation loss: 0.00305, Accuracy: 200/423 (47.28%)\n",
            "\tTest loss: 0.00285, Accuracy: 223/443 (50.34%)\n",
            "\n",
            "Training Progress: \tEpoch 43 [0/1692 (0.00%)]\t\tLoss: 1.05548\n",
            "Training Progress: \tEpoch 43 [320/1692 (18.87%)]\t\tLoss: 0.68438\n",
            "Training Progress: \tEpoch 43 [640/1692 (37.74%)]\t\tLoss: 0.79070\n",
            "Training Progress: \tEpoch 43 [960/1692 (56.60%)]\t\tLoss: 0.58593\n",
            "Training Progress: \tEpoch 43 [1280/1692 (75.47%)]\t\tLoss: 0.92688\n",
            "Training Progress: \tEpoch 43 [1600/1692 (94.34%)]\t\tLoss: 0.47311\n",
            "\tTrain loss: 0.02001, Accuracy: 1264/1692 (74.70%)\n",
            "\tValidation loss: 0.00292, Accuracy: 198/423 (46.81%)\n",
            "\tTest loss: 0.00283, Accuracy: 219/443 (49.44%)\n",
            "\n",
            "Training Progress: \tEpoch 44 [0/1692 (0.00%)]\t\tLoss: 0.94865\n",
            "Training Progress: \tEpoch 44 [320/1692 (18.87%)]\t\tLoss: 0.82322\n",
            "Training Progress: \tEpoch 44 [640/1692 (37.74%)]\t\tLoss: 0.54649\n",
            "Training Progress: \tEpoch 44 [960/1692 (56.60%)]\t\tLoss: 0.77232\n",
            "Training Progress: \tEpoch 44 [1280/1692 (75.47%)]\t\tLoss: 0.75383\n",
            "Training Progress: \tEpoch 44 [1600/1692 (94.34%)]\t\tLoss: 0.53814\n",
            "\tTrain loss: 0.02093, Accuracy: 1263/1692 (74.65%)\n",
            "\tValidation loss: 0.00294, Accuracy: 210/423 (49.65%)\n",
            "\tTest loss: 0.00277, Accuracy: 220/443 (49.66%)\n",
            "\n",
            "Training Progress: \tEpoch 45 [0/1692 (0.00%)]\t\tLoss: 0.86893\n",
            "Training Progress: \tEpoch 45 [320/1692 (18.87%)]\t\tLoss: 0.82319\n",
            "Training Progress: \tEpoch 45 [640/1692 (37.74%)]\t\tLoss: 0.84666\n",
            "Training Progress: \tEpoch 45 [960/1692 (56.60%)]\t\tLoss: 0.70813\n",
            "Training Progress: \tEpoch 45 [1280/1692 (75.47%)]\t\tLoss: 0.88967\n",
            "Training Progress: \tEpoch 45 [1600/1692 (94.34%)]\t\tLoss: 0.67108\n",
            "\tTrain loss: 0.01948, Accuracy: 1277/1692 (75.47%)\n",
            "\tValidation loss: 0.00302, Accuracy: 208/423 (49.17%)\n",
            "\tTest loss: 0.00282, Accuracy: 224/443 (50.56%)\n",
            "\n",
            "Training Progress: \tEpoch 46 [0/1692 (0.00%)]\t\tLoss: 0.98334\n",
            "Training Progress: \tEpoch 46 [320/1692 (18.87%)]\t\tLoss: 0.90173\n",
            "Training Progress: \tEpoch 46 [640/1692 (37.74%)]\t\tLoss: 0.73323\n",
            "Training Progress: \tEpoch 46 [960/1692 (56.60%)]\t\tLoss: 0.57216\n",
            "Training Progress: \tEpoch 46 [1280/1692 (75.47%)]\t\tLoss: 0.70987\n",
            "Training Progress: \tEpoch 46 [1600/1692 (94.34%)]\t\tLoss: 0.69710\n",
            "\tTrain loss: 0.02014, Accuracy: 1244/1692 (73.52%)\n",
            "\tValidation loss: 0.00307, Accuracy: 208/423 (49.17%)\n",
            "\tTest loss: 0.00295, Accuracy: 219/443 (49.44%)\n",
            "\n",
            "Training Progress: \tEpoch 47 [0/1692 (0.00%)]\t\tLoss: 0.85732\n",
            "Training Progress: \tEpoch 47 [320/1692 (18.87%)]\t\tLoss: 0.71750\n",
            "Training Progress: \tEpoch 47 [640/1692 (37.74%)]\t\tLoss: 0.79700\n",
            "Training Progress: \tEpoch 47 [960/1692 (56.60%)]\t\tLoss: 0.61303\n",
            "Training Progress: \tEpoch 47 [1280/1692 (75.47%)]\t\tLoss: 0.79542\n",
            "Training Progress: \tEpoch 47 [1600/1692 (94.34%)]\t\tLoss: 0.66730\n",
            "\tTrain loss: 0.01763, Accuracy: 1356/1692 (80.14%)\n",
            "\tValidation loss: 0.00277, Accuracy: 226/423 (53.43%)\n",
            "\tTest loss: 0.00276, Accuracy: 223/443 (50.34%)\n",
            "\n",
            "Training Progress: \tEpoch 48 [0/1692 (0.00%)]\t\tLoss: 1.14981\n",
            "Training Progress: \tEpoch 48 [320/1692 (18.87%)]\t\tLoss: 0.92962\n",
            "Training Progress: \tEpoch 48 [640/1692 (37.74%)]\t\tLoss: 0.77347\n",
            "Training Progress: \tEpoch 48 [960/1692 (56.60%)]\t\tLoss: 0.89302\n",
            "Training Progress: \tEpoch 48 [1280/1692 (75.47%)]\t\tLoss: 1.02162\n",
            "Training Progress: \tEpoch 48 [1600/1692 (94.34%)]\t\tLoss: 0.43589\n",
            "\tTrain loss: 0.02095, Accuracy: 1277/1692 (75.47%)\n",
            "\tValidation loss: 0.00303, Accuracy: 205/423 (48.46%)\n",
            "\tTest loss: 0.00294, Accuracy: 216/443 (48.76%)\n",
            "\n",
            "Training Progress: \tEpoch 49 [0/1692 (0.00%)]\t\tLoss: 0.85278\n",
            "Training Progress: \tEpoch 49 [320/1692 (18.87%)]\t\tLoss: 0.64791\n",
            "Training Progress: \tEpoch 49 [640/1692 (37.74%)]\t\tLoss: 0.45776\n",
            "Training Progress: \tEpoch 49 [960/1692 (56.60%)]\t\tLoss: 0.51686\n",
            "Training Progress: \tEpoch 49 [1280/1692 (75.47%)]\t\tLoss: 0.74450\n",
            "Training Progress: \tEpoch 49 [1600/1692 (94.34%)]\t\tLoss: 0.48765\n",
            "\tTrain loss: 0.01746, Accuracy: 1338/1692 (79.08%)\n",
            "\tValidation loss: 0.00300, Accuracy: 212/423 (50.12%)\n",
            "\tTest loss: 0.00292, Accuracy: 219/443 (49.44%)\n",
            "\n",
            "Training Progress: \tEpoch 50 [0/1692 (0.00%)]\t\tLoss: 0.80310\n",
            "Training Progress: \tEpoch 50 [320/1692 (18.87%)]\t\tLoss: 0.82031\n",
            "Training Progress: \tEpoch 50 [640/1692 (37.74%)]\t\tLoss: 0.54495\n",
            "Training Progress: \tEpoch 50 [960/1692 (56.60%)]\t\tLoss: 0.59951\n",
            "Training Progress: \tEpoch 50 [1280/1692 (75.47%)]\t\tLoss: 1.02087\n",
            "Training Progress: \tEpoch 50 [1600/1692 (94.34%)]\t\tLoss: 0.64129\n",
            "\tTrain loss: 0.01874, Accuracy: 1316/1692 (77.78%)\n",
            "\tValidation loss: 0.00296, Accuracy: 208/423 (49.17%)\n",
            "\tTest loss: 0.00287, Accuracy: 217/443 (48.98%)\n",
            "\n",
            "Training Progress: \tEpoch 51 [0/1692 (0.00%)]\t\tLoss: 1.03757\n",
            "Training Progress: \tEpoch 51 [320/1692 (18.87%)]\t\tLoss: 0.61257\n",
            "Training Progress: \tEpoch 51 [640/1692 (37.74%)]\t\tLoss: 0.81617\n",
            "Training Progress: \tEpoch 51 [960/1692 (56.60%)]\t\tLoss: 0.64363\n",
            "Training Progress: \tEpoch 51 [1280/1692 (75.47%)]\t\tLoss: 0.83598\n",
            "Training Progress: \tEpoch 51 [1600/1692 (94.34%)]\t\tLoss: 0.61231\n",
            "\tTrain loss: 0.01634, Accuracy: 1359/1692 (80.32%)\n",
            "\tValidation loss: 0.00286, Accuracy: 212/423 (50.12%)\n",
            "\tTest loss: 0.00283, Accuracy: 226/443 (51.02%)\n",
            "\n",
            "Training Progress: \tEpoch 52 [0/1692 (0.00%)]\t\tLoss: 0.82322\n",
            "Training Progress: \tEpoch 52 [320/1692 (18.87%)]\t\tLoss: 0.52370\n",
            "Training Progress: \tEpoch 52 [640/1692 (37.74%)]\t\tLoss: 0.84621\n",
            "Training Progress: \tEpoch 52 [960/1692 (56.60%)]\t\tLoss: 0.56503\n",
            "Training Progress: \tEpoch 52 [1280/1692 (75.47%)]\t\tLoss: 0.81616\n",
            "Training Progress: \tEpoch 52 [1600/1692 (94.34%)]\t\tLoss: 0.62067\n",
            "\tTrain loss: 0.01759, Accuracy: 1320/1692 (78.01%)\n",
            "\tValidation loss: 0.00298, Accuracy: 212/423 (50.12%)\n",
            "\tTest loss: 0.00295, Accuracy: 221/443 (49.89%)\n",
            "\n",
            "Training Progress: \tEpoch 53 [0/1692 (0.00%)]\t\tLoss: 0.73893\n",
            "Training Progress: \tEpoch 53 [320/1692 (18.87%)]\t\tLoss: 0.81120\n",
            "Training Progress: \tEpoch 53 [640/1692 (37.74%)]\t\tLoss: 0.61268\n",
            "Training Progress: \tEpoch 53 [960/1692 (56.60%)]\t\tLoss: 0.60759\n",
            "Training Progress: \tEpoch 53 [1280/1692 (75.47%)]\t\tLoss: 0.86212\n",
            "Training Progress: \tEpoch 53 [1600/1692 (94.34%)]\t\tLoss: 0.51720\n",
            "\tTrain loss: 0.01581, Accuracy: 1386/1692 (81.91%)\n",
            "\tValidation loss: 0.00299, Accuracy: 215/423 (50.83%)\n",
            "\tTest loss: 0.00293, Accuracy: 226/443 (51.02%)\n",
            "\n",
            "Training Progress: \tEpoch 54 [0/1692 (0.00%)]\t\tLoss: 0.91880\n",
            "Training Progress: \tEpoch 54 [320/1692 (18.87%)]\t\tLoss: 0.79310\n",
            "Training Progress: \tEpoch 54 [640/1692 (37.74%)]\t\tLoss: 0.67895\n",
            "Training Progress: \tEpoch 54 [960/1692 (56.60%)]\t\tLoss: 0.90600\n",
            "Training Progress: \tEpoch 54 [1280/1692 (75.47%)]\t\tLoss: 0.85864\n",
            "Training Progress: \tEpoch 54 [1600/1692 (94.34%)]\t\tLoss: 0.67194\n",
            "\tTrain loss: 0.01523, Accuracy: 1401/1692 (82.80%)\n",
            "\tValidation loss: 0.00299, Accuracy: 221/423 (52.25%)\n",
            "\tTest loss: 0.00300, Accuracy: 221/443 (49.89%)\n",
            "\n",
            "Training Progress: \tEpoch 55 [0/1692 (0.00%)]\t\tLoss: 1.05676\n",
            "Training Progress: \tEpoch 55 [320/1692 (18.87%)]\t\tLoss: 0.76333\n",
            "Training Progress: \tEpoch 55 [640/1692 (37.74%)]\t\tLoss: 0.75221\n",
            "Training Progress: \tEpoch 55 [960/1692 (56.60%)]\t\tLoss: 0.60282\n",
            "Training Progress: \tEpoch 55 [1280/1692 (75.47%)]\t\tLoss: 0.69531\n",
            "Training Progress: \tEpoch 55 [1600/1692 (94.34%)]\t\tLoss: 0.42174\n",
            "\tTrain loss: 0.01298, Accuracy: 1465/1692 (86.58%)\n",
            "\tValidation loss: 0.00294, Accuracy: 211/423 (49.88%)\n",
            "\tTest loss: 0.00291, Accuracy: 226/443 (51.02%)\n",
            "\n",
            "Training Progress: \tEpoch 56 [0/1692 (0.00%)]\t\tLoss: 0.73971\n",
            "Training Progress: \tEpoch 56 [320/1692 (18.87%)]\t\tLoss: 0.74228\n",
            "Training Progress: \tEpoch 56 [640/1692 (37.74%)]\t\tLoss: 0.80209\n",
            "Training Progress: \tEpoch 56 [960/1692 (56.60%)]\t\tLoss: 0.52933\n",
            "Training Progress: \tEpoch 56 [1280/1692 (75.47%)]\t\tLoss: 0.63875\n",
            "Training Progress: \tEpoch 56 [1600/1692 (94.34%)]\t\tLoss: 0.55694\n",
            "\tTrain loss: 0.01733, Accuracy: 1300/1692 (76.83%)\n",
            "\tValidation loss: 0.00339, Accuracy: 192/423 (45.39%)\n",
            "\tTest loss: 0.00339, Accuracy: 203/443 (45.82%)\n",
            "\n",
            "Training Progress: \tEpoch 57 [0/1692 (0.00%)]\t\tLoss: 0.89055\n",
            "Training Progress: \tEpoch 57 [320/1692 (18.87%)]\t\tLoss: 0.59449\n",
            "Training Progress: \tEpoch 57 [640/1692 (37.74%)]\t\tLoss: 0.69244\n",
            "Training Progress: \tEpoch 57 [960/1692 (56.60%)]\t\tLoss: 0.51627\n",
            "Training Progress: \tEpoch 57 [1280/1692 (75.47%)]\t\tLoss: 0.65325\n",
            "Training Progress: \tEpoch 57 [1600/1692 (94.34%)]\t\tLoss: 0.51949\n",
            "\tTrain loss: 0.01451, Accuracy: 1418/1692 (83.81%)\n",
            "\tValidation loss: 0.00313, Accuracy: 209/423 (49.41%)\n",
            "\tTest loss: 0.00312, Accuracy: 215/443 (48.53%)\n",
            "\n",
            "Training Progress: \tEpoch 58 [0/1692 (0.00%)]\t\tLoss: 0.70850\n",
            "Training Progress: \tEpoch 58 [320/1692 (18.87%)]\t\tLoss: 0.76421\n",
            "Training Progress: \tEpoch 58 [640/1692 (37.74%)]\t\tLoss: 0.68865\n",
            "Training Progress: \tEpoch 58 [960/1692 (56.60%)]\t\tLoss: 0.61528\n",
            "Training Progress: \tEpoch 58 [1280/1692 (75.47%)]\t\tLoss: 0.83203\n",
            "Training Progress: \tEpoch 58 [1600/1692 (94.34%)]\t\tLoss: 0.39477\n",
            "\tTrain loss: 0.01358, Accuracy: 1443/1692 (85.28%)\n",
            "\tValidation loss: 0.00310, Accuracy: 210/423 (49.65%)\n",
            "\tTest loss: 0.00303, Accuracy: 217/443 (48.98%)\n",
            "\n",
            "Training Progress: \tEpoch 59 [0/1692 (0.00%)]\t\tLoss: 0.74425\n",
            "Training Progress: \tEpoch 59 [320/1692 (18.87%)]\t\tLoss: 0.52656\n",
            "Training Progress: \tEpoch 59 [640/1692 (37.74%)]\t\tLoss: 0.49071\n",
            "Training Progress: \tEpoch 59 [960/1692 (56.60%)]\t\tLoss: 0.58242\n",
            "Training Progress: \tEpoch 59 [1280/1692 (75.47%)]\t\tLoss: 0.89281\n",
            "Training Progress: \tEpoch 59 [1600/1692 (94.34%)]\t\tLoss: 0.58433\n",
            "\tTrain loss: 0.01566, Accuracy: 1367/1692 (80.79%)\n",
            "\tValidation loss: 0.00329, Accuracy: 205/423 (48.46%)\n",
            "\tTest loss: 0.00323, Accuracy: 218/443 (49.21%)\n",
            "\n",
            "Training Progress: \tEpoch 60 [0/1692 (0.00%)]\t\tLoss: 0.86665\n",
            "Training Progress: \tEpoch 60 [320/1692 (18.87%)]\t\tLoss: 0.51412\n",
            "Training Progress: \tEpoch 60 [640/1692 (37.74%)]\t\tLoss: 0.68431\n",
            "Training Progress: \tEpoch 60 [960/1692 (56.60%)]\t\tLoss: 0.69376\n",
            "Training Progress: \tEpoch 60 [1280/1692 (75.47%)]\t\tLoss: 0.70852\n",
            "Training Progress: \tEpoch 60 [1600/1692 (94.34%)]\t\tLoss: 0.35630\n",
            "\tTrain loss: 0.01199, Accuracy: 1476/1692 (87.23%)\n",
            "\tValidation loss: 0.00299, Accuracy: 204/423 (48.23%)\n",
            "\tTest loss: 0.00296, Accuracy: 222/443 (50.11%)\n",
            "\n",
            "Training Progress: \tEpoch 61 [0/1692 (0.00%)]\t\tLoss: 0.66656\n",
            "Training Progress: \tEpoch 61 [320/1692 (18.87%)]\t\tLoss: 0.64222\n",
            "Training Progress: \tEpoch 61 [640/1692 (37.74%)]\t\tLoss: 0.59584\n",
            "Training Progress: \tEpoch 61 [960/1692 (56.60%)]\t\tLoss: 0.50402\n",
            "Training Progress: \tEpoch 61 [1280/1692 (75.47%)]\t\tLoss: 0.48664\n",
            "Training Progress: \tEpoch 61 [1600/1692 (94.34%)]\t\tLoss: 0.60753\n",
            "\tTrain loss: 0.01483, Accuracy: 1402/1692 (82.86%)\n",
            "\tValidation loss: 0.00309, Accuracy: 200/423 (47.28%)\n",
            "\tTest loss: 0.00304, Accuracy: 226/443 (51.02%)\n",
            "\n",
            "Training Progress: \tEpoch 62 [0/1692 (0.00%)]\t\tLoss: 0.82946\n",
            "Training Progress: \tEpoch 62 [320/1692 (18.87%)]\t\tLoss: 0.56474\n",
            "Training Progress: \tEpoch 62 [640/1692 (37.74%)]\t\tLoss: 0.55744\n",
            "Training Progress: \tEpoch 62 [960/1692 (56.60%)]\t\tLoss: 0.49718\n",
            "Training Progress: \tEpoch 62 [1280/1692 (75.47%)]\t\tLoss: 0.47844\n",
            "Training Progress: \tEpoch 62 [1600/1692 (94.34%)]\t\tLoss: 0.50729\n",
            "\tTrain loss: 0.01305, Accuracy: 1433/1692 (84.69%)\n",
            "\tValidation loss: 0.00346, Accuracy: 194/423 (45.86%)\n",
            "\tTest loss: 0.00332, Accuracy: 212/443 (47.86%)\n",
            "\n",
            "Training Progress: \tEpoch 63 [0/1692 (0.00%)]\t\tLoss: 0.69448\n",
            "Training Progress: \tEpoch 63 [320/1692 (18.87%)]\t\tLoss: 0.60548\n",
            "Training Progress: \tEpoch 63 [640/1692 (37.74%)]\t\tLoss: 0.46782\n",
            "Training Progress: \tEpoch 63 [960/1692 (56.60%)]\t\tLoss: 0.42604\n",
            "Training Progress: \tEpoch 63 [1280/1692 (75.47%)]\t\tLoss: 0.65884\n",
            "Training Progress: \tEpoch 63 [1600/1692 (94.34%)]\t\tLoss: 0.36496\n",
            "\tTrain loss: 0.01346, Accuracy: 1410/1692 (83.33%)\n",
            "\tValidation loss: 0.00324, Accuracy: 200/423 (47.28%)\n",
            "\tTest loss: 0.00309, Accuracy: 224/443 (50.56%)\n",
            "\n",
            "Training Progress: \tEpoch 64 [0/1692 (0.00%)]\t\tLoss: 0.88938\n",
            "Training Progress: \tEpoch 64 [320/1692 (18.87%)]\t\tLoss: 0.63034\n",
            "Training Progress: \tEpoch 64 [640/1692 (37.74%)]\t\tLoss: 0.44338\n",
            "Training Progress: \tEpoch 64 [960/1692 (56.60%)]\t\tLoss: 0.50296\n",
            "Training Progress: \tEpoch 64 [1280/1692 (75.47%)]\t\tLoss: 0.79496\n",
            "Training Progress: \tEpoch 64 [1600/1692 (94.34%)]\t\tLoss: 0.47595\n",
            "\tTrain loss: 0.01116, Accuracy: 1475/1692 (87.17%)\n",
            "\tValidation loss: 0.00308, Accuracy: 221/423 (52.25%)\n",
            "\tTest loss: 0.00306, Accuracy: 226/443 (51.02%)\n",
            "\n",
            "Training Progress: \tEpoch 65 [0/1692 (0.00%)]\t\tLoss: 0.72875\n",
            "Training Progress: \tEpoch 65 [320/1692 (18.87%)]\t\tLoss: 0.72317\n",
            "Training Progress: \tEpoch 65 [640/1692 (37.74%)]\t\tLoss: 0.56535\n",
            "Training Progress: \tEpoch 65 [960/1692 (56.60%)]\t\tLoss: 0.58838\n",
            "Training Progress: \tEpoch 65 [1280/1692 (75.47%)]\t\tLoss: 0.67432\n",
            "Training Progress: \tEpoch 65 [1600/1692 (94.34%)]\t\tLoss: 0.33579\n",
            "\tTrain loss: 0.01153, Accuracy: 1480/1692 (87.47%)\n",
            "\tValidation loss: 0.00325, Accuracy: 199/423 (47.04%)\n",
            "\tTest loss: 0.00311, Accuracy: 218/443 (49.21%)\n",
            "\n",
            "Training Progress: \tEpoch 66 [0/1692 (0.00%)]\t\tLoss: 0.74190\n",
            "Training Progress: \tEpoch 66 [320/1692 (18.87%)]\t\tLoss: 0.36940\n",
            "Training Progress: \tEpoch 66 [640/1692 (37.74%)]\t\tLoss: 0.77486\n",
            "Training Progress: \tEpoch 66 [960/1692 (56.60%)]\t\tLoss: 0.64247\n",
            "Training Progress: \tEpoch 66 [1280/1692 (75.47%)]\t\tLoss: 0.81293\n",
            "Training Progress: \tEpoch 66 [1600/1692 (94.34%)]\t\tLoss: 0.59234\n",
            "\tTrain loss: 0.00950, Accuracy: 1525/1692 (90.13%)\n",
            "\tValidation loss: 0.00322, Accuracy: 212/423 (50.12%)\n",
            "\tTest loss: 0.00329, Accuracy: 229/443 (51.69%)\n",
            "\n",
            "Training Progress: \tEpoch 67 [0/1692 (0.00%)]\t\tLoss: 0.65844\n",
            "Training Progress: \tEpoch 67 [320/1692 (18.87%)]\t\tLoss: 0.50292\n",
            "Training Progress: \tEpoch 67 [640/1692 (37.74%)]\t\tLoss: 0.66869\n",
            "Training Progress: \tEpoch 67 [960/1692 (56.60%)]\t\tLoss: 0.47428\n",
            "Training Progress: \tEpoch 67 [1280/1692 (75.47%)]\t\tLoss: 0.46719\n",
            "Training Progress: \tEpoch 67 [1600/1692 (94.34%)]\t\tLoss: 0.46091\n",
            "\tTrain loss: 0.01238, Accuracy: 1447/1692 (85.52%)\n",
            "\tValidation loss: 0.00326, Accuracy: 204/423 (48.23%)\n",
            "\tTest loss: 0.00333, Accuracy: 212/443 (47.86%)\n",
            "\n",
            "Training Progress: \tEpoch 68 [0/1692 (0.00%)]\t\tLoss: 0.53881\n",
            "Training Progress: \tEpoch 68 [320/1692 (18.87%)]\t\tLoss: 0.74414\n",
            "Training Progress: \tEpoch 68 [640/1692 (37.74%)]\t\tLoss: 0.54858\n",
            "Training Progress: \tEpoch 68 [960/1692 (56.60%)]\t\tLoss: 0.55165\n",
            "Training Progress: \tEpoch 68 [1280/1692 (75.47%)]\t\tLoss: 0.34611\n",
            "Training Progress: \tEpoch 68 [1600/1692 (94.34%)]\t\tLoss: 0.37272\n",
            "\tTrain loss: 0.01172, Accuracy: 1471/1692 (86.94%)\n",
            "\tValidation loss: 0.00338, Accuracy: 194/423 (45.86%)\n",
            "\tTest loss: 0.00328, Accuracy: 219/443 (49.44%)\n",
            "\n",
            "Training Progress: \tEpoch 69 [0/1692 (0.00%)]\t\tLoss: 0.66227\n",
            "Training Progress: \tEpoch 69 [320/1692 (18.87%)]\t\tLoss: 0.46763\n",
            "Training Progress: \tEpoch 69 [640/1692 (37.74%)]\t\tLoss: 0.38504\n",
            "Training Progress: \tEpoch 69 [960/1692 (56.60%)]\t\tLoss: 0.55223\n",
            "Training Progress: \tEpoch 69 [1280/1692 (75.47%)]\t\tLoss: 0.33702\n",
            "Training Progress: \tEpoch 69 [1600/1692 (94.34%)]\t\tLoss: 0.42183\n",
            "\tTrain loss: 0.00967, Accuracy: 1512/1692 (89.36%)\n",
            "\tValidation loss: 0.00329, Accuracy: 208/423 (49.17%)\n",
            "\tTest loss: 0.00323, Accuracy: 232/443 (52.37%)\n",
            "\n",
            "Training Progress: \tEpoch 70 [0/1692 (0.00%)]\t\tLoss: 0.56997\n",
            "Training Progress: \tEpoch 70 [320/1692 (18.87%)]\t\tLoss: 0.59770\n",
            "Training Progress: \tEpoch 70 [640/1692 (37.74%)]\t\tLoss: 0.62153\n",
            "Training Progress: \tEpoch 70 [960/1692 (56.60%)]\t\tLoss: 0.56659\n",
            "Training Progress: \tEpoch 70 [1280/1692 (75.47%)]\t\tLoss: 0.51955\n",
            "Training Progress: \tEpoch 70 [1600/1692 (94.34%)]\t\tLoss: 0.50384\n",
            "\tTrain loss: 0.01080, Accuracy: 1466/1692 (86.64%)\n",
            "\tValidation loss: 0.00362, Accuracy: 193/423 (45.63%)\n",
            "\tTest loss: 0.00365, Accuracy: 217/443 (48.98%)\n",
            "\n",
            "Training Progress: \tEpoch 71 [0/1692 (0.00%)]\t\tLoss: 0.59543\n",
            "Training Progress: \tEpoch 71 [320/1692 (18.87%)]\t\tLoss: 0.42868\n",
            "Training Progress: \tEpoch 71 [640/1692 (37.74%)]\t\tLoss: 0.43972\n",
            "Training Progress: \tEpoch 71 [960/1692 (56.60%)]\t\tLoss: 0.66725\n",
            "Training Progress: \tEpoch 71 [1280/1692 (75.47%)]\t\tLoss: 0.51044\n",
            "Training Progress: \tEpoch 71 [1600/1692 (94.34%)]\t\tLoss: 0.25956\n",
            "\tTrain loss: 0.00999, Accuracy: 1531/1692 (90.48%)\n",
            "\tValidation loss: 0.00326, Accuracy: 200/423 (47.28%)\n",
            "\tTest loss: 0.00312, Accuracy: 234/443 (52.82%)\n",
            "\n",
            "Training Progress: \tEpoch 72 [0/1692 (0.00%)]\t\tLoss: 0.65462\n",
            "Training Progress: \tEpoch 72 [320/1692 (18.87%)]\t\tLoss: 0.28376\n",
            "Training Progress: \tEpoch 72 [640/1692 (37.74%)]\t\tLoss: 0.76166\n",
            "Training Progress: \tEpoch 72 [960/1692 (56.60%)]\t\tLoss: 0.61009\n",
            "Training Progress: \tEpoch 72 [1280/1692 (75.47%)]\t\tLoss: 0.69036\n",
            "Training Progress: \tEpoch 72 [1600/1692 (94.34%)]\t\tLoss: 0.31287\n",
            "\tTrain loss: 0.00969, Accuracy: 1516/1692 (89.60%)\n",
            "\tValidation loss: 0.00337, Accuracy: 199/423 (47.04%)\n",
            "\tTest loss: 0.00344, Accuracy: 218/443 (49.21%)\n",
            "\n",
            "Training Progress: \tEpoch 73 [0/1692 (0.00%)]\t\tLoss: 0.69813\n",
            "Training Progress: \tEpoch 73 [320/1692 (18.87%)]\t\tLoss: 0.50795\n",
            "Training Progress: \tEpoch 73 [640/1692 (37.74%)]\t\tLoss: 0.45300\n",
            "Training Progress: \tEpoch 73 [960/1692 (56.60%)]\t\tLoss: 0.32578\n",
            "Training Progress: \tEpoch 73 [1280/1692 (75.47%)]\t\tLoss: 0.48222\n",
            "Training Progress: \tEpoch 73 [1600/1692 (94.34%)]\t\tLoss: 0.55598\n",
            "\tTrain loss: 0.00981, Accuracy: 1512/1692 (89.36%)\n",
            "\tValidation loss: 0.00351, Accuracy: 198/423 (46.81%)\n",
            "\tTest loss: 0.00338, Accuracy: 227/443 (51.24%)\n",
            "\n",
            "Training Progress: \tEpoch 74 [0/1692 (0.00%)]\t\tLoss: 0.58198\n",
            "Training Progress: \tEpoch 74 [320/1692 (18.87%)]\t\tLoss: 0.54908\n",
            "Training Progress: \tEpoch 74 [640/1692 (37.74%)]\t\tLoss: 0.60614\n",
            "Training Progress: \tEpoch 74 [960/1692 (56.60%)]\t\tLoss: 0.59179\n",
            "Training Progress: \tEpoch 74 [1280/1692 (75.47%)]\t\tLoss: 0.45334\n",
            "Training Progress: \tEpoch 74 [1600/1692 (94.34%)]\t\tLoss: 0.58970\n",
            "\tTrain loss: 0.00817, Accuracy: 1555/1692 (91.90%)\n",
            "\tValidation loss: 0.00321, Accuracy: 214/423 (50.59%)\n",
            "\tTest loss: 0.00318, Accuracy: 225/443 (50.79%)\n",
            "\n",
            "Training Progress: \tEpoch 75 [0/1692 (0.00%)]\t\tLoss: 0.56022\n",
            "Training Progress: \tEpoch 75 [320/1692 (18.87%)]\t\tLoss: 0.49697\n",
            "Training Progress: \tEpoch 75 [640/1692 (37.74%)]\t\tLoss: 0.53907\n",
            "Training Progress: \tEpoch 75 [960/1692 (56.60%)]\t\tLoss: 0.49016\n",
            "Training Progress: \tEpoch 75 [1280/1692 (75.47%)]\t\tLoss: 0.36935\n",
            "Training Progress: \tEpoch 75 [1600/1692 (94.34%)]\t\tLoss: 0.53764\n",
            "\tTrain loss: 0.00858, Accuracy: 1546/1692 (91.37%)\n",
            "\tValidation loss: 0.00349, Accuracy: 205/423 (48.46%)\n",
            "\tTest loss: 0.00339, Accuracy: 228/443 (51.47%)\n",
            "\n",
            "Training Progress: \tEpoch 76 [0/1692 (0.00%)]\t\tLoss: 0.79257\n",
            "Training Progress: \tEpoch 76 [320/1692 (18.87%)]\t\tLoss: 0.50705\n",
            "Training Progress: \tEpoch 76 [640/1692 (37.74%)]\t\tLoss: 0.58383\n",
            "Training Progress: \tEpoch 76 [960/1692 (56.60%)]\t\tLoss: 0.54984\n",
            "Training Progress: \tEpoch 76 [1280/1692 (75.47%)]\t\tLoss: 0.40630\n",
            "Training Progress: \tEpoch 76 [1600/1692 (94.34%)]\t\tLoss: 0.36868\n",
            "\tTrain loss: 0.00998, Accuracy: 1518/1692 (89.72%)\n",
            "\tValidation loss: 0.00352, Accuracy: 204/423 (48.23%)\n",
            "\tTest loss: 0.00337, Accuracy: 227/443 (51.24%)\n",
            "\n",
            "Training Progress: \tEpoch 77 [0/1692 (0.00%)]\t\tLoss: 0.60082\n",
            "Training Progress: \tEpoch 77 [320/1692 (18.87%)]\t\tLoss: 0.20668\n",
            "Training Progress: \tEpoch 77 [640/1692 (37.74%)]\t\tLoss: 0.43581\n",
            "Training Progress: \tEpoch 77 [960/1692 (56.60%)]\t\tLoss: 0.60819\n",
            "Training Progress: \tEpoch 77 [1280/1692 (75.47%)]\t\tLoss: 0.41349\n",
            "Training Progress: \tEpoch 77 [1600/1692 (94.34%)]\t\tLoss: 0.33772\n",
            "\tTrain loss: 0.00824, Accuracy: 1545/1692 (91.31%)\n",
            "\tValidation loss: 0.00356, Accuracy: 200/423 (47.28%)\n",
            "\tTest loss: 0.00344, Accuracy: 219/443 (49.44%)\n",
            "\n",
            "Training Progress: \tEpoch 78 [0/1692 (0.00%)]\t\tLoss: 0.39421\n",
            "Training Progress: \tEpoch 78 [320/1692 (18.87%)]\t\tLoss: 0.24974\n",
            "Training Progress: \tEpoch 78 [640/1692 (37.74%)]\t\tLoss: 0.29271\n",
            "Training Progress: \tEpoch 78 [960/1692 (56.60%)]\t\tLoss: 0.47296\n",
            "Training Progress: \tEpoch 78 [1280/1692 (75.47%)]\t\tLoss: 0.67261\n",
            "Training Progress: \tEpoch 78 [1600/1692 (94.34%)]\t\tLoss: 0.36376\n",
            "\tTrain loss: 0.00767, Accuracy: 1561/1692 (92.26%)\n",
            "\tValidation loss: 0.00350, Accuracy: 203/423 (47.99%)\n",
            "\tTest loss: 0.00351, Accuracy: 216/443 (48.76%)\n",
            "\n",
            "Training Progress: \tEpoch 79 [0/1692 (0.00%)]\t\tLoss: 0.35737\n",
            "Training Progress: \tEpoch 79 [320/1692 (18.87%)]\t\tLoss: 0.52807\n",
            "Training Progress: \tEpoch 79 [640/1692 (37.74%)]\t\tLoss: 0.52559\n",
            "Training Progress: \tEpoch 79 [960/1692 (56.60%)]\t\tLoss: 0.42935\n",
            "Training Progress: \tEpoch 79 [1280/1692 (75.47%)]\t\tLoss: 0.58545\n",
            "Training Progress: \tEpoch 79 [1600/1692 (94.34%)]\t\tLoss: 0.50423\n",
            "\tTrain loss: 0.00688, Accuracy: 1588/1692 (93.85%)\n",
            "\tValidation loss: 0.00324, Accuracy: 214/423 (50.59%)\n",
            "\tTest loss: 0.00336, Accuracy: 223/443 (50.34%)\n",
            "\n",
            "Training Progress: \tEpoch 80 [0/1692 (0.00%)]\t\tLoss: 0.81995\n",
            "Training Progress: \tEpoch 80 [320/1692 (18.87%)]\t\tLoss: 0.48001\n",
            "Training Progress: \tEpoch 80 [640/1692 (37.74%)]\t\tLoss: 0.40890\n",
            "Training Progress: \tEpoch 80 [960/1692 (56.60%)]\t\tLoss: 0.45902\n",
            "Training Progress: \tEpoch 80 [1280/1692 (75.47%)]\t\tLoss: 0.56081\n",
            "Training Progress: \tEpoch 80 [1600/1692 (94.34%)]\t\tLoss: 0.28229\n",
            "\tTrain loss: 0.00694, Accuracy: 1594/1692 (94.21%)\n",
            "\tValidation loss: 0.00315, Accuracy: 214/423 (50.59%)\n",
            "\tTest loss: 0.00326, Accuracy: 218/443 (49.21%)\n",
            "\n",
            "Training Progress: \tEpoch 81 [0/1692 (0.00%)]\t\tLoss: 0.44609\n",
            "Training Progress: \tEpoch 81 [320/1692 (18.87%)]\t\tLoss: 0.45429\n",
            "Training Progress: \tEpoch 81 [640/1692 (37.74%)]\t\tLoss: 0.35493\n",
            "Training Progress: \tEpoch 81 [960/1692 (56.60%)]\t\tLoss: 0.35935\n",
            "Training Progress: \tEpoch 81 [1280/1692 (75.47%)]\t\tLoss: 0.38351\n",
            "Training Progress: \tEpoch 81 [1600/1692 (94.34%)]\t\tLoss: 0.43312\n",
            "\tTrain loss: 0.00618, Accuracy: 1585/1692 (93.68%)\n",
            "\tValidation loss: 0.00348, Accuracy: 222/423 (52.48%)\n",
            "\tTest loss: 0.00353, Accuracy: 225/443 (50.79%)\n",
            "\n",
            "Training Progress: \tEpoch 82 [0/1692 (0.00%)]\t\tLoss: 0.43164\n",
            "Training Progress: \tEpoch 82 [320/1692 (18.87%)]\t\tLoss: 0.33463\n",
            "Training Progress: \tEpoch 82 [640/1692 (37.74%)]\t\tLoss: 0.40394\n",
            "Training Progress: \tEpoch 82 [960/1692 (56.60%)]\t\tLoss: 0.38172\n",
            "Training Progress: \tEpoch 82 [1280/1692 (75.47%)]\t\tLoss: 0.47299\n",
            "Training Progress: \tEpoch 82 [1600/1692 (94.34%)]\t\tLoss: 0.20700\n",
            "\tTrain loss: 0.00766, Accuracy: 1556/1692 (91.96%)\n",
            "\tValidation loss: 0.00362, Accuracy: 205/423 (48.46%)\n",
            "\tTest loss: 0.00360, Accuracy: 214/443 (48.31%)\n",
            "\n",
            "Training Progress: \tEpoch 83 [0/1692 (0.00%)]\t\tLoss: 0.44230\n",
            "Training Progress: \tEpoch 83 [320/1692 (18.87%)]\t\tLoss: 0.34169\n",
            "Training Progress: \tEpoch 83 [640/1692 (37.74%)]\t\tLoss: 0.59396\n",
            "Training Progress: \tEpoch 83 [960/1692 (56.60%)]\t\tLoss: 0.48475\n",
            "Training Progress: \tEpoch 83 [1280/1692 (75.47%)]\t\tLoss: 0.23628\n",
            "Training Progress: \tEpoch 83 [1600/1692 (94.34%)]\t\tLoss: 0.35146\n",
            "\tTrain loss: 0.00599, Accuracy: 1598/1692 (94.44%)\n",
            "\tValidation loss: 0.00348, Accuracy: 218/423 (51.54%)\n",
            "\tTest loss: 0.00352, Accuracy: 225/443 (50.79%)\n",
            "\n",
            "Training Progress: \tEpoch 84 [0/1692 (0.00%)]\t\tLoss: 0.46928\n",
            "Training Progress: \tEpoch 84 [320/1692 (18.87%)]\t\tLoss: 0.46707\n",
            "Training Progress: \tEpoch 84 [640/1692 (37.74%)]\t\tLoss: 0.47523\n",
            "Training Progress: \tEpoch 84 [960/1692 (56.60%)]\t\tLoss: 0.29600\n",
            "Training Progress: \tEpoch 84 [1280/1692 (75.47%)]\t\tLoss: 0.61896\n",
            "Training Progress: \tEpoch 84 [1600/1692 (94.34%)]\t\tLoss: 0.55912\n",
            "\tTrain loss: 0.00667, Accuracy: 1595/1692 (94.27%)\n",
            "\tValidation loss: 0.00345, Accuracy: 214/423 (50.59%)\n",
            "\tTest loss: 0.00345, Accuracy: 217/443 (48.98%)\n",
            "\n",
            "Training Progress: \tEpoch 85 [0/1692 (0.00%)]\t\tLoss: 0.54056\n",
            "Training Progress: \tEpoch 85 [320/1692 (18.87%)]\t\tLoss: 0.36159\n",
            "Training Progress: \tEpoch 85 [640/1692 (37.74%)]\t\tLoss: 0.29093\n",
            "Training Progress: \tEpoch 85 [960/1692 (56.60%)]\t\tLoss: 0.46456\n",
            "Training Progress: \tEpoch 85 [1280/1692 (75.47%)]\t\tLoss: 0.55779\n",
            "Training Progress: \tEpoch 85 [1600/1692 (94.34%)]\t\tLoss: 0.29436\n",
            "\tTrain loss: 0.00756, Accuracy: 1567/1692 (92.61%)\n",
            "\tValidation loss: 0.00377, Accuracy: 207/423 (48.94%)\n",
            "\tTest loss: 0.00365, Accuracy: 219/443 (49.44%)\n",
            "\n",
            "Training Progress: \tEpoch 86 [0/1692 (0.00%)]\t\tLoss: 0.53155\n",
            "Training Progress: \tEpoch 86 [320/1692 (18.87%)]\t\tLoss: 0.44643\n",
            "Training Progress: \tEpoch 86 [640/1692 (37.74%)]\t\tLoss: 0.24780\n",
            "Training Progress: \tEpoch 86 [960/1692 (56.60%)]\t\tLoss: 0.35850\n",
            "Training Progress: \tEpoch 86 [1280/1692 (75.47%)]\t\tLoss: 0.54560\n",
            "Training Progress: \tEpoch 86 [1600/1692 (94.34%)]\t\tLoss: 0.39537\n",
            "\tTrain loss: 0.00631, Accuracy: 1595/1692 (94.27%)\n",
            "\tValidation loss: 0.00360, Accuracy: 203/423 (47.99%)\n",
            "\tTest loss: 0.00351, Accuracy: 214/443 (48.31%)\n",
            "\n",
            "Training Progress: \tEpoch 87 [0/1692 (0.00%)]\t\tLoss: 0.53119\n",
            "Training Progress: \tEpoch 87 [320/1692 (18.87%)]\t\tLoss: 0.48987\n",
            "Training Progress: \tEpoch 87 [640/1692 (37.74%)]\t\tLoss: 0.26659\n",
            "Training Progress: \tEpoch 87 [960/1692 (56.60%)]\t\tLoss: 0.39692\n",
            "Training Progress: \tEpoch 87 [1280/1692 (75.47%)]\t\tLoss: 0.34706\n",
            "Training Progress: \tEpoch 87 [1600/1692 (94.34%)]\t\tLoss: 0.26146\n",
            "\tTrain loss: 0.00649, Accuracy: 1579/1692 (93.32%)\n",
            "\tValidation loss: 0.00384, Accuracy: 203/423 (47.99%)\n",
            "\tTest loss: 0.00370, Accuracy: 217/443 (48.98%)\n",
            "\n",
            "Training Progress: \tEpoch 88 [0/1692 (0.00%)]\t\tLoss: 0.63342\n",
            "Training Progress: \tEpoch 88 [320/1692 (18.87%)]\t\tLoss: 0.27688\n",
            "Training Progress: \tEpoch 88 [640/1692 (37.74%)]\t\tLoss: 0.38429\n",
            "Training Progress: \tEpoch 88 [960/1692 (56.60%)]\t\tLoss: 0.51821\n",
            "Training Progress: \tEpoch 88 [1280/1692 (75.47%)]\t\tLoss: 0.31250\n",
            "Training Progress: \tEpoch 88 [1600/1692 (94.34%)]\t\tLoss: 0.07361\n",
            "\tTrain loss: 0.00677, Accuracy: 1568/1692 (92.67%)\n",
            "\tValidation loss: 0.00401, Accuracy: 195/423 (46.10%)\n",
            "\tTest loss: 0.00384, Accuracy: 231/443 (52.14%)\n",
            "\n",
            "Training Progress: \tEpoch 89 [0/1692 (0.00%)]\t\tLoss: 0.53297\n",
            "Training Progress: \tEpoch 89 [320/1692 (18.87%)]\t\tLoss: 0.37366\n",
            "Training Progress: \tEpoch 89 [640/1692 (37.74%)]\t\tLoss: 0.45025\n",
            "Training Progress: \tEpoch 89 [960/1692 (56.60%)]\t\tLoss: 0.50199\n",
            "Training Progress: \tEpoch 89 [1280/1692 (75.47%)]\t\tLoss: 0.58142\n",
            "Training Progress: \tEpoch 89 [1600/1692 (94.34%)]\t\tLoss: 0.24233\n",
            "\tTrain loss: 0.00780, Accuracy: 1557/1692 (92.02%)\n",
            "\tValidation loss: 0.00390, Accuracy: 193/423 (45.63%)\n",
            "\tTest loss: 0.00391, Accuracy: 205/443 (46.28%)\n",
            "\n",
            "Training Progress: \tEpoch 90 [0/1692 (0.00%)]\t\tLoss: 0.59804\n",
            "Training Progress: \tEpoch 90 [320/1692 (18.87%)]\t\tLoss: 0.50693\n",
            "Training Progress: \tEpoch 90 [640/1692 (37.74%)]\t\tLoss: 0.52484\n",
            "Training Progress: \tEpoch 90 [960/1692 (56.60%)]\t\tLoss: 0.38027\n",
            "Training Progress: \tEpoch 90 [1280/1692 (75.47%)]\t\tLoss: 0.48516\n",
            "Training Progress: \tEpoch 90 [1600/1692 (94.34%)]\t\tLoss: 0.26508\n",
            "\tTrain loss: 0.00510, Accuracy: 1628/1692 (96.22%)\n",
            "\tValidation loss: 0.00344, Accuracy: 213/423 (50.35%)\n",
            "\tTest loss: 0.00334, Accuracy: 222/443 (50.11%)\n",
            "\n",
            "Training Progress: \tEpoch 91 [0/1692 (0.00%)]\t\tLoss: 0.45796\n",
            "Training Progress: \tEpoch 91 [320/1692 (18.87%)]\t\tLoss: 0.47547\n",
            "Training Progress: \tEpoch 91 [640/1692 (37.74%)]\t\tLoss: 0.39093\n",
            "Training Progress: \tEpoch 91 [960/1692 (56.60%)]\t\tLoss: 0.31247\n",
            "Training Progress: \tEpoch 91 [1280/1692 (75.47%)]\t\tLoss: 0.42970\n",
            "Training Progress: \tEpoch 91 [1600/1692 (94.34%)]\t\tLoss: 0.44327\n",
            "\tTrain loss: 0.00554, Accuracy: 1598/1692 (94.44%)\n",
            "\tValidation loss: 0.00375, Accuracy: 203/423 (47.99%)\n",
            "\tTest loss: 0.00364, Accuracy: 226/443 (51.02%)\n",
            "\n",
            "Training Progress: \tEpoch 92 [0/1692 (0.00%)]\t\tLoss: 0.31087\n",
            "Training Progress: \tEpoch 92 [320/1692 (18.87%)]\t\tLoss: 0.36575\n",
            "Training Progress: \tEpoch 92 [640/1692 (37.74%)]\t\tLoss: 0.34889\n",
            "Training Progress: \tEpoch 92 [960/1692 (56.60%)]\t\tLoss: 0.49794\n",
            "Training Progress: \tEpoch 92 [1280/1692 (75.47%)]\t\tLoss: 0.34200\n",
            "Training Progress: \tEpoch 92 [1600/1692 (94.34%)]\t\tLoss: 0.24361\n",
            "\tTrain loss: 0.00675, Accuracy: 1570/1692 (92.79%)\n",
            "\tValidation loss: 0.00409, Accuracy: 194/423 (45.86%)\n",
            "\tTest loss: 0.00381, Accuracy: 218/443 (49.21%)\n",
            "\n",
            "Training Progress: \tEpoch 93 [0/1692 (0.00%)]\t\tLoss: 0.49012\n",
            "Training Progress: \tEpoch 93 [320/1692 (18.87%)]\t\tLoss: 0.47948\n",
            "Training Progress: \tEpoch 93 [640/1692 (37.74%)]\t\tLoss: 0.34588\n",
            "Training Progress: \tEpoch 93 [960/1692 (56.60%)]\t\tLoss: 0.34711\n",
            "Training Progress: \tEpoch 93 [1280/1692 (75.47%)]\t\tLoss: 0.39000\n",
            "Training Progress: \tEpoch 93 [1600/1692 (94.34%)]\t\tLoss: 0.35928\n",
            "\tTrain loss: 0.00724, Accuracy: 1550/1692 (91.61%)\n",
            "\tValidation loss: 0.00365, Accuracy: 200/423 (47.28%)\n",
            "\tTest loss: 0.00350, Accuracy: 224/443 (50.56%)\n",
            "\n",
            "Training Progress: \tEpoch 94 [0/1692 (0.00%)]\t\tLoss: 0.53231\n",
            "Training Progress: \tEpoch 94 [320/1692 (18.87%)]\t\tLoss: 0.46291\n",
            "Training Progress: \tEpoch 94 [640/1692 (37.74%)]\t\tLoss: 0.27505\n",
            "Training Progress: \tEpoch 94 [960/1692 (56.60%)]\t\tLoss: 0.56800\n",
            "Training Progress: \tEpoch 94 [1280/1692 (75.47%)]\t\tLoss: 0.52747\n",
            "Training Progress: \tEpoch 94 [1600/1692 (94.34%)]\t\tLoss: 0.40337\n",
            "\tTrain loss: 0.00484, Accuracy: 1604/1692 (94.80%)\n",
            "\tValidation loss: 0.00375, Accuracy: 209/423 (49.41%)\n",
            "\tTest loss: 0.00356, Accuracy: 229/443 (51.69%)\n",
            "\n",
            "Training Progress: \tEpoch 95 [0/1692 (0.00%)]\t\tLoss: 0.34798\n",
            "Training Progress: \tEpoch 95 [320/1692 (18.87%)]\t\tLoss: 0.23851\n",
            "Training Progress: \tEpoch 95 [640/1692 (37.74%)]\t\tLoss: 0.16141\n",
            "Training Progress: \tEpoch 95 [960/1692 (56.60%)]\t\tLoss: 0.62656\n",
            "Training Progress: \tEpoch 95 [1280/1692 (75.47%)]\t\tLoss: 0.50814\n",
            "Training Progress: \tEpoch 95 [1600/1692 (94.34%)]\t\tLoss: 0.34762\n",
            "\tTrain loss: 0.00426, Accuracy: 1623/1692 (95.92%)\n",
            "\tValidation loss: 0.00352, Accuracy: 216/423 (51.06%)\n",
            "\tTest loss: 0.00344, Accuracy: 227/443 (51.24%)\n",
            "\n",
            "Training Progress: \tEpoch 96 [0/1692 (0.00%)]\t\tLoss: 0.68635\n",
            "Training Progress: \tEpoch 96 [320/1692 (18.87%)]\t\tLoss: 0.51116\n",
            "Training Progress: \tEpoch 96 [640/1692 (37.74%)]\t\tLoss: 0.24950\n",
            "Training Progress: \tEpoch 96 [960/1692 (56.60%)]\t\tLoss: 0.37395\n",
            "Training Progress: \tEpoch 96 [1280/1692 (75.47%)]\t\tLoss: 0.37251\n",
            "Training Progress: \tEpoch 96 [1600/1692 (94.34%)]\t\tLoss: 0.25627\n",
            "\tTrain loss: 0.00571, Accuracy: 1576/1692 (93.14%)\n",
            "\tValidation loss: 0.00356, Accuracy: 212/423 (50.12%)\n",
            "\tTest loss: 0.00352, Accuracy: 238/443 (53.72%)\n",
            "\n",
            "Training Progress: \tEpoch 97 [0/1692 (0.00%)]\t\tLoss: 0.37666\n",
            "Training Progress: \tEpoch 97 [320/1692 (18.87%)]\t\tLoss: 0.13767\n",
            "Training Progress: \tEpoch 97 [640/1692 (37.74%)]\t\tLoss: 0.35861\n",
            "Training Progress: \tEpoch 97 [960/1692 (56.60%)]\t\tLoss: 0.43385\n",
            "Training Progress: \tEpoch 97 [1280/1692 (75.47%)]\t\tLoss: 0.43557\n",
            "Training Progress: \tEpoch 97 [1600/1692 (94.34%)]\t\tLoss: 0.16979\n",
            "\tTrain loss: 0.00575, Accuracy: 1593/1692 (94.15%)\n",
            "\tValidation loss: 0.00357, Accuracy: 209/423 (49.41%)\n",
            "\tTest loss: 0.00339, Accuracy: 218/443 (49.21%)\n",
            "\n",
            "Training Progress: \tEpoch 98 [0/1692 (0.00%)]\t\tLoss: 0.65757\n",
            "Training Progress: \tEpoch 98 [320/1692 (18.87%)]\t\tLoss: 0.33147\n",
            "Training Progress: \tEpoch 98 [640/1692 (37.74%)]\t\tLoss: 0.44805\n",
            "Training Progress: \tEpoch 98 [960/1692 (56.60%)]\t\tLoss: 0.37421\n",
            "Training Progress: \tEpoch 98 [1280/1692 (75.47%)]\t\tLoss: 0.39445\n",
            "Training Progress: \tEpoch 98 [1600/1692 (94.34%)]\t\tLoss: 0.18841\n",
            "\tTrain loss: 0.00481, Accuracy: 1611/1692 (95.21%)\n",
            "\tValidation loss: 0.00377, Accuracy: 202/423 (47.75%)\n",
            "\tTest loss: 0.00353, Accuracy: 229/443 (51.69%)\n",
            "\n",
            "Training Progress: \tEpoch 99 [0/1692 (0.00%)]\t\tLoss: 0.27134\n",
            "Training Progress: \tEpoch 99 [320/1692 (18.87%)]\t\tLoss: 0.21539\n",
            "Training Progress: \tEpoch 99 [640/1692 (37.74%)]\t\tLoss: 0.22685\n",
            "Training Progress: \tEpoch 99 [960/1692 (56.60%)]\t\tLoss: 0.38589\n",
            "Training Progress: \tEpoch 99 [1280/1692 (75.47%)]\t\tLoss: 0.23197\n",
            "Training Progress: \tEpoch 99 [1600/1692 (94.34%)]\t\tLoss: 0.27900\n",
            "\tTrain loss: 0.00526, Accuracy: 1595/1692 (94.27%)\n",
            "\tValidation loss: 0.00396, Accuracy: 198/423 (46.81%)\n",
            "\tTest loss: 0.00367, Accuracy: 228/443 (51.47%)\n",
            "\n",
            "Training Progress: \tEpoch 100 [0/1692 (0.00%)]\t\tLoss: 0.66850\n",
            "Training Progress: \tEpoch 100 [320/1692 (18.87%)]\t\tLoss: 0.30702\n",
            "Training Progress: \tEpoch 100 [640/1692 (37.74%)]\t\tLoss: 0.23210\n",
            "Training Progress: \tEpoch 100 [960/1692 (56.60%)]\t\tLoss: 0.20703\n",
            "Training Progress: \tEpoch 100 [1280/1692 (75.47%)]\t\tLoss: 0.83702\n",
            "Training Progress: \tEpoch 100 [1600/1692 (94.34%)]\t\tLoss: 0.11905\n",
            "\tTrain loss: 0.00537, Accuracy: 1595/1692 (94.27%)\n",
            "\tValidation loss: 0.00374, Accuracy: 199/423 (47.04%)\n",
            "\tTest loss: 0.00376, Accuracy: 224/443 (50.56%)\n",
            "\n",
            "[[tensor(0.0434), 0.24881796690307328], [tensor(0.0429), 0.33156028368794327], [tensor(0.0416), 0.3486997635933806], [tensor(0.0413), 0.3516548463356974], [tensor(0.0405), 0.3764775413711584], [tensor(0.0403), 0.36524822695035464], [tensor(0.0395), 0.41548463356973997], [tensor(0.0385), 0.4308510638297872], [tensor(0.0378), 0.4379432624113475], [tensor(0.0375), 0.43380614657210403], [tensor(0.0377), 0.42789598108747046], [tensor(0.0364), 0.4627659574468085], [tensor(0.0357), 0.4958628841607565], [tensor(0.0363), 0.4710401891252955], [tensor(0.0346), 0.5118203309692672], [tensor(0.0336), 0.5419621749408984], [tensor(0.0331), 0.5313238770685579], [tensor(0.0318), 0.5620567375886525], [tensor(0.0323), 0.5460992907801419], [tensor(0.0321), 0.5425531914893617], [tensor(0.0297), 0.5963356973995272], [tensor(0.0301), 0.5839243498817966], [tensor(0.0303), 0.5803782505910166], [tensor(0.0335), 0.5366430260047281], [tensor(0.0288), 0.5981087470449172], [tensor(0.0291), 0.601063829787234], [tensor(0.0291), 0.6087470449172577], [tensor(0.0254), 0.6578014184397163], [tensor(0.0269), 0.642434988179669], [tensor(0.0279), 0.625886524822695], [tensor(0.0252), 0.6578014184397163], [tensor(0.0289), 0.6087470449172577], [tensor(0.0262), 0.6477541371158393], [tensor(0.0257), 0.6536643026004728], [tensor(0.0231), 0.6950354609929078], [tensor(0.0237), 0.6879432624113475], [tensor(0.0250), 0.6855791962174941], [tensor(0.0246), 0.6548463356973995], [tensor(0.0219), 0.7204491725768322], [tensor(0.0240), 0.6808510638297872], [tensor(0.0228), 0.7056737588652482], [tensor(0.0228), 0.7062647754137116], [tensor(0.0200), 0.7470449172576832], [tensor(0.0209), 0.7464539007092199], [tensor(0.0195), 0.7547281323877069], [tensor(0.0201), 0.735224586288416], [tensor(0.0176), 0.8014184397163121], [tensor(0.0209), 0.7547281323877069], [tensor(0.0175), 0.7907801418439716], [tensor(0.0187), 0.7777777777777778], [tensor(0.0163), 0.8031914893617021], [tensor(0.0176), 0.7801418439716312], [tensor(0.0158), 0.8191489361702128], [tensor(0.0152), 0.8280141843971631], [tensor(0.0130), 0.8658392434988179], [tensor(0.0173), 0.7683215130023641], [tensor(0.0145), 0.8380614657210402], [tensor(0.0136), 0.8528368794326241], [tensor(0.0157), 0.807919621749409], [tensor(0.0120), 0.8723404255319149], [tensor(0.0148), 0.8286052009456265], [tensor(0.0131), 0.8469267139479906], [tensor(0.0135), 0.8333333333333334], [tensor(0.0112), 0.8717494089834515], [tensor(0.0115), 0.8747044917257684], [tensor(0.0095), 0.9013002364066194], [tensor(0.0124), 0.8552009456264775], [tensor(0.0117), 0.8693853427895981], [tensor(0.0097), 0.8936170212765957], [tensor(0.0108), 0.8664302600472813], [tensor(0.0100), 0.9048463356973995], [tensor(0.0097), 0.8959810874704491], [tensor(0.0098), 0.8936170212765957], [tensor(0.0082), 0.9190307328605201], [tensor(0.0086), 0.9137115839243499], [tensor(0.0100), 0.8971631205673759], [tensor(0.0082), 0.9131205673758865], [tensor(0.0077), 0.9225768321513003], [tensor(0.0069), 0.9385342789598109], [tensor(0.0069), 0.942080378250591], [tensor(0.0062), 0.9367612293144209], [tensor(0.0077), 0.9196217494089834], [tensor(0.0060), 0.9444444444444444], [tensor(0.0067), 0.9426713947990544], [tensor(0.0076), 0.9261229314420804], [tensor(0.0063), 0.9426713947990544], [tensor(0.0065), 0.9332151300236406], [tensor(0.0068), 0.9267139479905437], [tensor(0.0078), 0.9202127659574468], [tensor(0.0051), 0.9621749408983451], [tensor(0.0055), 0.9444444444444444], [tensor(0.0067), 0.9278959810874704], [tensor(0.0072), 0.9160756501182034], [tensor(0.0048), 0.9479905437352246], [tensor(0.0043), 0.9592198581560284], [tensor(0.0057), 0.9314420803782506], [tensor(0.0057), 0.9414893617021277], [tensor(0.0048), 0.9521276595744681], [tensor(0.0053), 0.9426713947990544], [tensor(0.0054), 0.9426713947990544]]\n",
            "<class 'list'>\n",
            "[[tensor(0.0033), 0.2576832151300236], [tensor(0.0032), 0.3380614657210402], [tensor(0.0031), 0.36879432624113473], [tensor(0.0031), 0.36879432624113473], [tensor(0.0031), 0.375886524822695], [tensor(0.0031), 0.3640661938534279], [tensor(0.0030), 0.3735224586288416], [tensor(0.0030), 0.3877068557919622], [tensor(0.0029), 0.4066193853427896], [tensor(0.0029), 0.42080378250591016], [tensor(0.0030), 0.4160756501182033], [tensor(0.0029), 0.42789598108747046], [tensor(0.0028), 0.44208037825059104], [tensor(0.0029), 0.4491725768321513], [tensor(0.0029), 0.43498817966903075], [tensor(0.0028), 0.4728132387706856], [tensor(0.0028), 0.46335697399527187], [tensor(0.0027), 0.4799054373522459], [tensor(0.0028), 0.47044917257683216], [tensor(0.0027), 0.4728132387706856], [tensor(0.0026), 0.5224586288416075], [tensor(0.0028), 0.48226950354609927], [tensor(0.0027), 0.47754137115839246], [tensor(0.0030), 0.41134751773049644], [tensor(0.0027), 0.4799054373522459], [tensor(0.0028), 0.47754137115839246], [tensor(0.0028), 0.47044917257683216], [tensor(0.0026), 0.5059101654846335], [tensor(0.0027), 0.4562647754137116], [tensor(0.0029), 0.4799054373522459], [tensor(0.0026), 0.524822695035461], [tensor(0.0030), 0.44208037825059104], [tensor(0.0030), 0.4444444444444444], [tensor(0.0029), 0.4988179669030733], [tensor(0.0027), 0.48936170212765956], [tensor(0.0028), 0.5059101654846335], [tensor(0.0029), 0.5011820330969267], [tensor(0.0029), 0.48699763593380613], [tensor(0.0028), 0.5106382978723404], [tensor(0.0031), 0.458628841607565], [tensor(0.0030), 0.46099290780141844], [tensor(0.0031), 0.4728132387706856], [tensor(0.0029), 0.46808510638297873], [tensor(0.0029), 0.49645390070921985], [tensor(0.0030), 0.491725768321513], [tensor(0.0031), 0.491725768321513], [tensor(0.0028), 0.5342789598108747], [tensor(0.0030), 0.4846335697399527], [tensor(0.0030), 0.5011820330969267], [tensor(0.0030), 0.491725768321513], [tensor(0.0029), 0.5011820330969267], [tensor(0.0030), 0.5011820330969267], [tensor(0.0030), 0.508274231678487], [tensor(0.0030), 0.5224586288416075], [tensor(0.0029), 0.4988179669030733], [tensor(0.0034), 0.45390070921985815], [tensor(0.0031), 0.4940898345153664], [tensor(0.0031), 0.49645390070921985], [tensor(0.0033), 0.4846335697399527], [tensor(0.0030), 0.48226950354609927], [tensor(0.0031), 0.4728132387706856], [tensor(0.0035), 0.458628841607565], [tensor(0.0032), 0.4728132387706856], [tensor(0.0031), 0.5224586288416075], [tensor(0.0033), 0.47044917257683216], [tensor(0.0032), 0.5011820330969267], [tensor(0.0033), 0.48226950354609927], [tensor(0.0034), 0.458628841607565], [tensor(0.0033), 0.491725768321513], [tensor(0.0036), 0.4562647754137116], [tensor(0.0033), 0.4728132387706856], [tensor(0.0034), 0.47044917257683216], [tensor(0.0035), 0.46808510638297873], [tensor(0.0032), 0.5059101654846335], [tensor(0.0035), 0.4846335697399527], [tensor(0.0035), 0.48226950354609927], [tensor(0.0036), 0.4728132387706856], [tensor(0.0035), 0.4799054373522459], [tensor(0.0032), 0.5059101654846335], [tensor(0.0032), 0.5059101654846335], [tensor(0.0035), 0.524822695035461], [tensor(0.0036), 0.4846335697399527], [tensor(0.0035), 0.5153664302600472], [tensor(0.0035), 0.5059101654846335], [tensor(0.0038), 0.48936170212765956], [tensor(0.0036), 0.4799054373522459], [tensor(0.0038), 0.4799054373522459], [tensor(0.0040), 0.46099290780141844], [tensor(0.0039), 0.4562647754137116], [tensor(0.0034), 0.5035460992907801], [tensor(0.0037), 0.4799054373522459], [tensor(0.0041), 0.458628841607565], [tensor(0.0036), 0.4728132387706856], [tensor(0.0037), 0.4940898345153664], [tensor(0.0035), 0.5106382978723404], [tensor(0.0036), 0.5011820330969267], [tensor(0.0036), 0.4940898345153664], [tensor(0.0038), 0.47754137115839246], [tensor(0.0040), 0.46808510638297873], [tensor(0.0037), 0.47044917257683216]]\n",
            "<class 'list'>\n",
            "[[tensor(0.0031), 0.2505643340857788], [tensor(0.0031), 0.31376975169300225], [tensor(0.0030), 0.35214446952595935], [tensor(0.0030), 0.35665914221218964], [tensor(0.0029), 0.3476297968397291], [tensor(0.0029), 0.34988713318284426], [tensor(0.0029), 0.3476297968397291], [tensor(0.0029), 0.3837471783295711], [tensor(0.0028), 0.3927765237020316], [tensor(0.0028), 0.40857787810383744], [tensor(0.0029), 0.39503386004514673], [tensor(0.0028), 0.40632054176072235], [tensor(0.0028), 0.4108352144469526], [tensor(0.0028), 0.4153498871331828], [tensor(0.0027), 0.4582392776523702], [tensor(0.0026), 0.4717832957110609], [tensor(0.0026), 0.46952595936794583], [tensor(0.0026), 0.4672686230248307], [tensor(0.0026), 0.4604966139954853], [tensor(0.0026), 0.4898419864559819], [tensor(0.0024), 0.5304740406320542], [tensor(0.0026), 0.45598194130925507], [tensor(0.0026), 0.4762979683972912], [tensor(0.0029), 0.44243792325056436], [tensor(0.0026), 0.5011286681715575], [tensor(0.0026), 0.4853273137697517], [tensor(0.0027), 0.4762979683972912], [tensor(0.0025), 0.5146726862302483], [tensor(0.0026), 0.49209932279909707], [tensor(0.0027), 0.4785553047404063], [tensor(0.0025), 0.510158013544018], [tensor(0.0028), 0.49209932279909707], [tensor(0.0028), 0.4853273137697517], [tensor(0.0027), 0.5011286681715575], [tensor(0.0027), 0.5079006772009029], [tensor(0.0026), 0.5033860045146726], [tensor(0.0026), 0.5237020316027088], [tensor(0.0027), 0.5056433408577878], [tensor(0.0028), 0.49209932279909707], [tensor(0.0028), 0.510158013544018], [tensor(0.0029), 0.4853273137697517], [tensor(0.0029), 0.5033860045146726], [tensor(0.0028), 0.49435665914221216], [tensor(0.0028), 0.4966139954853273], [tensor(0.0028), 0.5056433408577878], [tensor(0.0030), 0.49435665914221216], [tensor(0.0028), 0.5033860045146726], [tensor(0.0029), 0.48758465011286684], [tensor(0.0029), 0.49435665914221216], [tensor(0.0029), 0.4898419864559819], [tensor(0.0028), 0.510158013544018], [tensor(0.0029), 0.49887133182844245], [tensor(0.0029), 0.510158013544018], [tensor(0.0030), 0.49887133182844245], [tensor(0.0029), 0.510158013544018], [tensor(0.0034), 0.4582392776523702], [tensor(0.0031), 0.4853273137697517], [tensor(0.0030), 0.4898419864559819], [tensor(0.0032), 0.49209932279909707], [tensor(0.0030), 0.5011286681715575], [tensor(0.0030), 0.510158013544018], [tensor(0.0033), 0.4785553047404063], [tensor(0.0031), 0.5056433408577878], [tensor(0.0031), 0.510158013544018], [tensor(0.0031), 0.49209932279909707], [tensor(0.0033), 0.5169300225733634], [tensor(0.0033), 0.4785553047404063], [tensor(0.0033), 0.49435665914221216], [tensor(0.0032), 0.5237020316027088], [tensor(0.0036), 0.4898419864559819], [tensor(0.0031), 0.5282167042889391], [tensor(0.0034), 0.49209932279909707], [tensor(0.0034), 0.5124153498871332], [tensor(0.0032), 0.5079006772009029], [tensor(0.0034), 0.5146726862302483], [tensor(0.0034), 0.5124153498871332], [tensor(0.0034), 0.49435665914221216], [tensor(0.0035), 0.48758465011286684], [tensor(0.0034), 0.5033860045146726], [tensor(0.0033), 0.49209932279909707], [tensor(0.0035), 0.5079006772009029], [tensor(0.0036), 0.48306997742663654], [tensor(0.0035), 0.5079006772009029], [tensor(0.0034), 0.4898419864559819], [tensor(0.0036), 0.49435665914221216], [tensor(0.0035), 0.48306997742663654], [tensor(0.0037), 0.4898419864559819], [tensor(0.0038), 0.5214446952595937], [tensor(0.0039), 0.46275395033860045], [tensor(0.0033), 0.5011286681715575], [tensor(0.0036), 0.510158013544018], [tensor(0.0038), 0.49209932279909707], [tensor(0.0035), 0.5056433408577878], [tensor(0.0036), 0.5169300225733634], [tensor(0.0034), 0.5124153498871332], [tensor(0.0035), 0.5372460496613995], [tensor(0.0034), 0.49209932279909707], [tensor(0.0035), 0.5169300225733634], [tensor(0.0037), 0.5146726862302483], [tensor(0.0038), 0.5056433408577878]]\n",
            "<class 'list'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Best validation accuracy:\n",
        "0.4822\n",
        "\n",
        "Best test accuracy:\n",
        "0.5350\n",
        "\n",
        "## Plotting Metrics v/s Number of Epochs:\n",
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "TpbeW8W5lr4k"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyP29OkKe_t0"
      },
      "source": [
        "## GRU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCBfGtuyckWL",
        "outputId": "edbcd673-7a55-4ea0-c3e0-17cd3fe57313",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = GRU().cuda()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR, betas=BETAS, eps=EPS, weight_decay=DECAY)\n",
        "train_and_evaluate(model, optimizer, data_loaders, num_epochs=EPOCHS)\n",
        "del model"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Progress: \tEpoch 1 [0/1692 (0.00%)]\t\tLoss: 1.54475\n",
            "Training Progress: \tEpoch 1 [320/1692 (18.87%)]\t\tLoss: 1.47711\n",
            "Training Progress: \tEpoch 1 [640/1692 (37.74%)]\t\tLoss: 1.48251\n",
            "Training Progress: \tEpoch 1 [960/1692 (56.60%)]\t\tLoss: 1.41522\n",
            "Training Progress: \tEpoch 1 [1280/1692 (75.47%)]\t\tLoss: 1.43397\n",
            "Training Progress: \tEpoch 1 [1600/1692 (94.34%)]\t\tLoss: 1.39656\n",
            "\tTrain loss: 0.04328, Accuracy: 474/1692 (28.01%)\n",
            "\tValidation loss: 0.00331, Accuracy: 110/423 (26.00%)\n",
            "\tTest loss: 0.00319, Accuracy: 98/443 (22.12%)\n",
            "\n",
            "Training Progress: \tEpoch 2 [0/1692 (0.00%)]\t\tLoss: 1.40604\n",
            "Training Progress: \tEpoch 2 [320/1692 (18.87%)]\t\tLoss: 1.49009\n",
            "Training Progress: \tEpoch 2 [640/1692 (37.74%)]\t\tLoss: 1.41010\n",
            "Training Progress: \tEpoch 2 [960/1692 (56.60%)]\t\tLoss: 1.41484\n",
            "Training Progress: \tEpoch 2 [1280/1692 (75.47%)]\t\tLoss: 1.38825\n",
            "Training Progress: \tEpoch 2 [1600/1692 (94.34%)]\t\tLoss: 1.36391\n",
            "\tTrain loss: 0.04295, Accuracy: 532/1692 (31.44%)\n",
            "\tValidation loss: 0.00332, Accuracy: 98/423 (23.17%)\n",
            "\tTest loss: 0.00317, Accuracy: 93/443 (20.99%)\n",
            "\n",
            "Training Progress: \tEpoch 3 [0/1692 (0.00%)]\t\tLoss: 1.29256\n",
            "Training Progress: \tEpoch 3 [320/1692 (18.87%)]\t\tLoss: 1.47465\n",
            "Training Progress: \tEpoch 3 [640/1692 (37.74%)]\t\tLoss: 1.40381\n",
            "Training Progress: \tEpoch 3 [960/1692 (56.60%)]\t\tLoss: 1.43423\n",
            "Training Progress: \tEpoch 3 [1280/1692 (75.47%)]\t\tLoss: 1.47294\n",
            "Training Progress: \tEpoch 3 [1600/1692 (94.34%)]\t\tLoss: 1.34727\n",
            "\tTrain loss: 0.04279, Accuracy: 569/1692 (33.63%)\n",
            "\tValidation loss: 0.00332, Accuracy: 104/423 (24.59%)\n",
            "\tTest loss: 0.00317, Accuracy: 102/443 (23.02%)\n",
            "\n",
            "Training Progress: \tEpoch 4 [0/1692 (0.00%)]\t\tLoss: 1.35091\n",
            "Training Progress: \tEpoch 4 [320/1692 (18.87%)]\t\tLoss: 1.41429\n",
            "Training Progress: \tEpoch 4 [640/1692 (37.74%)]\t\tLoss: 1.37318\n",
            "Training Progress: \tEpoch 4 [960/1692 (56.60%)]\t\tLoss: 1.39082\n",
            "Training Progress: \tEpoch 4 [1280/1692 (75.47%)]\t\tLoss: 1.36443\n",
            "Training Progress: \tEpoch 4 [1600/1692 (94.34%)]\t\tLoss: 1.34295\n",
            "\tTrain loss: 0.04263, Accuracy: 555/1692 (32.80%)\n",
            "\tValidation loss: 0.00332, Accuracy: 105/423 (24.82%)\n",
            "\tTest loss: 0.00318, Accuracy: 90/443 (20.32%)\n",
            "\n",
            "Training Progress: \tEpoch 5 [0/1692 (0.00%)]\t\tLoss: 1.38706\n",
            "Training Progress: \tEpoch 5 [320/1692 (18.87%)]\t\tLoss: 1.34568\n",
            "Training Progress: \tEpoch 5 [640/1692 (37.74%)]\t\tLoss: 1.37901\n",
            "Training Progress: \tEpoch 5 [960/1692 (56.60%)]\t\tLoss: 1.37709\n",
            "Training Progress: \tEpoch 5 [1280/1692 (75.47%)]\t\tLoss: 1.39509\n",
            "Training Progress: \tEpoch 5 [1600/1692 (94.34%)]\t\tLoss: 1.36525\n",
            "\tTrain loss: 0.04256, Accuracy: 573/1692 (33.87%)\n",
            "\tValidation loss: 0.00332, Accuracy: 108/423 (25.53%)\n",
            "\tTest loss: 0.00317, Accuracy: 102/443 (23.02%)\n",
            "\n",
            "Training Progress: \tEpoch 6 [0/1692 (0.00%)]\t\tLoss: 1.41987\n",
            "Training Progress: \tEpoch 6 [320/1692 (18.87%)]\t\tLoss: 1.37824\n",
            "Training Progress: \tEpoch 6 [640/1692 (37.74%)]\t\tLoss: 1.39952\n",
            "Training Progress: \tEpoch 6 [960/1692 (56.60%)]\t\tLoss: 1.36430\n",
            "Training Progress: \tEpoch 6 [1280/1692 (75.47%)]\t\tLoss: 1.31384\n",
            "Training Progress: \tEpoch 6 [1600/1692 (94.34%)]\t\tLoss: 1.39491\n",
            "\tTrain loss: 0.04248, Accuracy: 581/1692 (34.34%)\n",
            "\tValidation loss: 0.00332, Accuracy: 104/423 (24.59%)\n",
            "\tTest loss: 0.00317, Accuracy: 103/443 (23.25%)\n",
            "\n",
            "Training Progress: \tEpoch 7 [0/1692 (0.00%)]\t\tLoss: 1.37308\n",
            "Training Progress: \tEpoch 7 [320/1692 (18.87%)]\t\tLoss: 1.38795\n",
            "Training Progress: \tEpoch 7 [640/1692 (37.74%)]\t\tLoss: 1.36835\n",
            "Training Progress: \tEpoch 7 [960/1692 (56.60%)]\t\tLoss: 1.44334\n",
            "Training Progress: \tEpoch 7 [1280/1692 (75.47%)]\t\tLoss: 1.35161\n",
            "Training Progress: \tEpoch 7 [1600/1692 (94.34%)]\t\tLoss: 1.32255\n",
            "\tTrain loss: 0.04235, Accuracy: 584/1692 (34.52%)\n",
            "\tValidation loss: 0.00333, Accuracy: 109/423 (25.77%)\n",
            "\tTest loss: 0.00315, Accuracy: 103/443 (23.25%)\n",
            "\n",
            "Training Progress: \tEpoch 8 [0/1692 (0.00%)]\t\tLoss: 1.38406\n",
            "Training Progress: \tEpoch 8 [320/1692 (18.87%)]\t\tLoss: 1.42543\n",
            "Training Progress: \tEpoch 8 [640/1692 (37.74%)]\t\tLoss: 1.31744\n",
            "Training Progress: \tEpoch 8 [960/1692 (56.60%)]\t\tLoss: 1.37286\n",
            "Training Progress: \tEpoch 8 [1280/1692 (75.47%)]\t\tLoss: 1.34619\n",
            "Training Progress: \tEpoch 8 [1600/1692 (94.34%)]\t\tLoss: 1.36624\n",
            "\tTrain loss: 0.04216, Accuracy: 604/1692 (35.70%)\n",
            "\tValidation loss: 0.00333, Accuracy: 98/423 (23.17%)\n",
            "\tTest loss: 0.00316, Accuracy: 116/443 (26.19%)\n",
            "\n",
            "Training Progress: \tEpoch 9 [0/1692 (0.00%)]\t\tLoss: 1.37667\n",
            "Training Progress: \tEpoch 9 [320/1692 (18.87%)]\t\tLoss: 1.36057\n",
            "Training Progress: \tEpoch 9 [640/1692 (37.74%)]\t\tLoss: 1.37990\n",
            "Training Progress: \tEpoch 9 [960/1692 (56.60%)]\t\tLoss: 1.34649\n",
            "Training Progress: \tEpoch 9 [1280/1692 (75.47%)]\t\tLoss: 1.37062\n",
            "Training Progress: \tEpoch 9 [1600/1692 (94.34%)]\t\tLoss: 1.31763\n",
            "\tTrain loss: 0.04199, Accuracy: 623/1692 (36.82%)\n",
            "\tValidation loss: 0.00334, Accuracy: 107/423 (25.30%)\n",
            "\tTest loss: 0.00317, Accuracy: 109/443 (24.60%)\n",
            "\n",
            "Training Progress: \tEpoch 10 [0/1692 (0.00%)]\t\tLoss: 1.39466\n",
            "Training Progress: \tEpoch 10 [320/1692 (18.87%)]\t\tLoss: 1.39184\n",
            "Training Progress: \tEpoch 10 [640/1692 (37.74%)]\t\tLoss: 1.37064\n",
            "Training Progress: \tEpoch 10 [960/1692 (56.60%)]\t\tLoss: 1.34923\n",
            "Training Progress: \tEpoch 10 [1280/1692 (75.47%)]\t\tLoss: 1.38880\n",
            "Training Progress: \tEpoch 10 [1600/1692 (94.34%)]\t\tLoss: 1.34277\n",
            "\tTrain loss: 0.04188, Accuracy: 621/1692 (36.70%)\n",
            "\tValidation loss: 0.00334, Accuracy: 102/423 (24.11%)\n",
            "\tTest loss: 0.00316, Accuracy: 113/443 (25.51%)\n",
            "\n",
            "Training Progress: \tEpoch 11 [0/1692 (0.00%)]\t\tLoss: 1.37031\n",
            "Training Progress: \tEpoch 11 [320/1692 (18.87%)]\t\tLoss: 1.44917\n",
            "Training Progress: \tEpoch 11 [640/1692 (37.74%)]\t\tLoss: 1.28853\n",
            "Training Progress: \tEpoch 11 [960/1692 (56.60%)]\t\tLoss: 1.29576\n",
            "Training Progress: \tEpoch 11 [1280/1692 (75.47%)]\t\tLoss: 1.34686\n",
            "Training Progress: \tEpoch 11 [1600/1692 (94.34%)]\t\tLoss: 1.30951\n",
            "\tTrain loss: 0.04172, Accuracy: 624/1692 (36.88%)\n",
            "\tValidation loss: 0.00336, Accuracy: 102/423 (24.11%)\n",
            "\tTest loss: 0.00316, Accuracy: 109/443 (24.60%)\n",
            "\n",
            "Training Progress: \tEpoch 12 [0/1692 (0.00%)]\t\tLoss: 1.32774\n",
            "Training Progress: \tEpoch 12 [320/1692 (18.87%)]\t\tLoss: 1.39313\n",
            "Training Progress: \tEpoch 12 [640/1692 (37.74%)]\t\tLoss: 1.23850\n",
            "Training Progress: \tEpoch 12 [960/1692 (56.60%)]\t\tLoss: 1.40531\n",
            "Training Progress: \tEpoch 12 [1280/1692 (75.47%)]\t\tLoss: 1.38425\n",
            "Training Progress: \tEpoch 12 [1600/1692 (94.34%)]\t\tLoss: 1.37896\n",
            "\tTrain loss: 0.04159, Accuracy: 653/1692 (38.59%)\n",
            "\tValidation loss: 0.00334, Accuracy: 109/423 (25.77%)\n",
            "\tTest loss: 0.00316, Accuracy: 109/443 (24.60%)\n",
            "\n",
            "Training Progress: \tEpoch 13 [0/1692 (0.00%)]\t\tLoss: 1.34070\n",
            "Training Progress: \tEpoch 13 [320/1692 (18.87%)]\t\tLoss: 1.44607\n",
            "Training Progress: \tEpoch 13 [640/1692 (37.74%)]\t\tLoss: 1.30549\n",
            "Training Progress: \tEpoch 13 [960/1692 (56.60%)]\t\tLoss: 1.34745\n",
            "Training Progress: \tEpoch 13 [1280/1692 (75.47%)]\t\tLoss: 1.33785\n",
            "Training Progress: \tEpoch 13 [1600/1692 (94.34%)]\t\tLoss: 1.33642\n",
            "\tTrain loss: 0.04142, Accuracy: 640/1692 (37.83%)\n",
            "\tValidation loss: 0.00337, Accuracy: 104/423 (24.59%)\n",
            "\tTest loss: 0.00319, Accuracy: 110/443 (24.83%)\n",
            "\n",
            "Training Progress: \tEpoch 14 [0/1692 (0.00%)]\t\tLoss: 1.38746\n",
            "Training Progress: \tEpoch 14 [320/1692 (18.87%)]\t\tLoss: 1.43471\n",
            "Training Progress: \tEpoch 14 [640/1692 (37.74%)]\t\tLoss: 1.28771\n",
            "Training Progress: \tEpoch 14 [960/1692 (56.60%)]\t\tLoss: 1.36882\n",
            "Training Progress: \tEpoch 14 [1280/1692 (75.47%)]\t\tLoss: 1.30579\n",
            "Training Progress: \tEpoch 14 [1600/1692 (94.34%)]\t\tLoss: 1.29366\n",
            "\tTrain loss: 0.04127, Accuracy: 640/1692 (37.83%)\n",
            "\tValidation loss: 0.00336, Accuracy: 99/423 (23.40%)\n",
            "\tTest loss: 0.00318, Accuracy: 110/443 (24.83%)\n",
            "\n",
            "Training Progress: \tEpoch 15 [0/1692 (0.00%)]\t\tLoss: 1.31699\n",
            "Training Progress: \tEpoch 15 [320/1692 (18.87%)]\t\tLoss: 1.35572\n",
            "Training Progress: \tEpoch 15 [640/1692 (37.74%)]\t\tLoss: 1.26350\n",
            "Training Progress: \tEpoch 15 [960/1692 (56.60%)]\t\tLoss: 1.31923\n",
            "Training Progress: \tEpoch 15 [1280/1692 (75.47%)]\t\tLoss: 1.34171\n",
            "Training Progress: \tEpoch 15 [1600/1692 (94.34%)]\t\tLoss: 1.35284\n",
            "\tTrain loss: 0.04114, Accuracy: 638/1692 (37.71%)\n",
            "\tValidation loss: 0.00337, Accuracy: 101/423 (23.88%)\n",
            "\tTest loss: 0.00318, Accuracy: 120/443 (27.09%)\n",
            "\n",
            "Training Progress: \tEpoch 16 [0/1692 (0.00%)]\t\tLoss: 1.41644\n",
            "Training Progress: \tEpoch 16 [320/1692 (18.87%)]\t\tLoss: 1.40269\n",
            "Training Progress: \tEpoch 16 [640/1692 (37.74%)]\t\tLoss: 1.32907\n",
            "Training Progress: \tEpoch 16 [960/1692 (56.60%)]\t\tLoss: 1.35096\n",
            "Training Progress: \tEpoch 16 [1280/1692 (75.47%)]\t\tLoss: 1.30546\n",
            "Training Progress: \tEpoch 16 [1600/1692 (94.34%)]\t\tLoss: 1.32960\n",
            "\tTrain loss: 0.04099, Accuracy: 649/1692 (38.36%)\n",
            "\tValidation loss: 0.00336, Accuracy: 108/423 (25.53%)\n",
            "\tTest loss: 0.00320, Accuracy: 108/443 (24.38%)\n",
            "\n",
            "Training Progress: \tEpoch 17 [0/1692 (0.00%)]\t\tLoss: 1.38242\n",
            "Training Progress: \tEpoch 17 [320/1692 (18.87%)]\t\tLoss: 1.43457\n",
            "Training Progress: \tEpoch 17 [640/1692 (37.74%)]\t\tLoss: 1.29143\n",
            "Training Progress: \tEpoch 17 [960/1692 (56.60%)]\t\tLoss: 1.29274\n",
            "Training Progress: \tEpoch 17 [1280/1692 (75.47%)]\t\tLoss: 1.33614\n",
            "Training Progress: \tEpoch 17 [1600/1692 (94.34%)]\t\tLoss: 1.33091\n",
            "\tTrain loss: 0.04065, Accuracy: 663/1692 (39.18%)\n",
            "\tValidation loss: 0.00340, Accuracy: 107/423 (25.30%)\n",
            "\tTest loss: 0.00321, Accuracy: 105/443 (23.70%)\n",
            "\n",
            "Training Progress: \tEpoch 18 [0/1692 (0.00%)]\t\tLoss: 1.31045\n",
            "Training Progress: \tEpoch 18 [320/1692 (18.87%)]\t\tLoss: 1.40165\n",
            "Training Progress: \tEpoch 18 [640/1692 (37.74%)]\t\tLoss: 1.30869\n",
            "Training Progress: \tEpoch 18 [960/1692 (56.60%)]\t\tLoss: 1.31748\n",
            "Training Progress: \tEpoch 18 [1280/1692 (75.47%)]\t\tLoss: 1.33426\n",
            "Training Progress: \tEpoch 18 [1600/1692 (94.34%)]\t\tLoss: 1.38740\n",
            "\tTrain loss: 0.04061, Accuracy: 674/1692 (39.83%)\n",
            "\tValidation loss: 0.00341, Accuracy: 101/423 (23.88%)\n",
            "\tTest loss: 0.00322, Accuracy: 115/443 (25.96%)\n",
            "\n",
            "Training Progress: \tEpoch 19 [0/1692 (0.00%)]\t\tLoss: 1.34496\n",
            "Training Progress: \tEpoch 19 [320/1692 (18.87%)]\t\tLoss: 1.42488\n",
            "Training Progress: \tEpoch 19 [640/1692 (37.74%)]\t\tLoss: 1.30076\n",
            "Training Progress: \tEpoch 19 [960/1692 (56.60%)]\t\tLoss: 1.29728\n",
            "Training Progress: \tEpoch 19 [1280/1692 (75.47%)]\t\tLoss: 1.34227\n",
            "Training Progress: \tEpoch 19 [1600/1692 (94.34%)]\t\tLoss: 1.33797\n",
            "\tTrain loss: 0.04051, Accuracy: 667/1692 (39.42%)\n",
            "\tValidation loss: 0.00339, Accuracy: 104/423 (24.59%)\n",
            "\tTest loss: 0.00322, Accuracy: 103/443 (23.25%)\n",
            "\n",
            "Training Progress: \tEpoch 20 [0/1692 (0.00%)]\t\tLoss: 1.31138\n",
            "Training Progress: \tEpoch 20 [320/1692 (18.87%)]\t\tLoss: 1.43463\n",
            "Training Progress: \tEpoch 20 [640/1692 (37.74%)]\t\tLoss: 1.28207\n",
            "Training Progress: \tEpoch 20 [960/1692 (56.60%)]\t\tLoss: 1.30441\n",
            "Training Progress: \tEpoch 20 [1280/1692 (75.47%)]\t\tLoss: 1.29689\n",
            "Training Progress: \tEpoch 20 [1600/1692 (94.34%)]\t\tLoss: 1.26051\n",
            "\tTrain loss: 0.04023, Accuracy: 695/1692 (41.08%)\n",
            "\tValidation loss: 0.00339, Accuracy: 98/423 (23.17%)\n",
            "\tTest loss: 0.00321, Accuracy: 111/443 (25.06%)\n",
            "\n",
            "Training Progress: \tEpoch 21 [0/1692 (0.00%)]\t\tLoss: 1.26614\n",
            "Training Progress: \tEpoch 21 [320/1692 (18.87%)]\t\tLoss: 1.41420\n",
            "Training Progress: \tEpoch 21 [640/1692 (37.74%)]\t\tLoss: 1.32781\n",
            "Training Progress: \tEpoch 21 [960/1692 (56.60%)]\t\tLoss: 1.27098\n",
            "Training Progress: \tEpoch 21 [1280/1692 (75.47%)]\t\tLoss: 1.36813\n",
            "Training Progress: \tEpoch 21 [1600/1692 (94.34%)]\t\tLoss: 1.34634\n",
            "\tTrain loss: 0.04018, Accuracy: 710/1692 (41.96%)\n",
            "\tValidation loss: 0.00341, Accuracy: 95/423 (22.46%)\n",
            "\tTest loss: 0.00322, Accuracy: 119/443 (26.86%)\n",
            "\n",
            "Training Progress: \tEpoch 22 [0/1692 (0.00%)]\t\tLoss: 1.28847\n",
            "Training Progress: \tEpoch 22 [320/1692 (18.87%)]\t\tLoss: 1.46138\n",
            "Training Progress: \tEpoch 22 [640/1692 (37.74%)]\t\tLoss: 1.33304\n",
            "Training Progress: \tEpoch 22 [960/1692 (56.60%)]\t\tLoss: 1.24761\n",
            "Training Progress: \tEpoch 22 [1280/1692 (75.47%)]\t\tLoss: 1.29517\n",
            "Training Progress: \tEpoch 22 [1600/1692 (94.34%)]\t\tLoss: 1.33741\n",
            "\tTrain loss: 0.03980, Accuracy: 715/1692 (42.26%)\n",
            "\tValidation loss: 0.00343, Accuracy: 97/423 (22.93%)\n",
            "\tTest loss: 0.00323, Accuracy: 121/443 (27.31%)\n",
            "\n",
            "Training Progress: \tEpoch 23 [0/1692 (0.00%)]\t\tLoss: 1.22386\n",
            "Training Progress: \tEpoch 23 [320/1692 (18.87%)]\t\tLoss: 1.43063\n",
            "Training Progress: \tEpoch 23 [640/1692 (37.74%)]\t\tLoss: 1.23064\n",
            "Training Progress: \tEpoch 23 [960/1692 (56.60%)]\t\tLoss: 1.20910\n",
            "Training Progress: \tEpoch 23 [1280/1692 (75.47%)]\t\tLoss: 1.31885\n",
            "Training Progress: \tEpoch 23 [1600/1692 (94.34%)]\t\tLoss: 1.25925\n",
            "\tTrain loss: 0.03957, Accuracy: 736/1692 (43.50%)\n",
            "\tValidation loss: 0.00346, Accuracy: 93/423 (21.99%)\n",
            "\tTest loss: 0.00325, Accuracy: 123/443 (27.77%)\n",
            "\n",
            "Training Progress: \tEpoch 24 [0/1692 (0.00%)]\t\tLoss: 1.28835\n",
            "Training Progress: \tEpoch 24 [320/1692 (18.87%)]\t\tLoss: 1.36608\n",
            "Training Progress: \tEpoch 24 [640/1692 (37.74%)]\t\tLoss: 1.24056\n",
            "Training Progress: \tEpoch 24 [960/1692 (56.60%)]\t\tLoss: 1.24945\n",
            "Training Progress: \tEpoch 24 [1280/1692 (75.47%)]\t\tLoss: 1.23878\n",
            "Training Progress: \tEpoch 24 [1600/1692 (94.34%)]\t\tLoss: 1.35950\n",
            "\tTrain loss: 0.03934, Accuracy: 720/1692 (42.55%)\n",
            "\tValidation loss: 0.00348, Accuracy: 95/423 (22.46%)\n",
            "\tTest loss: 0.00325, Accuracy: 121/443 (27.31%)\n",
            "\n",
            "Training Progress: \tEpoch 25 [0/1692 (0.00%)]\t\tLoss: 1.26092\n",
            "Training Progress: \tEpoch 25 [320/1692 (18.87%)]\t\tLoss: 1.40943\n",
            "Training Progress: \tEpoch 25 [640/1692 (37.74%)]\t\tLoss: 1.32915\n",
            "Training Progress: \tEpoch 25 [960/1692 (56.60%)]\t\tLoss: 1.34749\n",
            "Training Progress: \tEpoch 25 [1280/1692 (75.47%)]\t\tLoss: 1.32806\n",
            "Training Progress: \tEpoch 25 [1600/1692 (94.34%)]\t\tLoss: 1.31674\n",
            "\tTrain loss: 0.03924, Accuracy: 730/1692 (43.14%)\n",
            "\tValidation loss: 0.00347, Accuracy: 92/423 (21.75%)\n",
            "\tTest loss: 0.00328, Accuracy: 124/443 (27.99%)\n",
            "\n",
            "Training Progress: \tEpoch 26 [0/1692 (0.00%)]\t\tLoss: 1.23324\n",
            "Training Progress: \tEpoch 26 [320/1692 (18.87%)]\t\tLoss: 1.37693\n",
            "Training Progress: \tEpoch 26 [640/1692 (37.74%)]\t\tLoss: 1.23386\n",
            "Training Progress: \tEpoch 26 [960/1692 (56.60%)]\t\tLoss: 1.31721\n",
            "Training Progress: \tEpoch 26 [1280/1692 (75.47%)]\t\tLoss: 1.30699\n",
            "Training Progress: \tEpoch 26 [1600/1692 (94.34%)]\t\tLoss: 1.23674\n",
            "\tTrain loss: 0.03883, Accuracy: 757/1692 (44.74%)\n",
            "\tValidation loss: 0.00347, Accuracy: 105/423 (24.82%)\n",
            "\tTest loss: 0.00328, Accuracy: 109/443 (24.60%)\n",
            "\n",
            "Training Progress: \tEpoch 27 [0/1692 (0.00%)]\t\tLoss: 1.27680\n",
            "Training Progress: \tEpoch 27 [320/1692 (18.87%)]\t\tLoss: 1.32357\n",
            "Training Progress: \tEpoch 27 [640/1692 (37.74%)]\t\tLoss: 1.18811\n",
            "Training Progress: \tEpoch 27 [960/1692 (56.60%)]\t\tLoss: 1.24582\n",
            "Training Progress: \tEpoch 27 [1280/1692 (75.47%)]\t\tLoss: 1.22578\n",
            "Training Progress: \tEpoch 27 [1600/1692 (94.34%)]\t\tLoss: 1.26617\n",
            "\tTrain loss: 0.03844, Accuracy: 774/1692 (45.74%)\n",
            "\tValidation loss: 0.00352, Accuracy: 99/423 (23.40%)\n",
            "\tTest loss: 0.00330, Accuracy: 114/443 (25.73%)\n",
            "\n",
            "Training Progress: \tEpoch 28 [0/1692 (0.00%)]\t\tLoss: 1.38576\n",
            "Training Progress: \tEpoch 28 [320/1692 (18.87%)]\t\tLoss: 1.43256\n",
            "Training Progress: \tEpoch 28 [640/1692 (37.74%)]\t\tLoss: 1.20415\n",
            "Training Progress: \tEpoch 28 [960/1692 (56.60%)]\t\tLoss: 1.22819\n",
            "Training Progress: \tEpoch 28 [1280/1692 (75.47%)]\t\tLoss: 1.24371\n",
            "Training Progress: \tEpoch 28 [1600/1692 (94.34%)]\t\tLoss: 1.34367\n",
            "\tTrain loss: 0.03841, Accuracy: 785/1692 (46.39%)\n",
            "\tValidation loss: 0.00349, Accuracy: 106/423 (25.06%)\n",
            "\tTest loss: 0.00332, Accuracy: 111/443 (25.06%)\n",
            "\n",
            "Training Progress: \tEpoch 29 [0/1692 (0.00%)]\t\tLoss: 1.21339\n",
            "Training Progress: \tEpoch 29 [320/1692 (18.87%)]\t\tLoss: 1.35837\n",
            "Training Progress: \tEpoch 29 [640/1692 (37.74%)]\t\tLoss: 1.19538\n",
            "Training Progress: \tEpoch 29 [960/1692 (56.60%)]\t\tLoss: 1.31340\n",
            "Training Progress: \tEpoch 29 [1280/1692 (75.47%)]\t\tLoss: 1.17960\n",
            "Training Progress: \tEpoch 29 [1600/1692 (94.34%)]\t\tLoss: 1.25496\n",
            "\tTrain loss: 0.03803, Accuracy: 792/1692 (46.81%)\n",
            "\tValidation loss: 0.00353, Accuracy: 100/423 (23.64%)\n",
            "\tTest loss: 0.00331, Accuracy: 117/443 (26.41%)\n",
            "\n",
            "Training Progress: \tEpoch 30 [0/1692 (0.00%)]\t\tLoss: 1.26294\n",
            "Training Progress: \tEpoch 30 [320/1692 (18.87%)]\t\tLoss: 1.31418\n",
            "Training Progress: \tEpoch 30 [640/1692 (37.74%)]\t\tLoss: 1.15331\n",
            "Training Progress: \tEpoch 30 [960/1692 (56.60%)]\t\tLoss: 1.27606\n",
            "Training Progress: \tEpoch 30 [1280/1692 (75.47%)]\t\tLoss: 1.22476\n",
            "Training Progress: \tEpoch 30 [1600/1692 (94.34%)]\t\tLoss: 1.24743\n",
            "\tTrain loss: 0.03789, Accuracy: 791/1692 (46.75%)\n",
            "\tValidation loss: 0.00352, Accuracy: 117/423 (27.66%)\n",
            "\tTest loss: 0.00335, Accuracy: 113/443 (25.51%)\n",
            "\n",
            "Training Progress: \tEpoch 31 [0/1692 (0.00%)]\t\tLoss: 1.27311\n",
            "Training Progress: \tEpoch 31 [320/1692 (18.87%)]\t\tLoss: 1.41221\n",
            "Training Progress: \tEpoch 31 [640/1692 (37.74%)]\t\tLoss: 1.15280\n",
            "Training Progress: \tEpoch 31 [960/1692 (56.60%)]\t\tLoss: 1.12739\n",
            "Training Progress: \tEpoch 31 [1280/1692 (75.47%)]\t\tLoss: 1.14981\n",
            "Training Progress: \tEpoch 31 [1600/1692 (94.34%)]\t\tLoss: 1.23111\n",
            "\tTrain loss: 0.03772, Accuracy: 793/1692 (46.87%)\n",
            "\tValidation loss: 0.00357, Accuracy: 102/423 (24.11%)\n",
            "\tTest loss: 0.00334, Accuracy: 118/443 (26.64%)\n",
            "\n",
            "Training Progress: \tEpoch 32 [0/1692 (0.00%)]\t\tLoss: 1.33790\n",
            "Training Progress: \tEpoch 32 [320/1692 (18.87%)]\t\tLoss: 1.35654\n",
            "Training Progress: \tEpoch 32 [640/1692 (37.74%)]\t\tLoss: 1.12959\n",
            "Training Progress: \tEpoch 32 [960/1692 (56.60%)]\t\tLoss: 1.31223\n",
            "Training Progress: \tEpoch 32 [1280/1692 (75.47%)]\t\tLoss: 1.19186\n",
            "Training Progress: \tEpoch 32 [1600/1692 (94.34%)]\t\tLoss: 1.26009\n",
            "\tTrain loss: 0.03743, Accuracy: 793/1692 (46.87%)\n",
            "\tValidation loss: 0.00358, Accuracy: 104/423 (24.59%)\n",
            "\tTest loss: 0.00337, Accuracy: 119/443 (26.86%)\n",
            "\n",
            "Training Progress: \tEpoch 33 [0/1692 (0.00%)]\t\tLoss: 1.16671\n",
            "Training Progress: \tEpoch 33 [320/1692 (18.87%)]\t\tLoss: 1.39832\n",
            "Training Progress: \tEpoch 33 [640/1692 (37.74%)]\t\tLoss: 1.21081\n",
            "Training Progress: \tEpoch 33 [960/1692 (56.60%)]\t\tLoss: 1.18121\n",
            "Training Progress: \tEpoch 33 [1280/1692 (75.47%)]\t\tLoss: 1.17084\n",
            "Training Progress: \tEpoch 33 [1600/1692 (94.34%)]\t\tLoss: 1.34031\n",
            "\tTrain loss: 0.03687, Accuracy: 837/1692 (49.47%)\n",
            "\tValidation loss: 0.00358, Accuracy: 104/423 (24.59%)\n",
            "\tTest loss: 0.00340, Accuracy: 116/443 (26.19%)\n",
            "\n",
            "Training Progress: \tEpoch 34 [0/1692 (0.00%)]\t\tLoss: 1.22499\n",
            "Training Progress: \tEpoch 34 [320/1692 (18.87%)]\t\tLoss: 1.38282\n",
            "Training Progress: \tEpoch 34 [640/1692 (37.74%)]\t\tLoss: 1.17597\n",
            "Training Progress: \tEpoch 34 [960/1692 (56.60%)]\t\tLoss: 1.26152\n",
            "Training Progress: \tEpoch 34 [1280/1692 (75.47%)]\t\tLoss: 1.24359\n",
            "Training Progress: \tEpoch 34 [1600/1692 (94.34%)]\t\tLoss: 1.35307\n",
            "\tTrain loss: 0.03640, Accuracy: 876/1692 (51.77%)\n",
            "\tValidation loss: 0.00356, Accuracy: 109/423 (25.77%)\n",
            "\tTest loss: 0.00339, Accuracy: 109/443 (24.60%)\n",
            "\n",
            "Training Progress: \tEpoch 35 [0/1692 (0.00%)]\t\tLoss: 1.19288\n",
            "Training Progress: \tEpoch 35 [320/1692 (18.87%)]\t\tLoss: 1.34394\n",
            "Training Progress: \tEpoch 35 [640/1692 (37.74%)]\t\tLoss: 1.23070\n",
            "Training Progress: \tEpoch 35 [960/1692 (56.60%)]\t\tLoss: 1.19039\n",
            "Training Progress: \tEpoch 35 [1280/1692 (75.47%)]\t\tLoss: 1.17044\n",
            "Training Progress: \tEpoch 35 [1600/1692 (94.34%)]\t\tLoss: 1.27166\n",
            "\tTrain loss: 0.03647, Accuracy: 866/1692 (51.18%)\n",
            "\tValidation loss: 0.00359, Accuracy: 107/423 (25.30%)\n",
            "\tTest loss: 0.00341, Accuracy: 107/443 (24.15%)\n",
            "\n",
            "Training Progress: \tEpoch 36 [0/1692 (0.00%)]\t\tLoss: 1.22068\n",
            "Training Progress: \tEpoch 36 [320/1692 (18.87%)]\t\tLoss: 1.42655\n",
            "Training Progress: \tEpoch 36 [640/1692 (37.74%)]\t\tLoss: 1.31377\n",
            "Training Progress: \tEpoch 36 [960/1692 (56.60%)]\t\tLoss: 1.08919\n",
            "Training Progress: \tEpoch 36 [1280/1692 (75.47%)]\t\tLoss: 1.10628\n",
            "Training Progress: \tEpoch 36 [1600/1692 (94.34%)]\t\tLoss: 1.27323\n",
            "\tTrain loss: 0.03602, Accuracy: 853/1692 (50.41%)\n",
            "\tValidation loss: 0.00361, Accuracy: 102/423 (24.11%)\n",
            "\tTest loss: 0.00342, Accuracy: 113/443 (25.51%)\n",
            "\n",
            "Training Progress: \tEpoch 37 [0/1692 (0.00%)]\t\tLoss: 1.25777\n",
            "Training Progress: \tEpoch 37 [320/1692 (18.87%)]\t\tLoss: 1.31208\n",
            "Training Progress: \tEpoch 37 [640/1692 (37.74%)]\t\tLoss: 1.12943\n",
            "Training Progress: \tEpoch 37 [960/1692 (56.60%)]\t\tLoss: 1.18733\n",
            "Training Progress: \tEpoch 37 [1280/1692 (75.47%)]\t\tLoss: 1.15409\n",
            "Training Progress: \tEpoch 37 [1600/1692 (94.34%)]\t\tLoss: 1.35473\n",
            "\tTrain loss: 0.03583, Accuracy: 873/1692 (51.60%)\n",
            "\tValidation loss: 0.00364, Accuracy: 101/423 (23.88%)\n",
            "\tTest loss: 0.00344, Accuracy: 112/443 (25.28%)\n",
            "\n",
            "Training Progress: \tEpoch 38 [0/1692 (0.00%)]\t\tLoss: 1.23906\n",
            "Training Progress: \tEpoch 38 [320/1692 (18.87%)]\t\tLoss: 1.30234\n",
            "Training Progress: \tEpoch 38 [640/1692 (37.74%)]\t\tLoss: 1.09710\n",
            "Training Progress: \tEpoch 38 [960/1692 (56.60%)]\t\tLoss: 1.19618\n",
            "Training Progress: \tEpoch 38 [1280/1692 (75.47%)]\t\tLoss: 1.08630\n",
            "Training Progress: \tEpoch 38 [1600/1692 (94.34%)]\t\tLoss: 1.23851\n",
            "\tTrain loss: 0.03527, Accuracy: 874/1692 (51.65%)\n",
            "\tValidation loss: 0.00368, Accuracy: 113/423 (26.71%)\n",
            "\tTest loss: 0.00344, Accuracy: 116/443 (26.19%)\n",
            "\n",
            "Training Progress: \tEpoch 39 [0/1692 (0.00%)]\t\tLoss: 1.15434\n",
            "Training Progress: \tEpoch 39 [320/1692 (18.87%)]\t\tLoss: 1.27299\n",
            "Training Progress: \tEpoch 39 [640/1692 (37.74%)]\t\tLoss: 1.11882\n",
            "Training Progress: \tEpoch 39 [960/1692 (56.60%)]\t\tLoss: 1.19054\n",
            "Training Progress: \tEpoch 39 [1280/1692 (75.47%)]\t\tLoss: 1.02405\n",
            "Training Progress: \tEpoch 39 [1600/1692 (94.34%)]\t\tLoss: 1.17583\n",
            "\tTrain loss: 0.03497, Accuracy: 920/1692 (54.37%)\n",
            "\tValidation loss: 0.00370, Accuracy: 103/423 (24.35%)\n",
            "\tTest loss: 0.00342, Accuracy: 121/443 (27.31%)\n",
            "\n",
            "Training Progress: \tEpoch 40 [0/1692 (0.00%)]\t\tLoss: 1.16977\n",
            "Training Progress: \tEpoch 40 [320/1692 (18.87%)]\t\tLoss: 1.18561\n",
            "Training Progress: \tEpoch 40 [640/1692 (37.74%)]\t\tLoss: 1.28425\n",
            "Training Progress: \tEpoch 40 [960/1692 (56.60%)]\t\tLoss: 1.18934\n",
            "Training Progress: \tEpoch 40 [1280/1692 (75.47%)]\t\tLoss: 1.07948\n",
            "Training Progress: \tEpoch 40 [1600/1692 (94.34%)]\t\tLoss: 1.19268\n",
            "\tTrain loss: 0.03450, Accuracy: 912/1692 (53.90%)\n",
            "\tValidation loss: 0.00372, Accuracy: 98/423 (23.17%)\n",
            "\tTest loss: 0.00347, Accuracy: 121/443 (27.31%)\n",
            "\n",
            "Training Progress: \tEpoch 41 [0/1692 (0.00%)]\t\tLoss: 1.10896\n",
            "Training Progress: \tEpoch 41 [320/1692 (18.87%)]\t\tLoss: 1.22554\n",
            "Training Progress: \tEpoch 41 [640/1692 (37.74%)]\t\tLoss: 1.13321\n",
            "Training Progress: \tEpoch 41 [960/1692 (56.60%)]\t\tLoss: 1.19949\n",
            "Training Progress: \tEpoch 41 [1280/1692 (75.47%)]\t\tLoss: 1.22480\n",
            "Training Progress: \tEpoch 41 [1600/1692 (94.34%)]\t\tLoss: 1.16396\n",
            "\tTrain loss: 0.03463, Accuracy: 903/1692 (53.37%)\n",
            "\tValidation loss: 0.00371, Accuracy: 105/423 (24.82%)\n",
            "\tTest loss: 0.00348, Accuracy: 120/443 (27.09%)\n",
            "\n",
            "Training Progress: \tEpoch 42 [0/1692 (0.00%)]\t\tLoss: 1.02554\n",
            "Training Progress: \tEpoch 42 [320/1692 (18.87%)]\t\tLoss: 1.33346\n",
            "Training Progress: \tEpoch 42 [640/1692 (37.74%)]\t\tLoss: 1.13411\n",
            "Training Progress: \tEpoch 42 [960/1692 (56.60%)]\t\tLoss: 1.16525\n",
            "Training Progress: \tEpoch 42 [1280/1692 (75.47%)]\t\tLoss: 1.02460\n",
            "Training Progress: \tEpoch 42 [1600/1692 (94.34%)]\t\tLoss: 1.22554\n",
            "\tTrain loss: 0.03349, Accuracy: 983/1692 (58.10%)\n",
            "\tValidation loss: 0.00374, Accuracy: 103/423 (24.35%)\n",
            "\tTest loss: 0.00355, Accuracy: 113/443 (25.51%)\n",
            "\n",
            "Training Progress: \tEpoch 43 [0/1692 (0.00%)]\t\tLoss: 1.02229\n",
            "Training Progress: \tEpoch 43 [320/1692 (18.87%)]\t\tLoss: 1.27586\n",
            "Training Progress: \tEpoch 43 [640/1692 (37.74%)]\t\tLoss: 1.07581\n",
            "Training Progress: \tEpoch 43 [960/1692 (56.60%)]\t\tLoss: 1.11813\n",
            "Training Progress: \tEpoch 43 [1280/1692 (75.47%)]\t\tLoss: 1.10756\n",
            "Training Progress: \tEpoch 43 [1600/1692 (94.34%)]\t\tLoss: 1.12619\n",
            "\tTrain loss: 0.03297, Accuracy: 964/1692 (56.97%)\n",
            "\tValidation loss: 0.00378, Accuracy: 110/423 (26.00%)\n",
            "\tTest loss: 0.00354, Accuracy: 121/443 (27.31%)\n",
            "\n",
            "Training Progress: \tEpoch 44 [0/1692 (0.00%)]\t\tLoss: 1.07940\n",
            "Training Progress: \tEpoch 44 [320/1692 (18.87%)]\t\tLoss: 1.28866\n",
            "Training Progress: \tEpoch 44 [640/1692 (37.74%)]\t\tLoss: 1.11036\n",
            "Training Progress: \tEpoch 44 [960/1692 (56.60%)]\t\tLoss: 1.14239\n",
            "Training Progress: \tEpoch 44 [1280/1692 (75.47%)]\t\tLoss: 1.07870\n",
            "Training Progress: \tEpoch 44 [1600/1692 (94.34%)]\t\tLoss: 1.05779\n",
            "\tTrain loss: 0.03259, Accuracy: 977/1692 (57.74%)\n",
            "\tValidation loss: 0.00381, Accuracy: 112/423 (26.48%)\n",
            "\tTest loss: 0.00361, Accuracy: 106/443 (23.93%)\n",
            "\n",
            "Training Progress: \tEpoch 45 [0/1692 (0.00%)]\t\tLoss: 1.21961\n",
            "Training Progress: \tEpoch 45 [320/1692 (18.87%)]\t\tLoss: 1.42011\n",
            "Training Progress: \tEpoch 45 [640/1692 (37.74%)]\t\tLoss: 1.09934\n",
            "Training Progress: \tEpoch 45 [960/1692 (56.60%)]\t\tLoss: 1.16125\n",
            "Training Progress: \tEpoch 45 [1280/1692 (75.47%)]\t\tLoss: 0.95731\n",
            "Training Progress: \tEpoch 45 [1600/1692 (94.34%)]\t\tLoss: 1.16749\n",
            "\tTrain loss: 0.03214, Accuracy: 994/1692 (58.75%)\n",
            "\tValidation loss: 0.00383, Accuracy: 102/423 (24.11%)\n",
            "\tTest loss: 0.00361, Accuracy: 117/443 (26.41%)\n",
            "\n",
            "Training Progress: \tEpoch 46 [0/1692 (0.00%)]\t\tLoss: 1.12203\n",
            "Training Progress: \tEpoch 46 [320/1692 (18.87%)]\t\tLoss: 1.13256\n",
            "Training Progress: \tEpoch 46 [640/1692 (37.74%)]\t\tLoss: 0.95947\n",
            "Training Progress: \tEpoch 46 [960/1692 (56.60%)]\t\tLoss: 1.09046\n",
            "Training Progress: \tEpoch 46 [1280/1692 (75.47%)]\t\tLoss: 1.03241\n",
            "Training Progress: \tEpoch 46 [1600/1692 (94.34%)]\t\tLoss: 1.13690\n",
            "\tTrain loss: 0.03223, Accuracy: 995/1692 (58.81%)\n",
            "\tValidation loss: 0.00390, Accuracy: 101/423 (23.88%)\n",
            "\tTest loss: 0.00365, Accuracy: 115/443 (25.96%)\n",
            "\n",
            "Training Progress: \tEpoch 47 [0/1692 (0.00%)]\t\tLoss: 1.14203\n",
            "Training Progress: \tEpoch 47 [320/1692 (18.87%)]\t\tLoss: 1.33970\n",
            "Training Progress: \tEpoch 47 [640/1692 (37.74%)]\t\tLoss: 1.11495\n",
            "Training Progress: \tEpoch 47 [960/1692 (56.60%)]\t\tLoss: 1.06747\n",
            "Training Progress: \tEpoch 47 [1280/1692 (75.47%)]\t\tLoss: 0.99941\n",
            "Training Progress: \tEpoch 47 [1600/1692 (94.34%)]\t\tLoss: 1.02786\n",
            "\tTrain loss: 0.03192, Accuracy: 991/1692 (58.57%)\n",
            "\tValidation loss: 0.00396, Accuracy: 96/423 (22.70%)\n",
            "\tTest loss: 0.00366, Accuracy: 118/443 (26.64%)\n",
            "\n",
            "Training Progress: \tEpoch 48 [0/1692 (0.00%)]\t\tLoss: 1.09608\n",
            "Training Progress: \tEpoch 48 [320/1692 (18.87%)]\t\tLoss: 1.20309\n",
            "Training Progress: \tEpoch 48 [640/1692 (37.74%)]\t\tLoss: 1.14004\n",
            "Training Progress: \tEpoch 48 [960/1692 (56.60%)]\t\tLoss: 1.04231\n",
            "Training Progress: \tEpoch 48 [1280/1692 (75.47%)]\t\tLoss: 1.12367\n",
            "Training Progress: \tEpoch 48 [1600/1692 (94.34%)]\t\tLoss: 1.30914\n",
            "\tTrain loss: 0.03162, Accuracy: 1022/1692 (60.40%)\n",
            "\tValidation loss: 0.00388, Accuracy: 96/423 (22.70%)\n",
            "\tTest loss: 0.00356, Accuracy: 117/443 (26.41%)\n",
            "\n",
            "Training Progress: \tEpoch 49 [0/1692 (0.00%)]\t\tLoss: 0.93475\n",
            "Training Progress: \tEpoch 49 [320/1692 (18.87%)]\t\tLoss: 1.17599\n",
            "Training Progress: \tEpoch 49 [640/1692 (37.74%)]\t\tLoss: 1.12376\n",
            "Training Progress: \tEpoch 49 [960/1692 (56.60%)]\t\tLoss: 1.13801\n",
            "Training Progress: \tEpoch 49 [1280/1692 (75.47%)]\t\tLoss: 1.08691\n",
            "Training Progress: \tEpoch 49 [1600/1692 (94.34%)]\t\tLoss: 1.10810\n",
            "\tTrain loss: 0.03064, Accuracy: 1044/1692 (61.70%)\n",
            "\tValidation loss: 0.00398, Accuracy: 104/423 (24.59%)\n",
            "\tTest loss: 0.00367, Accuracy: 116/443 (26.19%)\n",
            "\n",
            "Training Progress: \tEpoch 50 [0/1692 (0.00%)]\t\tLoss: 1.11834\n",
            "Training Progress: \tEpoch 50 [320/1692 (18.87%)]\t\tLoss: 1.23728\n",
            "Training Progress: \tEpoch 50 [640/1692 (37.74%)]\t\tLoss: 1.11278\n",
            "Training Progress: \tEpoch 50 [960/1692 (56.60%)]\t\tLoss: 0.99441\n",
            "Training Progress: \tEpoch 50 [1280/1692 (75.47%)]\t\tLoss: 0.86160\n",
            "Training Progress: \tEpoch 50 [1600/1692 (94.34%)]\t\tLoss: 1.13869\n",
            "\tTrain loss: 0.03041, Accuracy: 1035/1692 (61.17%)\n",
            "\tValidation loss: 0.00408, Accuracy: 99/423 (23.40%)\n",
            "\tTest loss: 0.00376, Accuracy: 119/443 (26.86%)\n",
            "\n",
            "Training Progress: \tEpoch 51 [0/1692 (0.00%)]\t\tLoss: 0.92627\n",
            "Training Progress: \tEpoch 51 [320/1692 (18.87%)]\t\tLoss: 1.15277\n",
            "Training Progress: \tEpoch 51 [640/1692 (37.74%)]\t\tLoss: 1.15327\n",
            "Training Progress: \tEpoch 51 [960/1692 (56.60%)]\t\tLoss: 1.17496\n",
            "Training Progress: \tEpoch 51 [1280/1692 (75.47%)]\t\tLoss: 0.96696\n",
            "Training Progress: \tEpoch 51 [1600/1692 (94.34%)]\t\tLoss: 1.12814\n",
            "\tTrain loss: 0.03012, Accuracy: 1033/1692 (61.05%)\n",
            "\tValidation loss: 0.00406, Accuracy: 101/423 (23.88%)\n",
            "\tTest loss: 0.00382, Accuracy: 117/443 (26.41%)\n",
            "\n",
            "Training Progress: \tEpoch 52 [0/1692 (0.00%)]\t\tLoss: 1.17222\n",
            "Training Progress: \tEpoch 52 [320/1692 (18.87%)]\t\tLoss: 1.19188\n",
            "Training Progress: \tEpoch 52 [640/1692 (37.74%)]\t\tLoss: 1.08540\n",
            "Training Progress: \tEpoch 52 [960/1692 (56.60%)]\t\tLoss: 1.06467\n",
            "Training Progress: \tEpoch 52 [1280/1692 (75.47%)]\t\tLoss: 0.91431\n",
            "Training Progress: \tEpoch 52 [1600/1692 (94.34%)]\t\tLoss: 1.08435\n",
            "\tTrain loss: 0.02985, Accuracy: 1044/1692 (61.70%)\n",
            "\tValidation loss: 0.00414, Accuracy: 90/423 (21.28%)\n",
            "\tTest loss: 0.00383, Accuracy: 116/443 (26.19%)\n",
            "\n",
            "Training Progress: \tEpoch 53 [0/1692 (0.00%)]\t\tLoss: 0.88950\n",
            "Training Progress: \tEpoch 53 [320/1692 (18.87%)]\t\tLoss: 1.42901\n",
            "Training Progress: \tEpoch 53 [640/1692 (37.74%)]\t\tLoss: 1.33845\n",
            "Training Progress: \tEpoch 53 [960/1692 (56.60%)]\t\tLoss: 0.90434\n",
            "Training Progress: \tEpoch 53 [1280/1692 (75.47%)]\t\tLoss: 0.93705\n",
            "Training Progress: \tEpoch 53 [1600/1692 (94.34%)]\t\tLoss: 0.90186\n",
            "\tTrain loss: 0.02960, Accuracy: 1063/1692 (62.83%)\n",
            "\tValidation loss: 0.00407, Accuracy: 90/423 (21.28%)\n",
            "\tTest loss: 0.00381, Accuracy: 120/443 (27.09%)\n",
            "\n",
            "Training Progress: \tEpoch 54 [0/1692 (0.00%)]\t\tLoss: 1.07772\n",
            "Training Progress: \tEpoch 54 [320/1692 (18.87%)]\t\tLoss: 1.25481\n",
            "Training Progress: \tEpoch 54 [640/1692 (37.74%)]\t\tLoss: 1.06082\n",
            "Training Progress: \tEpoch 54 [960/1692 (56.60%)]\t\tLoss: 1.03024\n",
            "Training Progress: \tEpoch 54 [1280/1692 (75.47%)]\t\tLoss: 0.93888\n",
            "Training Progress: \tEpoch 54 [1600/1692 (94.34%)]\t\tLoss: 1.15807\n",
            "\tTrain loss: 0.02906, Accuracy: 1093/1692 (64.60%)\n",
            "\tValidation loss: 0.00419, Accuracy: 101/423 (23.88%)\n",
            "\tTest loss: 0.00385, Accuracy: 118/443 (26.64%)\n",
            "\n",
            "Training Progress: \tEpoch 55 [0/1692 (0.00%)]\t\tLoss: 0.91915\n",
            "Training Progress: \tEpoch 55 [320/1692 (18.87%)]\t\tLoss: 1.26384\n",
            "Training Progress: \tEpoch 55 [640/1692 (37.74%)]\t\tLoss: 1.18620\n",
            "Training Progress: \tEpoch 55 [960/1692 (56.60%)]\t\tLoss: 1.18117\n",
            "Training Progress: \tEpoch 55 [1280/1692 (75.47%)]\t\tLoss: 0.96997\n",
            "Training Progress: \tEpoch 55 [1600/1692 (94.34%)]\t\tLoss: 1.25896\n",
            "\tTrain loss: 0.02915, Accuracy: 1064/1692 (62.88%)\n",
            "\tValidation loss: 0.00419, Accuracy: 96/423 (22.70%)\n",
            "\tTest loss: 0.00392, Accuracy: 120/443 (27.09%)\n",
            "\n",
            "Training Progress: \tEpoch 56 [0/1692 (0.00%)]\t\tLoss: 1.13985\n",
            "Training Progress: \tEpoch 56 [320/1692 (18.87%)]\t\tLoss: 1.18075\n",
            "Training Progress: \tEpoch 56 [640/1692 (37.74%)]\t\tLoss: 1.12523\n",
            "Training Progress: \tEpoch 56 [960/1692 (56.60%)]\t\tLoss: 1.07914\n",
            "Training Progress: \tEpoch 56 [1280/1692 (75.47%)]\t\tLoss: 0.88739\n",
            "Training Progress: \tEpoch 56 [1600/1692 (94.34%)]\t\tLoss: 1.13218\n",
            "\tTrain loss: 0.02845, Accuracy: 1090/1692 (64.42%)\n",
            "\tValidation loss: 0.00422, Accuracy: 99/423 (23.40%)\n",
            "\tTest loss: 0.00390, Accuracy: 113/443 (25.51%)\n",
            "\n",
            "Training Progress: \tEpoch 57 [0/1692 (0.00%)]\t\tLoss: 1.15370\n",
            "Training Progress: \tEpoch 57 [320/1692 (18.87%)]\t\tLoss: 1.14095\n",
            "Training Progress: \tEpoch 57 [640/1692 (37.74%)]\t\tLoss: 1.12679\n",
            "Training Progress: \tEpoch 57 [960/1692 (56.60%)]\t\tLoss: 1.18590\n",
            "Training Progress: \tEpoch 57 [1280/1692 (75.47%)]\t\tLoss: 0.87678\n",
            "Training Progress: \tEpoch 57 [1600/1692 (94.34%)]\t\tLoss: 1.08833\n",
            "\tTrain loss: 0.02806, Accuracy: 1107/1692 (65.43%)\n",
            "\tValidation loss: 0.00420, Accuracy: 98/423 (23.17%)\n",
            "\tTest loss: 0.00392, Accuracy: 119/443 (26.86%)\n",
            "\n",
            "Training Progress: \tEpoch 58 [0/1692 (0.00%)]\t\tLoss: 1.01025\n",
            "Training Progress: \tEpoch 58 [320/1692 (18.87%)]\t\tLoss: 1.06179\n",
            "Training Progress: \tEpoch 58 [640/1692 (37.74%)]\t\tLoss: 1.10736\n",
            "Training Progress: \tEpoch 58 [960/1692 (56.60%)]\t\tLoss: 1.03920\n",
            "Training Progress: \tEpoch 58 [1280/1692 (75.47%)]\t\tLoss: 0.83905\n",
            "Training Progress: \tEpoch 58 [1600/1692 (94.34%)]\t\tLoss: 0.93729\n",
            "\tTrain loss: 0.02790, Accuracy: 1106/1692 (65.37%)\n",
            "\tValidation loss: 0.00426, Accuracy: 104/423 (24.59%)\n",
            "\tTest loss: 0.00396, Accuracy: 119/443 (26.86%)\n",
            "\n",
            "Training Progress: \tEpoch 59 [0/1692 (0.00%)]\t\tLoss: 0.92789\n",
            "Training Progress: \tEpoch 59 [320/1692 (18.87%)]\t\tLoss: 1.08351\n",
            "Training Progress: \tEpoch 59 [640/1692 (37.74%)]\t\tLoss: 1.12485\n",
            "Training Progress: \tEpoch 59 [960/1692 (56.60%)]\t\tLoss: 1.11408\n",
            "Training Progress: \tEpoch 59 [1280/1692 (75.47%)]\t\tLoss: 0.89088\n",
            "Training Progress: \tEpoch 59 [1600/1692 (94.34%)]\t\tLoss: 0.91586\n",
            "\tTrain loss: 0.02706, Accuracy: 1114/1692 (65.84%)\n",
            "\tValidation loss: 0.00424, Accuracy: 107/423 (25.30%)\n",
            "\tTest loss: 0.00404, Accuracy: 114/443 (25.73%)\n",
            "\n",
            "Training Progress: \tEpoch 60 [0/1692 (0.00%)]\t\tLoss: 0.81810\n",
            "Training Progress: \tEpoch 60 [320/1692 (18.87%)]\t\tLoss: 1.26610\n",
            "Training Progress: \tEpoch 60 [640/1692 (37.74%)]\t\tLoss: 1.15816\n",
            "Training Progress: \tEpoch 60 [960/1692 (56.60%)]\t\tLoss: 0.99988\n",
            "Training Progress: \tEpoch 60 [1280/1692 (75.47%)]\t\tLoss: 0.90043\n",
            "Training Progress: \tEpoch 60 [1600/1692 (94.34%)]\t\tLoss: 1.22880\n",
            "\tTrain loss: 0.02722, Accuracy: 1107/1692 (65.43%)\n",
            "\tValidation loss: 0.00426, Accuracy: 104/423 (24.59%)\n",
            "\tTest loss: 0.00403, Accuracy: 120/443 (27.09%)\n",
            "\n",
            "Training Progress: \tEpoch 61 [0/1692 (0.00%)]\t\tLoss: 0.94116\n",
            "Training Progress: \tEpoch 61 [320/1692 (18.87%)]\t\tLoss: 1.16362\n",
            "Training Progress: \tEpoch 61 [640/1692 (37.74%)]\t\tLoss: 1.04102\n",
            "Training Progress: \tEpoch 61 [960/1692 (56.60%)]\t\tLoss: 0.96847\n",
            "Training Progress: \tEpoch 61 [1280/1692 (75.47%)]\t\tLoss: 0.86684\n",
            "Training Progress: \tEpoch 61 [1600/1692 (94.34%)]\t\tLoss: 0.93844\n",
            "\tTrain loss: 0.02641, Accuracy: 1145/1692 (67.67%)\n",
            "\tValidation loss: 0.00434, Accuracy: 103/423 (24.35%)\n",
            "\tTest loss: 0.00404, Accuracy: 118/443 (26.64%)\n",
            "\n",
            "Training Progress: \tEpoch 62 [0/1692 (0.00%)]\t\tLoss: 1.05816\n",
            "Training Progress: \tEpoch 62 [320/1692 (18.87%)]\t\tLoss: 1.16306\n",
            "Training Progress: \tEpoch 62 [640/1692 (37.74%)]\t\tLoss: 1.16840\n",
            "Training Progress: \tEpoch 62 [960/1692 (56.60%)]\t\tLoss: 1.00230\n",
            "Training Progress: \tEpoch 62 [1280/1692 (75.47%)]\t\tLoss: 0.84531\n",
            "Training Progress: \tEpoch 62 [1600/1692 (94.34%)]\t\tLoss: 0.96980\n",
            "\tTrain loss: 0.02624, Accuracy: 1142/1692 (67.49%)\n",
            "\tValidation loss: 0.00436, Accuracy: 101/423 (23.88%)\n",
            "\tTest loss: 0.00410, Accuracy: 129/443 (29.12%)\n",
            "\n",
            "Training Progress: \tEpoch 63 [0/1692 (0.00%)]\t\tLoss: 1.01281\n",
            "Training Progress: \tEpoch 63 [320/1692 (18.87%)]\t\tLoss: 1.13187\n",
            "Training Progress: \tEpoch 63 [640/1692 (37.74%)]\t\tLoss: 0.93444\n",
            "Training Progress: \tEpoch 63 [960/1692 (56.60%)]\t\tLoss: 1.11459\n",
            "Training Progress: \tEpoch 63 [1280/1692 (75.47%)]\t\tLoss: 0.94097\n",
            "Training Progress: \tEpoch 63 [1600/1692 (94.34%)]\t\tLoss: 0.91232\n",
            "\tTrain loss: 0.02628, Accuracy: 1137/1692 (67.20%)\n",
            "\tValidation loss: 0.00440, Accuracy: 102/423 (24.11%)\n",
            "\tTest loss: 0.00396, Accuracy: 121/443 (27.31%)\n",
            "\n",
            "Training Progress: \tEpoch 64 [0/1692 (0.00%)]\t\tLoss: 0.90544\n",
            "Training Progress: \tEpoch 64 [320/1692 (18.87%)]\t\tLoss: 1.00262\n",
            "Training Progress: \tEpoch 64 [640/1692 (37.74%)]\t\tLoss: 1.20678\n",
            "Training Progress: \tEpoch 64 [960/1692 (56.60%)]\t\tLoss: 0.85165\n",
            "Training Progress: \tEpoch 64 [1280/1692 (75.47%)]\t\tLoss: 0.80269\n",
            "Training Progress: \tEpoch 64 [1600/1692 (94.34%)]\t\tLoss: 0.86416\n",
            "\tTrain loss: 0.02515, Accuracy: 1165/1692 (68.85%)\n",
            "\tValidation loss: 0.00452, Accuracy: 98/423 (23.17%)\n",
            "\tTest loss: 0.00416, Accuracy: 121/443 (27.31%)\n",
            "\n",
            "Training Progress: \tEpoch 65 [0/1692 (0.00%)]\t\tLoss: 1.02765\n",
            "Training Progress: \tEpoch 65 [320/1692 (18.87%)]\t\tLoss: 1.00408\n",
            "Training Progress: \tEpoch 65 [640/1692 (37.74%)]\t\tLoss: 0.95448\n",
            "Training Progress: \tEpoch 65 [960/1692 (56.60%)]\t\tLoss: 0.92425\n",
            "Training Progress: \tEpoch 65 [1280/1692 (75.47%)]\t\tLoss: 0.86944\n",
            "Training Progress: \tEpoch 65 [1600/1692 (94.34%)]\t\tLoss: 0.71800\n",
            "\tTrain loss: 0.02439, Accuracy: 1180/1692 (69.74%)\n",
            "\tValidation loss: 0.00461, Accuracy: 104/423 (24.59%)\n",
            "\tTest loss: 0.00430, Accuracy: 109/443 (24.60%)\n",
            "\n",
            "Training Progress: \tEpoch 66 [0/1692 (0.00%)]\t\tLoss: 0.88682\n",
            "Training Progress: \tEpoch 66 [320/1692 (18.87%)]\t\tLoss: 1.05753\n",
            "Training Progress: \tEpoch 66 [640/1692 (37.74%)]\t\tLoss: 0.92741\n",
            "Training Progress: \tEpoch 66 [960/1692 (56.60%)]\t\tLoss: 1.05936\n",
            "Training Progress: \tEpoch 66 [1280/1692 (75.47%)]\t\tLoss: 0.74552\n",
            "Training Progress: \tEpoch 66 [1600/1692 (94.34%)]\t\tLoss: 1.07409\n",
            "\tTrain loss: 0.02439, Accuracy: 1182/1692 (69.86%)\n",
            "\tValidation loss: 0.00457, Accuracy: 101/423 (23.88%)\n",
            "\tTest loss: 0.00413, Accuracy: 118/443 (26.64%)\n",
            "\n",
            "Training Progress: \tEpoch 67 [0/1692 (0.00%)]\t\tLoss: 0.85440\n",
            "Training Progress: \tEpoch 67 [320/1692 (18.87%)]\t\tLoss: 1.06436\n",
            "Training Progress: \tEpoch 67 [640/1692 (37.74%)]\t\tLoss: 1.03559\n",
            "Training Progress: \tEpoch 67 [960/1692 (56.60%)]\t\tLoss: 0.97760\n",
            "Training Progress: \tEpoch 67 [1280/1692 (75.47%)]\t\tLoss: 0.91952\n",
            "Training Progress: \tEpoch 67 [1600/1692 (94.34%)]\t\tLoss: 0.98424\n",
            "\tTrain loss: 0.02454, Accuracy: 1163/1692 (68.74%)\n",
            "\tValidation loss: 0.00466, Accuracy: 100/423 (23.64%)\n",
            "\tTest loss: 0.00433, Accuracy: 108/443 (24.38%)\n",
            "\n",
            "Training Progress: \tEpoch 68 [0/1692 (0.00%)]\t\tLoss: 0.74745\n",
            "Training Progress: \tEpoch 68 [320/1692 (18.87%)]\t\tLoss: 1.05769\n",
            "Training Progress: \tEpoch 68 [640/1692 (37.74%)]\t\tLoss: 1.03335\n",
            "Training Progress: \tEpoch 68 [960/1692 (56.60%)]\t\tLoss: 0.78531\n",
            "Training Progress: \tEpoch 68 [1280/1692 (75.47%)]\t\tLoss: 1.10263\n",
            "Training Progress: \tEpoch 68 [1600/1692 (94.34%)]\t\tLoss: 0.92260\n",
            "\tTrain loss: 0.02372, Accuracy: 1211/1692 (71.57%)\n",
            "\tValidation loss: 0.00474, Accuracy: 101/423 (23.88%)\n",
            "\tTest loss: 0.00427, Accuracy: 115/443 (25.96%)\n",
            "\n",
            "Training Progress: \tEpoch 69 [0/1692 (0.00%)]\t\tLoss: 1.06139\n",
            "Training Progress: \tEpoch 69 [320/1692 (18.87%)]\t\tLoss: 1.07710\n",
            "Training Progress: \tEpoch 69 [640/1692 (37.74%)]\t\tLoss: 0.92588\n",
            "Training Progress: \tEpoch 69 [960/1692 (56.60%)]\t\tLoss: 0.92936\n",
            "Training Progress: \tEpoch 69 [1280/1692 (75.47%)]\t\tLoss: 1.01483\n",
            "Training Progress: \tEpoch 69 [1600/1692 (94.34%)]\t\tLoss: 0.90757\n",
            "\tTrain loss: 0.02333, Accuracy: 1215/1692 (71.81%)\n",
            "\tValidation loss: 0.00467, Accuracy: 95/423 (22.46%)\n",
            "\tTest loss: 0.00423, Accuracy: 125/443 (28.22%)\n",
            "\n",
            "Training Progress: \tEpoch 70 [0/1692 (0.00%)]\t\tLoss: 0.71624\n",
            "Training Progress: \tEpoch 70 [320/1692 (18.87%)]\t\tLoss: 1.03576\n",
            "Training Progress: \tEpoch 70 [640/1692 (37.74%)]\t\tLoss: 1.10903\n",
            "Training Progress: \tEpoch 70 [960/1692 (56.60%)]\t\tLoss: 0.90420\n",
            "Training Progress: \tEpoch 70 [1280/1692 (75.47%)]\t\tLoss: 0.83031\n",
            "Training Progress: \tEpoch 70 [1600/1692 (94.34%)]\t\tLoss: 0.87001\n",
            "\tTrain loss: 0.02335, Accuracy: 1190/1692 (70.33%)\n",
            "\tValidation loss: 0.00480, Accuracy: 97/423 (22.93%)\n",
            "\tTest loss: 0.00438, Accuracy: 115/443 (25.96%)\n",
            "\n",
            "Training Progress: \tEpoch 71 [0/1692 (0.00%)]\t\tLoss: 0.86137\n",
            "Training Progress: \tEpoch 71 [320/1692 (18.87%)]\t\tLoss: 1.03330\n",
            "Training Progress: \tEpoch 71 [640/1692 (37.74%)]\t\tLoss: 1.10813\n",
            "Training Progress: \tEpoch 71 [960/1692 (56.60%)]\t\tLoss: 0.88444\n",
            "Training Progress: \tEpoch 71 [1280/1692 (75.47%)]\t\tLoss: 0.67930\n",
            "Training Progress: \tEpoch 71 [1600/1692 (94.34%)]\t\tLoss: 0.82618\n",
            "\tTrain loss: 0.02317, Accuracy: 1217/1692 (71.93%)\n",
            "\tValidation loss: 0.00483, Accuracy: 105/423 (24.82%)\n",
            "\tTest loss: 0.00440, Accuracy: 109/443 (24.60%)\n",
            "\n",
            "Training Progress: \tEpoch 72 [0/1692 (0.00%)]\t\tLoss: 0.75435\n",
            "Training Progress: \tEpoch 72 [320/1692 (18.87%)]\t\tLoss: 1.21381\n",
            "Training Progress: \tEpoch 72 [640/1692 (37.74%)]\t\tLoss: 0.79583\n",
            "Training Progress: \tEpoch 72 [960/1692 (56.60%)]\t\tLoss: 0.91188\n",
            "Training Progress: \tEpoch 72 [1280/1692 (75.47%)]\t\tLoss: 0.84101\n",
            "Training Progress: \tEpoch 72 [1600/1692 (94.34%)]\t\tLoss: 0.80162\n",
            "\tTrain loss: 0.02268, Accuracy: 1227/1692 (72.52%)\n",
            "\tValidation loss: 0.00483, Accuracy: 103/423 (24.35%)\n",
            "\tTest loss: 0.00436, Accuracy: 110/443 (24.83%)\n",
            "\n",
            "Training Progress: \tEpoch 73 [0/1692 (0.00%)]\t\tLoss: 0.70824\n",
            "Training Progress: \tEpoch 73 [320/1692 (18.87%)]\t\tLoss: 1.09437\n",
            "Training Progress: \tEpoch 73 [640/1692 (37.74%)]\t\tLoss: 1.00267\n",
            "Training Progress: \tEpoch 73 [960/1692 (56.60%)]\t\tLoss: 0.80357\n",
            "Training Progress: \tEpoch 73 [1280/1692 (75.47%)]\t\tLoss: 0.80162\n",
            "Training Progress: \tEpoch 73 [1600/1692 (94.34%)]\t\tLoss: 0.76506\n",
            "\tTrain loss: 0.02340, Accuracy: 1192/1692 (70.45%)\n",
            "\tValidation loss: 0.00482, Accuracy: 112/423 (26.48%)\n",
            "\tTest loss: 0.00430, Accuracy: 116/443 (26.19%)\n",
            "\n",
            "Training Progress: \tEpoch 74 [0/1692 (0.00%)]\t\tLoss: 1.02192\n",
            "Training Progress: \tEpoch 74 [320/1692 (18.87%)]\t\tLoss: 1.16468\n",
            "Training Progress: \tEpoch 74 [640/1692 (37.74%)]\t\tLoss: 1.07774\n",
            "Training Progress: \tEpoch 74 [960/1692 (56.60%)]\t\tLoss: 0.80436\n",
            "Training Progress: \tEpoch 74 [1280/1692 (75.47%)]\t\tLoss: 0.99229\n",
            "Training Progress: \tEpoch 74 [1600/1692 (94.34%)]\t\tLoss: 0.78116\n",
            "\tTrain loss: 0.02179, Accuracy: 1237/1692 (73.11%)\n",
            "\tValidation loss: 0.00480, Accuracy: 98/423 (23.17%)\n",
            "\tTest loss: 0.00439, Accuracy: 122/443 (27.54%)\n",
            "\n",
            "Training Progress: \tEpoch 75 [0/1692 (0.00%)]\t\tLoss: 0.95514\n",
            "Training Progress: \tEpoch 75 [320/1692 (18.87%)]\t\tLoss: 1.21630\n",
            "Training Progress: \tEpoch 75 [640/1692 (37.74%)]\t\tLoss: 0.91954\n",
            "Training Progress: \tEpoch 75 [960/1692 (56.60%)]\t\tLoss: 0.94815\n",
            "Training Progress: \tEpoch 75 [1280/1692 (75.47%)]\t\tLoss: 0.64184\n",
            "Training Progress: \tEpoch 75 [1600/1692 (94.34%)]\t\tLoss: 0.90837\n",
            "\tTrain loss: 0.02178, Accuracy: 1253/1692 (74.05%)\n",
            "\tValidation loss: 0.00491, Accuracy: 98/423 (23.17%)\n",
            "\tTest loss: 0.00445, Accuracy: 130/443 (29.35%)\n",
            "\n",
            "Training Progress: \tEpoch 76 [0/1692 (0.00%)]\t\tLoss: 1.21225\n",
            "Training Progress: \tEpoch 76 [320/1692 (18.87%)]\t\tLoss: 0.82865\n",
            "Training Progress: \tEpoch 76 [640/1692 (37.74%)]\t\tLoss: 1.01057\n",
            "Training Progress: \tEpoch 76 [960/1692 (56.60%)]\t\tLoss: 0.94348\n",
            "Training Progress: \tEpoch 76 [1280/1692 (75.47%)]\t\tLoss: 0.74967\n",
            "Training Progress: \tEpoch 76 [1600/1692 (94.34%)]\t\tLoss: 0.76352\n",
            "\tTrain loss: 0.02155, Accuracy: 1234/1692 (72.93%)\n",
            "\tValidation loss: 0.00494, Accuracy: 97/423 (22.93%)\n",
            "\tTest loss: 0.00458, Accuracy: 120/443 (27.09%)\n",
            "\n",
            "Training Progress: \tEpoch 77 [0/1692 (0.00%)]\t\tLoss: 0.77494\n",
            "Training Progress: \tEpoch 77 [320/1692 (18.87%)]\t\tLoss: 1.06347\n",
            "Training Progress: \tEpoch 77 [640/1692 (37.74%)]\t\tLoss: 0.94594\n",
            "Training Progress: \tEpoch 77 [960/1692 (56.60%)]\t\tLoss: 0.75561\n",
            "Training Progress: \tEpoch 77 [1280/1692 (75.47%)]\t\tLoss: 0.78823\n",
            "Training Progress: \tEpoch 77 [1600/1692 (94.34%)]\t\tLoss: 0.97802\n",
            "\tTrain loss: 0.02148, Accuracy: 1245/1692 (73.58%)\n",
            "\tValidation loss: 0.00500, Accuracy: 102/423 (24.11%)\n",
            "\tTest loss: 0.00451, Accuracy: 125/443 (28.22%)\n",
            "\n",
            "Training Progress: \tEpoch 78 [0/1692 (0.00%)]\t\tLoss: 0.87188\n",
            "Training Progress: \tEpoch 78 [320/1692 (18.87%)]\t\tLoss: 0.91851\n",
            "Training Progress: \tEpoch 78 [640/1692 (37.74%)]\t\tLoss: 0.97189\n",
            "Training Progress: \tEpoch 78 [960/1692 (56.60%)]\t\tLoss: 0.88113\n",
            "Training Progress: \tEpoch 78 [1280/1692 (75.47%)]\t\tLoss: 0.78624\n",
            "Training Progress: \tEpoch 78 [1600/1692 (94.34%)]\t\tLoss: 0.72411\n",
            "\tTrain loss: 0.01994, Accuracy: 1297/1692 (76.65%)\n",
            "\tValidation loss: 0.00501, Accuracy: 100/423 (23.64%)\n",
            "\tTest loss: 0.00455, Accuracy: 124/443 (27.99%)\n",
            "\n",
            "Training Progress: \tEpoch 79 [0/1692 (0.00%)]\t\tLoss: 0.94046\n",
            "Training Progress: \tEpoch 79 [320/1692 (18.87%)]\t\tLoss: 1.31389\n",
            "Training Progress: \tEpoch 79 [640/1692 (37.74%)]\t\tLoss: 0.89977\n",
            "Training Progress: \tEpoch 79 [960/1692 (56.60%)]\t\tLoss: 0.82106\n",
            "Training Progress: \tEpoch 79 [1280/1692 (75.47%)]\t\tLoss: 0.82560\n",
            "Training Progress: \tEpoch 79 [1600/1692 (94.34%)]\t\tLoss: 0.78145\n",
            "\tTrain loss: 0.02021, Accuracy: 1280/1692 (75.65%)\n",
            "\tValidation loss: 0.00494, Accuracy: 103/423 (24.35%)\n",
            "\tTest loss: 0.00442, Accuracy: 122/443 (27.54%)\n",
            "\n",
            "Training Progress: \tEpoch 80 [0/1692 (0.00%)]\t\tLoss: 0.86303\n",
            "Training Progress: \tEpoch 80 [320/1692 (18.87%)]\t\tLoss: 0.94440\n",
            "Training Progress: \tEpoch 80 [640/1692 (37.74%)]\t\tLoss: 0.90093\n",
            "Training Progress: \tEpoch 80 [960/1692 (56.60%)]\t\tLoss: 0.94887\n",
            "Training Progress: \tEpoch 80 [1280/1692 (75.47%)]\t\tLoss: 0.71966\n",
            "Training Progress: \tEpoch 80 [1600/1692 (94.34%)]\t\tLoss: 0.58824\n",
            "\tTrain loss: 0.01960, Accuracy: 1297/1692 (76.65%)\n",
            "\tValidation loss: 0.00513, Accuracy: 92/423 (21.75%)\n",
            "\tTest loss: 0.00451, Accuracy: 126/443 (28.44%)\n",
            "\n",
            "Training Progress: \tEpoch 81 [0/1692 (0.00%)]\t\tLoss: 0.80591\n",
            "Training Progress: \tEpoch 81 [320/1692 (18.87%)]\t\tLoss: 0.85126\n",
            "Training Progress: \tEpoch 81 [640/1692 (37.74%)]\t\tLoss: 0.87154\n",
            "Training Progress: \tEpoch 81 [960/1692 (56.60%)]\t\tLoss: 0.73104\n",
            "Training Progress: \tEpoch 81 [1280/1692 (75.47%)]\t\tLoss: 0.70975\n",
            "Training Progress: \tEpoch 81 [1600/1692 (94.34%)]\t\tLoss: 0.65321\n",
            "\tTrain loss: 0.01912, Accuracy: 1305/1692 (77.13%)\n",
            "\tValidation loss: 0.00517, Accuracy: 94/423 (22.22%)\n",
            "\tTest loss: 0.00467, Accuracy: 123/443 (27.77%)\n",
            "\n",
            "Training Progress: \tEpoch 82 [0/1692 (0.00%)]\t\tLoss: 0.83982\n",
            "Training Progress: \tEpoch 82 [320/1692 (18.87%)]\t\tLoss: 1.14254\n",
            "Training Progress: \tEpoch 82 [640/1692 (37.74%)]\t\tLoss: 1.03953\n",
            "Training Progress: \tEpoch 82 [960/1692 (56.60%)]\t\tLoss: 0.86415\n",
            "Training Progress: \tEpoch 82 [1280/1692 (75.47%)]\t\tLoss: 0.59454\n",
            "Training Progress: \tEpoch 82 [1600/1692 (94.34%)]\t\tLoss: 0.59011\n",
            "\tTrain loss: 0.01937, Accuracy: 1316/1692 (77.78%)\n",
            "\tValidation loss: 0.00501, Accuracy: 88/423 (20.80%)\n",
            "\tTest loss: 0.00458, Accuracy: 115/443 (25.96%)\n",
            "\n",
            "Training Progress: \tEpoch 83 [0/1692 (0.00%)]\t\tLoss: 0.77461\n",
            "Training Progress: \tEpoch 83 [320/1692 (18.87%)]\t\tLoss: 1.04093\n",
            "Training Progress: \tEpoch 83 [640/1692 (37.74%)]\t\tLoss: 0.77447\n",
            "Training Progress: \tEpoch 83 [960/1692 (56.60%)]\t\tLoss: 0.99031\n",
            "Training Progress: \tEpoch 83 [1280/1692 (75.47%)]\t\tLoss: 0.63145\n",
            "Training Progress: \tEpoch 83 [1600/1692 (94.34%)]\t\tLoss: 0.57785\n",
            "\tTrain loss: 0.01948, Accuracy: 1310/1692 (77.42%)\n",
            "\tValidation loss: 0.00518, Accuracy: 96/423 (22.70%)\n",
            "\tTest loss: 0.00462, Accuracy: 121/443 (27.31%)\n",
            "\n",
            "Training Progress: \tEpoch 84 [0/1692 (0.00%)]\t\tLoss: 0.72778\n",
            "Training Progress: \tEpoch 84 [320/1692 (18.87%)]\t\tLoss: 0.89516\n",
            "Training Progress: \tEpoch 84 [640/1692 (37.74%)]\t\tLoss: 0.71396\n",
            "Training Progress: \tEpoch 84 [960/1692 (56.60%)]\t\tLoss: 0.64892\n",
            "Training Progress: \tEpoch 84 [1280/1692 (75.47%)]\t\tLoss: 0.86279\n",
            "Training Progress: \tEpoch 84 [1600/1692 (94.34%)]\t\tLoss: 0.65505\n",
            "\tTrain loss: 0.01888, Accuracy: 1312/1692 (77.54%)\n",
            "\tValidation loss: 0.00538, Accuracy: 96/423 (22.70%)\n",
            "\tTest loss: 0.00485, Accuracy: 119/443 (26.86%)\n",
            "\n",
            "Training Progress: \tEpoch 85 [0/1692 (0.00%)]\t\tLoss: 0.68077\n",
            "Training Progress: \tEpoch 85 [320/1692 (18.87%)]\t\tLoss: 1.02231\n",
            "Training Progress: \tEpoch 85 [640/1692 (37.74%)]\t\tLoss: 0.82587\n",
            "Training Progress: \tEpoch 85 [960/1692 (56.60%)]\t\tLoss: 0.82044\n",
            "Training Progress: \tEpoch 85 [1280/1692 (75.47%)]\t\tLoss: 0.52286\n",
            "Training Progress: \tEpoch 85 [1600/1692 (94.34%)]\t\tLoss: 0.64731\n",
            "\tTrain loss: 0.01864, Accuracy: 1311/1692 (77.48%)\n",
            "\tValidation loss: 0.00522, Accuracy: 111/423 (26.24%)\n",
            "\tTest loss: 0.00477, Accuracy: 116/443 (26.19%)\n",
            "\n",
            "Training Progress: \tEpoch 86 [0/1692 (0.00%)]\t\tLoss: 0.80924\n",
            "Training Progress: \tEpoch 86 [320/1692 (18.87%)]\t\tLoss: 0.98150\n",
            "Training Progress: \tEpoch 86 [640/1692 (37.74%)]\t\tLoss: 0.65419\n",
            "Training Progress: \tEpoch 86 [960/1692 (56.60%)]\t\tLoss: 0.59671\n",
            "Training Progress: \tEpoch 86 [1280/1692 (75.47%)]\t\tLoss: 0.56937\n",
            "Training Progress: \tEpoch 86 [1600/1692 (94.34%)]\t\tLoss: 0.55142\n",
            "\tTrain loss: 0.01778, Accuracy: 1333/1692 (78.78%)\n",
            "\tValidation loss: 0.00547, Accuracy: 101/423 (23.88%)\n",
            "\tTest loss: 0.00494, Accuracy: 113/443 (25.51%)\n",
            "\n",
            "Training Progress: \tEpoch 87 [0/1692 (0.00%)]\t\tLoss: 0.61448\n",
            "Training Progress: \tEpoch 87 [320/1692 (18.87%)]\t\tLoss: 1.02533\n",
            "Training Progress: \tEpoch 87 [640/1692 (37.74%)]\t\tLoss: 0.84992\n",
            "Training Progress: \tEpoch 87 [960/1692 (56.60%)]\t\tLoss: 0.72692\n",
            "Training Progress: \tEpoch 87 [1280/1692 (75.47%)]\t\tLoss: 0.76089\n",
            "Training Progress: \tEpoch 87 [1600/1692 (94.34%)]\t\tLoss: 0.70552\n",
            "\tTrain loss: 0.01773, Accuracy: 1353/1692 (79.96%)\n",
            "\tValidation loss: 0.00563, Accuracy: 94/423 (22.22%)\n",
            "\tTest loss: 0.00491, Accuracy: 123/443 (27.77%)\n",
            "\n",
            "Training Progress: \tEpoch 88 [0/1692 (0.00%)]\t\tLoss: 1.12880\n",
            "Training Progress: \tEpoch 88 [320/1692 (18.87%)]\t\tLoss: 1.23969\n",
            "Training Progress: \tEpoch 88 [640/1692 (37.74%)]\t\tLoss: 0.85262\n",
            "Training Progress: \tEpoch 88 [960/1692 (56.60%)]\t\tLoss: 0.81848\n",
            "Training Progress: \tEpoch 88 [1280/1692 (75.47%)]\t\tLoss: 0.95341\n",
            "Training Progress: \tEpoch 88 [1600/1692 (94.34%)]\t\tLoss: 0.68504\n",
            "\tTrain loss: 0.01836, Accuracy: 1324/1692 (78.25%)\n",
            "\tValidation loss: 0.00520, Accuracy: 104/423 (24.59%)\n",
            "\tTest loss: 0.00483, Accuracy: 119/443 (26.86%)\n",
            "\n",
            "Training Progress: \tEpoch 89 [0/1692 (0.00%)]\t\tLoss: 0.69213\n",
            "Training Progress: \tEpoch 89 [320/1692 (18.87%)]\t\tLoss: 0.80222\n",
            "Training Progress: \tEpoch 89 [640/1692 (37.74%)]\t\tLoss: 0.76966\n",
            "Training Progress: \tEpoch 89 [960/1692 (56.60%)]\t\tLoss: 0.66884\n",
            "Training Progress: \tEpoch 89 [1280/1692 (75.47%)]\t\tLoss: 0.68015\n",
            "Training Progress: \tEpoch 89 [1600/1692 (94.34%)]\t\tLoss: 0.77304\n",
            "\tTrain loss: 0.01765, Accuracy: 1348/1692 (79.67%)\n",
            "\tValidation loss: 0.00550, Accuracy: 101/423 (23.88%)\n",
            "\tTest loss: 0.00488, Accuracy: 124/443 (27.99%)\n",
            "\n",
            "Training Progress: \tEpoch 90 [0/1692 (0.00%)]\t\tLoss: 0.61390\n",
            "Training Progress: \tEpoch 90 [320/1692 (18.87%)]\t\tLoss: 0.99729\n",
            "Training Progress: \tEpoch 90 [640/1692 (37.74%)]\t\tLoss: 0.78598\n",
            "Training Progress: \tEpoch 90 [960/1692 (56.60%)]\t\tLoss: 0.85225\n",
            "Training Progress: \tEpoch 90 [1280/1692 (75.47%)]\t\tLoss: 0.65320\n",
            "Training Progress: \tEpoch 90 [1600/1692 (94.34%)]\t\tLoss: 0.82353\n",
            "\tTrain loss: 0.01789, Accuracy: 1336/1692 (78.96%)\n",
            "\tValidation loss: 0.00533, Accuracy: 101/423 (23.88%)\n",
            "\tTest loss: 0.00478, Accuracy: 129/443 (29.12%)\n",
            "\n",
            "Training Progress: \tEpoch 91 [0/1692 (0.00%)]\t\tLoss: 0.58823\n",
            "Training Progress: \tEpoch 91 [320/1692 (18.87%)]\t\tLoss: 0.92305\n",
            "Training Progress: \tEpoch 91 [640/1692 (37.74%)]\t\tLoss: 0.79930\n",
            "Training Progress: \tEpoch 91 [960/1692 (56.60%)]\t\tLoss: 0.86187\n",
            "Training Progress: \tEpoch 91 [1280/1692 (75.47%)]\t\tLoss: 0.71752\n",
            "Training Progress: \tEpoch 91 [1600/1692 (94.34%)]\t\tLoss: 0.62758\n",
            "\tTrain loss: 0.01674, Accuracy: 1368/1692 (80.85%)\n",
            "\tValidation loss: 0.00541, Accuracy: 100/423 (23.64%)\n",
            "\tTest loss: 0.00501, Accuracy: 117/443 (26.41%)\n",
            "\n",
            "Training Progress: \tEpoch 92 [0/1692 (0.00%)]\t\tLoss: 0.96296\n",
            "Training Progress: \tEpoch 92 [320/1692 (18.87%)]\t\tLoss: 0.80775\n",
            "Training Progress: \tEpoch 92 [640/1692 (37.74%)]\t\tLoss: 0.98638\n",
            "Training Progress: \tEpoch 92 [960/1692 (56.60%)]\t\tLoss: 0.72109\n",
            "Training Progress: \tEpoch 92 [1280/1692 (75.47%)]\t\tLoss: 0.56797\n",
            "Training Progress: \tEpoch 92 [1600/1692 (94.34%)]\t\tLoss: 0.70999\n",
            "\tTrain loss: 0.01710, Accuracy: 1361/1692 (80.44%)\n",
            "\tValidation loss: 0.00550, Accuracy: 100/423 (23.64%)\n",
            "\tTest loss: 0.00491, Accuracy: 124/443 (27.99%)\n",
            "\n",
            "Training Progress: \tEpoch 93 [0/1692 (0.00%)]\t\tLoss: 0.73539\n",
            "Training Progress: \tEpoch 93 [320/1692 (18.87%)]\t\tLoss: 0.78910\n",
            "Training Progress: \tEpoch 93 [640/1692 (37.74%)]\t\tLoss: 0.69364\n",
            "Training Progress: \tEpoch 93 [960/1692 (56.60%)]\t\tLoss: 0.65579\n",
            "Training Progress: \tEpoch 93 [1280/1692 (75.47%)]\t\tLoss: 0.74949\n",
            "Training Progress: \tEpoch 93 [1600/1692 (94.34%)]\t\tLoss: 0.64222\n",
            "\tTrain loss: 0.01591, Accuracy: 1376/1692 (81.32%)\n",
            "\tValidation loss: 0.00556, Accuracy: 108/423 (25.53%)\n",
            "\tTest loss: 0.00498, Accuracy: 119/443 (26.86%)\n",
            "\n",
            "Training Progress: \tEpoch 94 [0/1692 (0.00%)]\t\tLoss: 0.53936\n",
            "Training Progress: \tEpoch 94 [320/1692 (18.87%)]\t\tLoss: 0.99316\n",
            "Training Progress: \tEpoch 94 [640/1692 (37.74%)]\t\tLoss: 0.56137\n",
            "Training Progress: \tEpoch 94 [960/1692 (56.60%)]\t\tLoss: 0.54859\n",
            "Training Progress: \tEpoch 94 [1280/1692 (75.47%)]\t\tLoss: 0.66674\n",
            "Training Progress: \tEpoch 94 [1600/1692 (94.34%)]\t\tLoss: 0.59409\n",
            "\tTrain loss: 0.01593, Accuracy: 1372/1692 (81.09%)\n",
            "\tValidation loss: 0.00561, Accuracy: 111/423 (26.24%)\n",
            "\tTest loss: 0.00510, Accuracy: 120/443 (27.09%)\n",
            "\n",
            "Training Progress: \tEpoch 95 [0/1692 (0.00%)]\t\tLoss: 0.97661\n",
            "Training Progress: \tEpoch 95 [320/1692 (18.87%)]\t\tLoss: 0.75514\n",
            "Training Progress: \tEpoch 95 [640/1692 (37.74%)]\t\tLoss: 0.83884\n",
            "Training Progress: \tEpoch 95 [960/1692 (56.60%)]\t\tLoss: 0.62967\n",
            "Training Progress: \tEpoch 95 [1280/1692 (75.47%)]\t\tLoss: 0.62196\n",
            "Training Progress: \tEpoch 95 [1600/1692 (94.34%)]\t\tLoss: 0.56920\n",
            "\tTrain loss: 0.01572, Accuracy: 1396/1692 (82.51%)\n",
            "\tValidation loss: 0.00548, Accuracy: 103/423 (24.35%)\n",
            "\tTest loss: 0.00494, Accuracy: 117/443 (26.41%)\n",
            "\n",
            "Training Progress: \tEpoch 96 [0/1692 (0.00%)]\t\tLoss: 0.61363\n",
            "Training Progress: \tEpoch 96 [320/1692 (18.87%)]\t\tLoss: 0.85048\n",
            "Training Progress: \tEpoch 96 [640/1692 (37.74%)]\t\tLoss: 0.83391\n",
            "Training Progress: \tEpoch 96 [960/1692 (56.60%)]\t\tLoss: 0.49627\n",
            "Training Progress: \tEpoch 96 [1280/1692 (75.47%)]\t\tLoss: 0.64544\n",
            "Training Progress: \tEpoch 96 [1600/1692 (94.34%)]\t\tLoss: 0.64692\n",
            "\tTrain loss: 0.01560, Accuracy: 1384/1692 (81.80%)\n",
            "\tValidation loss: 0.00570, Accuracy: 98/423 (23.17%)\n",
            "\tTest loss: 0.00504, Accuracy: 128/443 (28.89%)\n",
            "\n",
            "Training Progress: \tEpoch 97 [0/1692 (0.00%)]\t\tLoss: 0.66487\n",
            "Training Progress: \tEpoch 97 [320/1692 (18.87%)]\t\tLoss: 0.75335\n",
            "Training Progress: \tEpoch 97 [640/1692 (37.74%)]\t\tLoss: 0.54126\n",
            "Training Progress: \tEpoch 97 [960/1692 (56.60%)]\t\tLoss: 0.70174\n",
            "Training Progress: \tEpoch 97 [1280/1692 (75.47%)]\t\tLoss: 0.49861\n",
            "Training Progress: \tEpoch 97 [1600/1692 (94.34%)]\t\tLoss: 0.63999\n",
            "\tTrain loss: 0.01537, Accuracy: 1387/1692 (81.97%)\n",
            "\tValidation loss: 0.00575, Accuracy: 104/423 (24.59%)\n",
            "\tTest loss: 0.00506, Accuracy: 126/443 (28.44%)\n",
            "\n",
            "Training Progress: \tEpoch 98 [0/1692 (0.00%)]\t\tLoss: 0.67915\n",
            "Training Progress: \tEpoch 98 [320/1692 (18.87%)]\t\tLoss: 0.80495\n",
            "Training Progress: \tEpoch 98 [640/1692 (37.74%)]\t\tLoss: 0.78859\n",
            "Training Progress: \tEpoch 98 [960/1692 (56.60%)]\t\tLoss: 0.80281\n",
            "Training Progress: \tEpoch 98 [1280/1692 (75.47%)]\t\tLoss: 0.73738\n",
            "Training Progress: \tEpoch 98 [1600/1692 (94.34%)]\t\tLoss: 0.51954\n",
            "\tTrain loss: 0.01440, Accuracy: 1413/1692 (83.51%)\n",
            "\tValidation loss: 0.00592, Accuracy: 96/423 (22.70%)\n",
            "\tTest loss: 0.00526, Accuracy: 123/443 (27.77%)\n",
            "\n",
            "Training Progress: \tEpoch 99 [0/1692 (0.00%)]\t\tLoss: 0.67508\n",
            "Training Progress: \tEpoch 99 [320/1692 (18.87%)]\t\tLoss: 0.77723\n",
            "Training Progress: \tEpoch 99 [640/1692 (37.74%)]\t\tLoss: 0.67934\n",
            "Training Progress: \tEpoch 99 [960/1692 (56.60%)]\t\tLoss: 0.49684\n",
            "Training Progress: \tEpoch 99 [1280/1692 (75.47%)]\t\tLoss: 0.51670\n",
            "Training Progress: \tEpoch 99 [1600/1692 (94.34%)]\t\tLoss: 0.69404\n",
            "\tTrain loss: 0.01535, Accuracy: 1393/1692 (82.33%)\n",
            "\tValidation loss: 0.00581, Accuracy: 103/423 (24.35%)\n",
            "\tTest loss: 0.00516, Accuracy: 114/443 (25.73%)\n",
            "\n",
            "Training Progress: \tEpoch 100 [0/1692 (0.00%)]\t\tLoss: 0.58696\n",
            "Training Progress: \tEpoch 100 [320/1692 (18.87%)]\t\tLoss: 0.77289\n",
            "Training Progress: \tEpoch 100 [640/1692 (37.74%)]\t\tLoss: 0.76320\n",
            "Training Progress: \tEpoch 100 [960/1692 (56.60%)]\t\tLoss: 0.47895\n",
            "Training Progress: \tEpoch 100 [1280/1692 (75.47%)]\t\tLoss: 0.62282\n",
            "Training Progress: \tEpoch 100 [1600/1692 (94.34%)]\t\tLoss: 0.75972\n",
            "\tTrain loss: 0.01426, Accuracy: 1415/1692 (83.63%)\n",
            "\tValidation loss: 0.00599, Accuracy: 98/423 (23.17%)\n",
            "\tTest loss: 0.00523, Accuracy: 121/443 (27.31%)\n",
            "\n",
            "[[tensor(0.0433), 0.2801418439716312], [tensor(0.0430), 0.3144208037825059], [tensor(0.0428), 0.33628841607565013], [tensor(0.0426), 0.3280141843971631], [tensor(0.0426), 0.33865248226950356], [tensor(0.0425), 0.3433806146572104], [tensor(0.0423), 0.34515366430260047], [tensor(0.0422), 0.35697399527186763], [tensor(0.0420), 0.3682033096926714], [tensor(0.0419), 0.3670212765957447], [tensor(0.0417), 0.36879432624113473], [tensor(0.0416), 0.3859338061465721], [tensor(0.0414), 0.37825059101654845], [tensor(0.0413), 0.37825059101654845], [tensor(0.0411), 0.37706855791962174], [tensor(0.0410), 0.3835697399527187], [tensor(0.0407), 0.39184397163120566], [tensor(0.0406), 0.3983451536643026], [tensor(0.0405), 0.3942080378250591], [tensor(0.0402), 0.4107565011820331], [tensor(0.0402), 0.41962174940898345], [tensor(0.0398), 0.42257683215130026], [tensor(0.0396), 0.43498817966903075], [tensor(0.0393), 0.425531914893617], [tensor(0.0392), 0.4314420803782506], [tensor(0.0388), 0.44739952718676124], [tensor(0.0384), 0.4574468085106383], [tensor(0.0384), 0.4639479905437352], [tensor(0.0380), 0.46808510638297873], [tensor(0.0379), 0.46749408983451535], [tensor(0.0377), 0.46867612293144206], [tensor(0.0374), 0.46867612293144206], [tensor(0.0369), 0.4946808510638298], [tensor(0.0364), 0.5177304964539007], [tensor(0.0365), 0.5118203309692672], [tensor(0.0360), 0.5041371158392435], [tensor(0.0358), 0.5159574468085106], [tensor(0.0353), 0.516548463356974], [tensor(0.0350), 0.5437352245862884], [tensor(0.0345), 0.5390070921985816], [tensor(0.0346), 0.5336879432624113], [tensor(0.0335), 0.5809692671394799], [tensor(0.0330), 0.5697399527186762], [tensor(0.0326), 0.5774231678486997], [tensor(0.0321), 0.5874704491725768], [tensor(0.0322), 0.5880614657210402], [tensor(0.0319), 0.5856973995271868], [tensor(0.0316), 0.6040189125295509], [tensor(0.0306), 0.6170212765957447], [tensor(0.0304), 0.6117021276595744], [tensor(0.0301), 0.6105200945626478], [tensor(0.0298), 0.6170212765957447], [tensor(0.0296), 0.6282505910165485], [tensor(0.0291), 0.6459810874704491], [tensor(0.0292), 0.6288416075650118], [tensor(0.0284), 0.6442080378250591], [tensor(0.0281), 0.6542553191489362], [tensor(0.0279), 0.6536643026004728], [tensor(0.0271), 0.6583924349881797], [tensor(0.0272), 0.6542553191489362], [tensor(0.0264), 0.6767139479905437], [tensor(0.0262), 0.6749408983451537], [tensor(0.0263), 0.6719858156028369], [tensor(0.0251), 0.6885342789598109], [tensor(0.0244), 0.6973995271867612], [tensor(0.0244), 0.6985815602836879], [tensor(0.0245), 0.6873522458628841], [tensor(0.0237), 0.7157210401891253], [tensor(0.0233), 0.7180851063829787], [tensor(0.0233), 0.7033096926713948], [tensor(0.0232), 0.7192671394799054], [tensor(0.0227), 0.725177304964539], [tensor(0.0234), 0.7044917257683215], [tensor(0.0218), 0.7310874704491725], [tensor(0.0218), 0.7405437352245863], [tensor(0.0216), 0.7293144208037825], [tensor(0.0215), 0.7358156028368794], [tensor(0.0199), 0.766548463356974], [tensor(0.0202), 0.7565011820330969], [tensor(0.0196), 0.766548463356974], [tensor(0.0191), 0.7712765957446809], [tensor(0.0194), 0.7777777777777778], [tensor(0.0195), 0.7742316784869976], [tensor(0.0189), 0.7754137115839244], [tensor(0.0186), 0.774822695035461], [tensor(0.0178), 0.7878250591016549], [tensor(0.0177), 0.799645390070922], [tensor(0.0184), 0.7825059101654847], [tensor(0.0176), 0.7966903073286052], [tensor(0.0179), 0.789598108747045], [tensor(0.0167), 0.8085106382978723], [tensor(0.0171), 0.8043735224586288], [tensor(0.0159), 0.8132387706855791], [tensor(0.0159), 0.8108747044917257], [tensor(0.0157), 0.8250591016548463], [tensor(0.0156), 0.817966903073286], [tensor(0.0154), 0.8197399527186762], [tensor(0.0144), 0.8351063829787234], [tensor(0.0154), 0.8232860520094563], [tensor(0.0143), 0.8362884160756501]]\n",
            "<class 'list'>\n",
            "[[tensor(0.0033), 0.26004728132387706], [tensor(0.0033), 0.23167848699763594], [tensor(0.0033), 0.2458628841607565], [tensor(0.0033), 0.24822695035460993], [tensor(0.0033), 0.2553191489361702], [tensor(0.0033), 0.2458628841607565], [tensor(0.0033), 0.2576832151300236], [tensor(0.0033), 0.23167848699763594], [tensor(0.0033), 0.25295508274231676], [tensor(0.0033), 0.24113475177304963], [tensor(0.0034), 0.24113475177304963], [tensor(0.0033), 0.2576832151300236], [tensor(0.0034), 0.2458628841607565], [tensor(0.0034), 0.23404255319148937], [tensor(0.0034), 0.23877068557919623], [tensor(0.0034), 0.2553191489361702], [tensor(0.0034), 0.25295508274231676], [tensor(0.0034), 0.23877068557919623], [tensor(0.0034), 0.2458628841607565], [tensor(0.0034), 0.23167848699763594], [tensor(0.0034), 0.22458628841607564], [tensor(0.0034), 0.2293144208037825], [tensor(0.0035), 0.2198581560283688], [tensor(0.0035), 0.22458628841607564], [tensor(0.0035), 0.21749408983451538], [tensor(0.0035), 0.24822695035460993], [tensor(0.0035), 0.23404255319148937], [tensor(0.0035), 0.25059101654846333], [tensor(0.0035), 0.2364066193853428], [tensor(0.0035), 0.2765957446808511], [tensor(0.0036), 0.24113475177304963], [tensor(0.0036), 0.2458628841607565], [tensor(0.0036), 0.2458628841607565], [tensor(0.0036), 0.2576832151300236], [tensor(0.0036), 0.25295508274231676], [tensor(0.0036), 0.24113475177304963], [tensor(0.0036), 0.23877068557919623], [tensor(0.0037), 0.26713947990543735], [tensor(0.0037), 0.24349881796690306], [tensor(0.0037), 0.23167848699763594], [tensor(0.0037), 0.24822695035460993], [tensor(0.0037), 0.24349881796690306], [tensor(0.0038), 0.26004728132387706], [tensor(0.0038), 0.2647754137115839], [tensor(0.0038), 0.24113475177304963], [tensor(0.0039), 0.23877068557919623], [tensor(0.0040), 0.22695035460992907], [tensor(0.0039), 0.22695035460992907], [tensor(0.0040), 0.2458628841607565], [tensor(0.0041), 0.23404255319148937], [tensor(0.0041), 0.23877068557919623], [tensor(0.0041), 0.2127659574468085], [tensor(0.0041), 0.2127659574468085], [tensor(0.0042), 0.23877068557919623], [tensor(0.0042), 0.22695035460992907], [tensor(0.0042), 0.23404255319148937], [tensor(0.0042), 0.23167848699763594], [tensor(0.0043), 0.2458628841607565], [tensor(0.0042), 0.25295508274231676], [tensor(0.0043), 0.2458628841607565], [tensor(0.0043), 0.24349881796690306], [tensor(0.0044), 0.23877068557919623], [tensor(0.0044), 0.24113475177304963], [tensor(0.0045), 0.23167848699763594], [tensor(0.0046), 0.2458628841607565], [tensor(0.0046), 0.23877068557919623], [tensor(0.0047), 0.2364066193853428], [tensor(0.0047), 0.23877068557919623], [tensor(0.0047), 0.22458628841607564], [tensor(0.0048), 0.2293144208037825], [tensor(0.0048), 0.24822695035460993], [tensor(0.0048), 0.24349881796690306], [tensor(0.0048), 0.2647754137115839], [tensor(0.0048), 0.23167848699763594], [tensor(0.0049), 0.23167848699763594], [tensor(0.0049), 0.2293144208037825], [tensor(0.0050), 0.24113475177304963], [tensor(0.0050), 0.2364066193853428], [tensor(0.0049), 0.24349881796690306], [tensor(0.0051), 0.21749408983451538], [tensor(0.0052), 0.2222222222222222], [tensor(0.0050), 0.20803782505910165], [tensor(0.0052), 0.22695035460992907], [tensor(0.0054), 0.22695035460992907], [tensor(0.0052), 0.2624113475177305], [tensor(0.0055), 0.23877068557919623], [tensor(0.0056), 0.2222222222222222], [tensor(0.0052), 0.2458628841607565], [tensor(0.0055), 0.23877068557919623], [tensor(0.0053), 0.23877068557919623], [tensor(0.0054), 0.2364066193853428], [tensor(0.0055), 0.2364066193853428], [tensor(0.0056), 0.2553191489361702], [tensor(0.0056), 0.2624113475177305], [tensor(0.0055), 0.24349881796690306], [tensor(0.0057), 0.23167848699763594], [tensor(0.0058), 0.2458628841607565], [tensor(0.0059), 0.22695035460992907], [tensor(0.0058), 0.24349881796690306], [tensor(0.0060), 0.23167848699763594]]\n",
            "<class 'list'>\n",
            "[[tensor(0.0032), 0.22121896162528218], [tensor(0.0032), 0.20993227990970656], [tensor(0.0032), 0.23024830699774265], [tensor(0.0032), 0.20316027088036118], [tensor(0.0032), 0.23024830699774265], [tensor(0.0032), 0.2325056433408578], [tensor(0.0032), 0.2325056433408578], [tensor(0.0032), 0.2618510158013544], [tensor(0.0032), 0.24604966139954854], [tensor(0.0032), 0.255079006772009], [tensor(0.0032), 0.24604966139954854], [tensor(0.0032), 0.24604966139954854], [tensor(0.0032), 0.24830699774266365], [tensor(0.0032), 0.24830699774266365], [tensor(0.0032), 0.2708803611738149], [tensor(0.0032), 0.24379232505643342], [tensor(0.0032), 0.23702031602708803], [tensor(0.0032), 0.2595936794582393], [tensor(0.0032), 0.2325056433408578], [tensor(0.0032), 0.2505643340857788], [tensor(0.0032), 0.2686230248306998], [tensor(0.0032), 0.27313769751693], [tensor(0.0033), 0.27765237020316025], [tensor(0.0032), 0.27313769751693], [tensor(0.0033), 0.2799097065462754], [tensor(0.0033), 0.24604966139954854], [tensor(0.0033), 0.25733634311512416], [tensor(0.0033), 0.2505643340857788], [tensor(0.0033), 0.26410835214446954], [tensor(0.0033), 0.255079006772009], [tensor(0.0033), 0.26636568848758463], [tensor(0.0034), 0.2686230248306998], [tensor(0.0034), 0.2618510158013544], [tensor(0.0034), 0.24604966139954854], [tensor(0.0034), 0.24153498871331827], [tensor(0.0034), 0.255079006772009], [tensor(0.0034), 0.2528216704288939], [tensor(0.0034), 0.2618510158013544], [tensor(0.0034), 0.27313769751693], [tensor(0.0035), 0.27313769751693], [tensor(0.0035), 0.2708803611738149], [tensor(0.0036), 0.255079006772009], [tensor(0.0035), 0.27313769751693], [tensor(0.0036), 0.23927765237020315], [tensor(0.0036), 0.26410835214446954], [tensor(0.0037), 0.2595936794582393], [tensor(0.0037), 0.26636568848758463], [tensor(0.0036), 0.26410835214446954], [tensor(0.0037), 0.2618510158013544], [tensor(0.0038), 0.2686230248306998], [tensor(0.0038), 0.26410835214446954], [tensor(0.0038), 0.2618510158013544], [tensor(0.0038), 0.2708803611738149], [tensor(0.0039), 0.26636568848758463], [tensor(0.0039), 0.2708803611738149], [tensor(0.0039), 0.255079006772009], [tensor(0.0039), 0.2686230248306998], [tensor(0.0040), 0.2686230248306998], [tensor(0.0040), 0.25733634311512416], [tensor(0.0040), 0.2708803611738149], [tensor(0.0040), 0.26636568848758463], [tensor(0.0041), 0.291196388261851], [tensor(0.0040), 0.27313769751693], [tensor(0.0042), 0.27313769751693], [tensor(0.0043), 0.24604966139954854], [tensor(0.0041), 0.26636568848758463], [tensor(0.0043), 0.24379232505643342], [tensor(0.0043), 0.2595936794582393], [tensor(0.0042), 0.28216704288939054], [tensor(0.0044), 0.2595936794582393], [tensor(0.0044), 0.24604966139954854], [tensor(0.0044), 0.24830699774266365], [tensor(0.0043), 0.2618510158013544], [tensor(0.0044), 0.27539503386004516], [tensor(0.0045), 0.29345372460496616], [tensor(0.0046), 0.2708803611738149], [tensor(0.0045), 0.28216704288939054], [tensor(0.0046), 0.2799097065462754], [tensor(0.0044), 0.27539503386004516], [tensor(0.0045), 0.28442437923250563], [tensor(0.0047), 0.27765237020316025], [tensor(0.0046), 0.2595936794582393], [tensor(0.0046), 0.27313769751693], [tensor(0.0048), 0.2686230248306998], [tensor(0.0048), 0.2618510158013544], [tensor(0.0049), 0.255079006772009], [tensor(0.0049), 0.27765237020316025], [tensor(0.0048), 0.2686230248306998], [tensor(0.0049), 0.2799097065462754], [tensor(0.0048), 0.291196388261851], [tensor(0.0050), 0.26410835214446954], [tensor(0.0049), 0.2799097065462754], [tensor(0.0050), 0.2686230248306998], [tensor(0.0051), 0.2708803611738149], [tensor(0.0049), 0.26410835214446954], [tensor(0.0050), 0.28893905191873587], [tensor(0.0051), 0.28442437923250563], [tensor(0.0053), 0.27765237020316025], [tensor(0.0052), 0.25733634311512416], [tensor(0.0052), 0.27313769751693]]\n",
            "<class 'list'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Best validation accuracy:\n",
        "0.2907\n",
        "\n",
        "Best test accuracy:\n",
        "0.2889\n",
        "\n",
        "## Plotting Metrics v/s Number of Epochs: \n",
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "X919OrAUmF39"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRWbk2VnfCy0"
      },
      "source": [
        "## CNN + GRU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivhCRh-a_jx4",
        "outputId": "e68cbacd-3071-4e57-bad8-fc975f6c1b71",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = CNN_GRU().cuda()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR, betas=BETAS, eps=EPS, weight_decay=DECAY)\n",
        "train_and_evaluate(model, optimizer, data_loaders, num_epochs=EPOCHS)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Progress: \tEpoch 1 [0/1692 (0.00%)]\t\tLoss: 1.40334\n",
            "Training Progress: \tEpoch 1 [320/1692 (18.87%)]\t\tLoss: 1.40755\n",
            "Training Progress: \tEpoch 1 [640/1692 (37.74%)]\t\tLoss: 1.39404\n",
            "Training Progress: \tEpoch 1 [960/1692 (56.60%)]\t\tLoss: 1.39243\n",
            "Training Progress: \tEpoch 1 [1280/1692 (75.47%)]\t\tLoss: 1.40018\n",
            "Training Progress: \tEpoch 1 [1600/1692 (94.34%)]\t\tLoss: 1.38327\n",
            "\tTrain loss: 0.04337, Accuracy: 454/1692 (26.83%)\n",
            "\tValidation loss: 0.00327, Accuracy: 119/423 (28.13%)\n",
            "\tTest loss: 0.00312, Accuracy: 122/443 (27.54%)\n",
            "\n",
            "Training Progress: \tEpoch 2 [0/1692 (0.00%)]\t\tLoss: 1.35312\n",
            "Training Progress: \tEpoch 2 [320/1692 (18.87%)]\t\tLoss: 1.38092\n",
            "Training Progress: \tEpoch 2 [640/1692 (37.74%)]\t\tLoss: 1.35015\n",
            "Training Progress: \tEpoch 2 [960/1692 (56.60%)]\t\tLoss: 1.36225\n",
            "Training Progress: \tEpoch 2 [1280/1692 (75.47%)]\t\tLoss: 1.42665\n",
            "Training Progress: \tEpoch 2 [1600/1692 (94.34%)]\t\tLoss: 1.36361\n",
            "\tTrain loss: 0.04336, Accuracy: 451/1692 (26.65%)\n",
            "\tValidation loss: 0.00327, Accuracy: 115/423 (27.19%)\n",
            "\tTest loss: 0.00312, Accuracy: 117/443 (26.41%)\n",
            "\n",
            "Training Progress: \tEpoch 3 [0/1692 (0.00%)]\t\tLoss: 1.31655\n",
            "Training Progress: \tEpoch 3 [320/1692 (18.87%)]\t\tLoss: 1.29991\n",
            "Training Progress: \tEpoch 3 [640/1692 (37.74%)]\t\tLoss: 1.33281\n",
            "Training Progress: \tEpoch 3 [960/1692 (56.60%)]\t\tLoss: 1.35555\n",
            "Training Progress: \tEpoch 3 [1280/1692 (75.47%)]\t\tLoss: 1.41944\n",
            "Training Progress: \tEpoch 3 [1600/1692 (94.34%)]\t\tLoss: 1.31494\n",
            "\tTrain loss: 0.04373, Accuracy: 466/1692 (27.54%)\n",
            "\tValidation loss: 0.00330, Accuracy: 119/423 (28.13%)\n",
            "\tTest loss: 0.00313, Accuracy: 120/443 (27.09%)\n",
            "\n",
            "Training Progress: \tEpoch 4 [0/1692 (0.00%)]\t\tLoss: 1.27851\n",
            "Training Progress: \tEpoch 4 [320/1692 (18.87%)]\t\tLoss: 1.29816\n",
            "Training Progress: \tEpoch 4 [640/1692 (37.74%)]\t\tLoss: 1.27603\n",
            "Training Progress: \tEpoch 4 [960/1692 (56.60%)]\t\tLoss: 1.33214\n",
            "Training Progress: \tEpoch 4 [1280/1692 (75.47%)]\t\tLoss: 1.37635\n",
            "Training Progress: \tEpoch 4 [1600/1692 (94.34%)]\t\tLoss: 1.25384\n",
            "\tTrain loss: 0.04310, Accuracy: 488/1692 (28.84%)\n",
            "\tValidation loss: 0.00327, Accuracy: 122/423 (28.84%)\n",
            "\tTest loss: 0.00308, Accuracy: 130/443 (29.35%)\n",
            "\n",
            "Training Progress: \tEpoch 5 [0/1692 (0.00%)]\t\tLoss: 1.30744\n",
            "Training Progress: \tEpoch 5 [320/1692 (18.87%)]\t\tLoss: 1.28879\n",
            "Training Progress: \tEpoch 5 [640/1692 (37.74%)]\t\tLoss: 1.29231\n",
            "Training Progress: \tEpoch 5 [960/1692 (56.60%)]\t\tLoss: 1.27613\n",
            "Training Progress: \tEpoch 5 [1280/1692 (75.47%)]\t\tLoss: 1.34318\n",
            "Training Progress: \tEpoch 5 [1600/1692 (94.34%)]\t\tLoss: 1.26535\n",
            "\tTrain loss: 0.04072, Accuracy: 647/1692 (38.24%)\n",
            "\tValidation loss: 0.00310, Accuracy: 151/423 (35.70%)\n",
            "\tTest loss: 0.00293, Accuracy: 169/443 (38.15%)\n",
            "\n",
            "Training Progress: \tEpoch 6 [0/1692 (0.00%)]\t\tLoss: 1.26128\n",
            "Training Progress: \tEpoch 6 [320/1692 (18.87%)]\t\tLoss: 1.21567\n",
            "Training Progress: \tEpoch 6 [640/1692 (37.74%)]\t\tLoss: 1.27434\n",
            "Training Progress: \tEpoch 6 [960/1692 (56.60%)]\t\tLoss: 1.35802\n",
            "Training Progress: \tEpoch 6 [1280/1692 (75.47%)]\t\tLoss: 1.34679\n",
            "Training Progress: \tEpoch 6 [1600/1692 (94.34%)]\t\tLoss: 1.19869\n",
            "\tTrain loss: 0.04106, Accuracy: 609/1692 (35.99%)\n",
            "\tValidation loss: 0.00319, Accuracy: 134/423 (31.68%)\n",
            "\tTest loss: 0.00297, Accuracy: 163/443 (36.79%)\n",
            "\n",
            "Training Progress: \tEpoch 7 [0/1692 (0.00%)]\t\tLoss: 1.31303\n",
            "Training Progress: \tEpoch 7 [320/1692 (18.87%)]\t\tLoss: 1.35196\n",
            "Training Progress: \tEpoch 7 [640/1692 (37.74%)]\t\tLoss: 1.25578\n",
            "Training Progress: \tEpoch 7 [960/1692 (56.60%)]\t\tLoss: 1.32208\n",
            "Training Progress: \tEpoch 7 [1280/1692 (75.47%)]\t\tLoss: 1.29529\n",
            "Training Progress: \tEpoch 7 [1600/1692 (94.34%)]\t\tLoss: 1.21813\n",
            "\tTrain loss: 0.04054, Accuracy: 628/1692 (37.12%)\n",
            "\tValidation loss: 0.00316, Accuracy: 152/423 (35.93%)\n",
            "\tTest loss: 0.00294, Accuracy: 163/443 (36.79%)\n",
            "\n",
            "Training Progress: \tEpoch 8 [0/1692 (0.00%)]\t\tLoss: 1.28605\n",
            "Training Progress: \tEpoch 8 [320/1692 (18.87%)]\t\tLoss: 1.28240\n",
            "Training Progress: \tEpoch 8 [640/1692 (37.74%)]\t\tLoss: 1.31185\n",
            "Training Progress: \tEpoch 8 [960/1692 (56.60%)]\t\tLoss: 1.24821\n",
            "Training Progress: \tEpoch 8 [1280/1692 (75.47%)]\t\tLoss: 1.25977\n",
            "Training Progress: \tEpoch 8 [1600/1692 (94.34%)]\t\tLoss: 1.23424\n",
            "\tTrain loss: 0.03911, Accuracy: 695/1692 (41.08%)\n",
            "\tValidation loss: 0.00306, Accuracy: 155/423 (36.64%)\n",
            "\tTest loss: 0.00286, Accuracy: 183/443 (41.31%)\n",
            "\n",
            "Training Progress: \tEpoch 9 [0/1692 (0.00%)]\t\tLoss: 1.21502\n",
            "Training Progress: \tEpoch 9 [320/1692 (18.87%)]\t\tLoss: 1.33437\n",
            "Training Progress: \tEpoch 9 [640/1692 (37.74%)]\t\tLoss: 1.17811\n",
            "Training Progress: \tEpoch 9 [960/1692 (56.60%)]\t\tLoss: 1.23799\n",
            "Training Progress: \tEpoch 9 [1280/1692 (75.47%)]\t\tLoss: 1.30284\n",
            "Training Progress: \tEpoch 9 [1600/1692 (94.34%)]\t\tLoss: 1.05798\n",
            "\tTrain loss: 0.03893, Accuracy: 715/1692 (42.26%)\n",
            "\tValidation loss: 0.00303, Accuracy: 167/423 (39.48%)\n",
            "\tTest loss: 0.00285, Accuracy: 185/443 (41.76%)\n",
            "\n",
            "Training Progress: \tEpoch 10 [0/1692 (0.00%)]\t\tLoss: 1.19039\n",
            "Training Progress: \tEpoch 10 [320/1692 (18.87%)]\t\tLoss: 1.35253\n",
            "Training Progress: \tEpoch 10 [640/1692 (37.74%)]\t\tLoss: 1.22633\n",
            "Training Progress: \tEpoch 10 [960/1692 (56.60%)]\t\tLoss: 1.21759\n",
            "Training Progress: \tEpoch 10 [1280/1692 (75.47%)]\t\tLoss: 1.28193\n",
            "Training Progress: \tEpoch 10 [1600/1692 (94.34%)]\t\tLoss: 1.13815\n",
            "\tTrain loss: 0.03765, Accuracy: 773/1692 (45.69%)\n",
            "\tValidation loss: 0.00296, Accuracy: 171/423 (40.43%)\n",
            "\tTest loss: 0.00279, Accuracy: 202/443 (45.60%)\n",
            "\n",
            "Training Progress: \tEpoch 11 [0/1692 (0.00%)]\t\tLoss: 1.15559\n",
            "Training Progress: \tEpoch 11 [320/1692 (18.87%)]\t\tLoss: 1.22522\n",
            "Training Progress: \tEpoch 11 [640/1692 (37.74%)]\t\tLoss: 1.16779\n",
            "Training Progress: \tEpoch 11 [960/1692 (56.60%)]\t\tLoss: 1.18157\n",
            "Training Progress: \tEpoch 11 [1280/1692 (75.47%)]\t\tLoss: 1.34917\n",
            "Training Progress: \tEpoch 11 [1600/1692 (94.34%)]\t\tLoss: 1.04311\n",
            "\tTrain loss: 0.03939, Accuracy: 720/1692 (42.55%)\n",
            "\tValidation loss: 0.00312, Accuracy: 168/423 (39.72%)\n",
            "\tTest loss: 0.00287, Accuracy: 197/443 (44.47%)\n",
            "\n",
            "Training Progress: \tEpoch 12 [0/1692 (0.00%)]\t\tLoss: 1.17176\n",
            "Training Progress: \tEpoch 12 [320/1692 (18.87%)]\t\tLoss: 1.14961\n",
            "Training Progress: \tEpoch 12 [640/1692 (37.74%)]\t\tLoss: 1.17997\n",
            "Training Progress: \tEpoch 12 [960/1692 (56.60%)]\t\tLoss: 1.16474\n",
            "Training Progress: \tEpoch 12 [1280/1692 (75.47%)]\t\tLoss: 1.29312\n",
            "Training Progress: \tEpoch 12 [1600/1692 (94.34%)]\t\tLoss: 1.08295\n",
            "\tTrain loss: 0.03522, Accuracy: 859/1692 (50.77%)\n",
            "\tValidation loss: 0.00290, Accuracy: 185/423 (43.74%)\n",
            "\tTest loss: 0.00267, Accuracy: 215/443 (48.53%)\n",
            "\n",
            "Training Progress: \tEpoch 13 [0/1692 (0.00%)]\t\tLoss: 1.19649\n",
            "Training Progress: \tEpoch 13 [320/1692 (18.87%)]\t\tLoss: 1.22154\n",
            "Training Progress: \tEpoch 13 [640/1692 (37.74%)]\t\tLoss: 1.10728\n",
            "Training Progress: \tEpoch 13 [960/1692 (56.60%)]\t\tLoss: 0.96509\n",
            "Training Progress: \tEpoch 13 [1280/1692 (75.47%)]\t\tLoss: 1.27782\n",
            "Training Progress: \tEpoch 13 [1600/1692 (94.34%)]\t\tLoss: 0.97053\n",
            "\tTrain loss: 0.03581, Accuracy: 817/1692 (48.29%)\n",
            "\tValidation loss: 0.00289, Accuracy: 186/423 (43.97%)\n",
            "\tTest loss: 0.00268, Accuracy: 213/443 (48.08%)\n",
            "\n",
            "Training Progress: \tEpoch 14 [0/1692 (0.00%)]\t\tLoss: 1.24501\n",
            "Training Progress: \tEpoch 14 [320/1692 (18.87%)]\t\tLoss: 1.21784\n",
            "Training Progress: \tEpoch 14 [640/1692 (37.74%)]\t\tLoss: 1.05023\n",
            "Training Progress: \tEpoch 14 [960/1692 (56.60%)]\t\tLoss: 1.05793\n",
            "Training Progress: \tEpoch 14 [1280/1692 (75.47%)]\t\tLoss: 1.25951\n",
            "Training Progress: \tEpoch 14 [1600/1692 (94.34%)]\t\tLoss: 0.93582\n",
            "\tTrain loss: 0.03750, Accuracy: 788/1692 (46.57%)\n",
            "\tValidation loss: 0.00301, Accuracy: 192/423 (45.39%)\n",
            "\tTest loss: 0.00274, Accuracy: 211/443 (47.63%)\n",
            "\n",
            "Training Progress: \tEpoch 15 [0/1692 (0.00%)]\t\tLoss: 1.18762\n",
            "Training Progress: \tEpoch 15 [320/1692 (18.87%)]\t\tLoss: 1.14379\n",
            "Training Progress: \tEpoch 15 [640/1692 (37.74%)]\t\tLoss: 1.12964\n",
            "Training Progress: \tEpoch 15 [960/1692 (56.60%)]\t\tLoss: 0.97941\n",
            "Training Progress: \tEpoch 15 [1280/1692 (75.47%)]\t\tLoss: 1.25225\n",
            "Training Progress: \tEpoch 15 [1600/1692 (94.34%)]\t\tLoss: 1.02793\n",
            "\tTrain loss: 0.03931, Accuracy: 759/1692 (44.86%)\n",
            "\tValidation loss: 0.00320, Accuracy: 176/423 (41.61%)\n",
            "\tTest loss: 0.00294, Accuracy: 199/443 (44.92%)\n",
            "\n",
            "Training Progress: \tEpoch 16 [0/1692 (0.00%)]\t\tLoss: 1.25370\n",
            "Training Progress: \tEpoch 16 [320/1692 (18.87%)]\t\tLoss: 1.06769\n",
            "Training Progress: \tEpoch 16 [640/1692 (37.74%)]\t\tLoss: 1.08245\n",
            "Training Progress: \tEpoch 16 [960/1692 (56.60%)]\t\tLoss: 1.02054\n",
            "Training Progress: \tEpoch 16 [1280/1692 (75.47%)]\t\tLoss: 1.23487\n",
            "Training Progress: \tEpoch 16 [1600/1692 (94.34%)]\t\tLoss: 0.94238\n",
            "\tTrain loss: 0.03593, Accuracy: 837/1692 (49.47%)\n",
            "\tValidation loss: 0.00303, Accuracy: 194/423 (45.86%)\n",
            "\tTest loss: 0.00273, Accuracy: 213/443 (48.08%)\n",
            "\n",
            "Training Progress: \tEpoch 17 [0/1692 (0.00%)]\t\tLoss: 1.18657\n",
            "Training Progress: \tEpoch 17 [320/1692 (18.87%)]\t\tLoss: 1.13804\n",
            "Training Progress: \tEpoch 17 [640/1692 (37.74%)]\t\tLoss: 0.98689\n",
            "Training Progress: \tEpoch 17 [960/1692 (56.60%)]\t\tLoss: 1.01467\n",
            "Training Progress: \tEpoch 17 [1280/1692 (75.47%)]\t\tLoss: 1.21101\n",
            "Training Progress: \tEpoch 17 [1600/1692 (94.34%)]\t\tLoss: 1.00801\n",
            "\tTrain loss: 0.03438, Accuracy: 882/1692 (52.13%)\n",
            "\tValidation loss: 0.00289, Accuracy: 198/423 (46.81%)\n",
            "\tTest loss: 0.00262, Accuracy: 224/443 (50.56%)\n",
            "\n",
            "Training Progress: \tEpoch 18 [0/1692 (0.00%)]\t\tLoss: 1.14667\n",
            "Training Progress: \tEpoch 18 [320/1692 (18.87%)]\t\tLoss: 1.00434\n",
            "Training Progress: \tEpoch 18 [640/1692 (37.74%)]\t\tLoss: 1.02517\n",
            "Training Progress: \tEpoch 18 [960/1692 (56.60%)]\t\tLoss: 0.95645\n",
            "Training Progress: \tEpoch 18 [1280/1692 (75.47%)]\t\tLoss: 1.21304\n",
            "Training Progress: \tEpoch 18 [1600/1692 (94.34%)]\t\tLoss: 0.99706\n",
            "\tTrain loss: 0.03497, Accuracy: 871/1692 (51.48%)\n",
            "\tValidation loss: 0.00302, Accuracy: 188/423 (44.44%)\n",
            "\tTest loss: 0.00270, Accuracy: 212/443 (47.86%)\n",
            "\n",
            "Training Progress: \tEpoch 19 [0/1692 (0.00%)]\t\tLoss: 1.12563\n",
            "Training Progress: \tEpoch 19 [320/1692 (18.87%)]\t\tLoss: 1.14595\n",
            "Training Progress: \tEpoch 19 [640/1692 (37.74%)]\t\tLoss: 0.92585\n",
            "Training Progress: \tEpoch 19 [960/1692 (56.60%)]\t\tLoss: 1.02699\n",
            "Training Progress: \tEpoch 19 [1280/1692 (75.47%)]\t\tLoss: 1.10750\n",
            "Training Progress: \tEpoch 19 [1600/1692 (94.34%)]\t\tLoss: 0.91806\n",
            "\tTrain loss: 0.03265, Accuracy: 938/1692 (55.44%)\n",
            "\tValidation loss: 0.00282, Accuracy: 208/423 (49.17%)\n",
            "\tTest loss: 0.00255, Accuracy: 220/443 (49.66%)\n",
            "\n",
            "Training Progress: \tEpoch 20 [0/1692 (0.00%)]\t\tLoss: 1.13978\n",
            "Training Progress: \tEpoch 20 [320/1692 (18.87%)]\t\tLoss: 0.97253\n",
            "Training Progress: \tEpoch 20 [640/1692 (37.74%)]\t\tLoss: 1.08695\n",
            "Training Progress: \tEpoch 20 [960/1692 (56.60%)]\t\tLoss: 1.01910\n",
            "Training Progress: \tEpoch 20 [1280/1692 (75.47%)]\t\tLoss: 1.19240\n",
            "Training Progress: \tEpoch 20 [1600/1692 (94.34%)]\t\tLoss: 0.92423\n",
            "\tTrain loss: 0.03243, Accuracy: 957/1692 (56.56%)\n",
            "\tValidation loss: 0.00285, Accuracy: 201/423 (47.52%)\n",
            "\tTest loss: 0.00260, Accuracy: 222/443 (50.11%)\n",
            "\n",
            "Training Progress: \tEpoch 21 [0/1692 (0.00%)]\t\tLoss: 1.17267\n",
            "Training Progress: \tEpoch 21 [320/1692 (18.87%)]\t\tLoss: 1.14147\n",
            "Training Progress: \tEpoch 21 [640/1692 (37.74%)]\t\tLoss: 1.05519\n",
            "Training Progress: \tEpoch 21 [960/1692 (56.60%)]\t\tLoss: 0.87263\n",
            "Training Progress: \tEpoch 21 [1280/1692 (75.47%)]\t\tLoss: 1.04735\n",
            "Training Progress: \tEpoch 21 [1600/1692 (94.34%)]\t\tLoss: 0.93140\n",
            "\tTrain loss: 0.03501, Accuracy: 887/1692 (52.42%)\n",
            "\tValidation loss: 0.00304, Accuracy: 192/423 (45.39%)\n",
            "\tTest loss: 0.00274, Accuracy: 217/443 (48.98%)\n",
            "\n",
            "Training Progress: \tEpoch 22 [0/1692 (0.00%)]\t\tLoss: 1.17824\n",
            "Training Progress: \tEpoch 22 [320/1692 (18.87%)]\t\tLoss: 1.05753\n",
            "Training Progress: \tEpoch 22 [640/1692 (37.74%)]\t\tLoss: 1.12768\n",
            "Training Progress: \tEpoch 22 [960/1692 (56.60%)]\t\tLoss: 0.98592\n",
            "Training Progress: \tEpoch 22 [1280/1692 (75.47%)]\t\tLoss: 1.12932\n",
            "Training Progress: \tEpoch 22 [1600/1692 (94.34%)]\t\tLoss: 0.86425\n",
            "\tTrain loss: 0.03352, Accuracy: 917/1692 (54.20%)\n",
            "\tValidation loss: 0.00297, Accuracy: 192/423 (45.39%)\n",
            "\tTest loss: 0.00275, Accuracy: 208/443 (46.95%)\n",
            "\n",
            "Training Progress: \tEpoch 23 [0/1692 (0.00%)]\t\tLoss: 1.13058\n",
            "Training Progress: \tEpoch 23 [320/1692 (18.87%)]\t\tLoss: 0.97786\n",
            "Training Progress: \tEpoch 23 [640/1692 (37.74%)]\t\tLoss: 0.94007\n",
            "Training Progress: \tEpoch 23 [960/1692 (56.60%)]\t\tLoss: 0.88941\n",
            "Training Progress: \tEpoch 23 [1280/1692 (75.47%)]\t\tLoss: 1.23453\n",
            "Training Progress: \tEpoch 23 [1600/1692 (94.34%)]\t\tLoss: 0.90501\n",
            "\tTrain loss: 0.03479, Accuracy: 859/1692 (50.77%)\n",
            "\tValidation loss: 0.00310, Accuracy: 189/423 (44.68%)\n",
            "\tTest loss: 0.00277, Accuracy: 220/443 (49.66%)\n",
            "\n",
            "Training Progress: \tEpoch 24 [0/1692 (0.00%)]\t\tLoss: 1.23782\n",
            "Training Progress: \tEpoch 24 [320/1692 (18.87%)]\t\tLoss: 1.00524\n",
            "Training Progress: \tEpoch 24 [640/1692 (37.74%)]\t\tLoss: 0.89078\n",
            "Training Progress: \tEpoch 24 [960/1692 (56.60%)]\t\tLoss: 0.82532\n",
            "Training Progress: \tEpoch 24 [1280/1692 (75.47%)]\t\tLoss: 1.19356\n",
            "Training Progress: \tEpoch 24 [1600/1692 (94.34%)]\t\tLoss: 0.96040\n",
            "\tTrain loss: 0.03222, Accuracy: 934/1692 (55.20%)\n",
            "\tValidation loss: 0.00297, Accuracy: 200/423 (47.28%)\n",
            "\tTest loss: 0.00270, Accuracy: 216/443 (48.76%)\n",
            "\n",
            "Training Progress: \tEpoch 25 [0/1692 (0.00%)]\t\tLoss: 1.09519\n",
            "Training Progress: \tEpoch 25 [320/1692 (18.87%)]\t\tLoss: 0.98705\n",
            "Training Progress: \tEpoch 25 [640/1692 (37.74%)]\t\tLoss: 0.98992\n",
            "Training Progress: \tEpoch 25 [960/1692 (56.60%)]\t\tLoss: 1.01245\n",
            "Training Progress: \tEpoch 25 [1280/1692 (75.47%)]\t\tLoss: 0.96044\n",
            "Training Progress: \tEpoch 25 [1600/1692 (94.34%)]\t\tLoss: 0.89670\n",
            "\tTrain loss: 0.03047, Accuracy: 993/1692 (58.69%)\n",
            "\tValidation loss: 0.00285, Accuracy: 206/423 (48.70%)\n",
            "\tTest loss: 0.00262, Accuracy: 226/443 (51.02%)\n",
            "\n",
            "Training Progress: \tEpoch 26 [0/1692 (0.00%)]\t\tLoss: 1.06584\n",
            "Training Progress: \tEpoch 26 [320/1692 (18.87%)]\t\tLoss: 0.99404\n",
            "Training Progress: \tEpoch 26 [640/1692 (37.74%)]\t\tLoss: 0.99011\n",
            "Training Progress: \tEpoch 26 [960/1692 (56.60%)]\t\tLoss: 0.89246\n",
            "Training Progress: \tEpoch 26 [1280/1692 (75.47%)]\t\tLoss: 1.17376\n",
            "Training Progress: \tEpoch 26 [1600/1692 (94.34%)]\t\tLoss: 1.03101\n",
            "\tTrain loss: 0.03022, Accuracy: 990/1692 (58.51%)\n",
            "\tValidation loss: 0.00279, Accuracy: 207/423 (48.94%)\n",
            "\tTest loss: 0.00256, Accuracy: 226/443 (51.02%)\n",
            "\n",
            "Training Progress: \tEpoch 27 [0/1692 (0.00%)]\t\tLoss: 1.08168\n",
            "Training Progress: \tEpoch 27 [320/1692 (18.87%)]\t\tLoss: 1.02602\n",
            "Training Progress: \tEpoch 27 [640/1692 (37.74%)]\t\tLoss: 1.02809\n",
            "Training Progress: \tEpoch 27 [960/1692 (56.60%)]\t\tLoss: 0.90307\n",
            "Training Progress: \tEpoch 27 [1280/1692 (75.47%)]\t\tLoss: 1.09855\n",
            "Training Progress: \tEpoch 27 [1600/1692 (94.34%)]\t\tLoss: 0.93554\n",
            "\tTrain loss: 0.03025, Accuracy: 975/1692 (57.62%)\n",
            "\tValidation loss: 0.00292, Accuracy: 201/423 (47.52%)\n",
            "\tTest loss: 0.00269, Accuracy: 222/443 (50.11%)\n",
            "\n",
            "Training Progress: \tEpoch 28 [0/1692 (0.00%)]\t\tLoss: 1.08863\n",
            "Training Progress: \tEpoch 28 [320/1692 (18.87%)]\t\tLoss: 1.12132\n",
            "Training Progress: \tEpoch 28 [640/1692 (37.74%)]\t\tLoss: 0.98784\n",
            "Training Progress: \tEpoch 28 [960/1692 (56.60%)]\t\tLoss: 0.84083\n",
            "Training Progress: \tEpoch 28 [1280/1692 (75.47%)]\t\tLoss: 1.14974\n",
            "Training Progress: \tEpoch 28 [1600/1692 (94.34%)]\t\tLoss: 0.80876\n",
            "\tTrain loss: 0.03129, Accuracy: 986/1692 (58.27%)\n",
            "\tValidation loss: 0.00299, Accuracy: 197/423 (46.57%)\n",
            "\tTest loss: 0.00272, Accuracy: 217/443 (48.98%)\n",
            "\n",
            "Training Progress: \tEpoch 29 [0/1692 (0.00%)]\t\tLoss: 1.17347\n",
            "Training Progress: \tEpoch 29 [320/1692 (18.87%)]\t\tLoss: 0.82347\n",
            "Training Progress: \tEpoch 29 [640/1692 (37.74%)]\t\tLoss: 0.93078\n",
            "Training Progress: \tEpoch 29 [960/1692 (56.60%)]\t\tLoss: 1.05451\n",
            "Training Progress: \tEpoch 29 [1280/1692 (75.47%)]\t\tLoss: 1.18999\n",
            "Training Progress: \tEpoch 29 [1600/1692 (94.34%)]\t\tLoss: 0.83798\n",
            "\tTrain loss: 0.02983, Accuracy: 1002/1692 (59.22%)\n",
            "\tValidation loss: 0.00292, Accuracy: 207/423 (48.94%)\n",
            "\tTest loss: 0.00264, Accuracy: 220/443 (49.66%)\n",
            "\n",
            "Training Progress: \tEpoch 30 [0/1692 (0.00%)]\t\tLoss: 0.93528\n",
            "Training Progress: \tEpoch 30 [320/1692 (18.87%)]\t\tLoss: 0.91374\n",
            "Training Progress: \tEpoch 30 [640/1692 (37.74%)]\t\tLoss: 0.91487\n",
            "Training Progress: \tEpoch 30 [960/1692 (56.60%)]\t\tLoss: 0.62837\n",
            "Training Progress: \tEpoch 30 [1280/1692 (75.47%)]\t\tLoss: 1.21022\n",
            "Training Progress: \tEpoch 30 [1600/1692 (94.34%)]\t\tLoss: 0.73026\n",
            "\tTrain loss: 0.03223, Accuracy: 964/1692 (56.97%)\n",
            "\tValidation loss: 0.00309, Accuracy: 195/423 (46.10%)\n",
            "\tTest loss: 0.00280, Accuracy: 214/443 (48.31%)\n",
            "\n",
            "Training Progress: \tEpoch 31 [0/1692 (0.00%)]\t\tLoss: 1.12744\n",
            "Training Progress: \tEpoch 31 [320/1692 (18.87%)]\t\tLoss: 0.89388\n",
            "Training Progress: \tEpoch 31 [640/1692 (37.74%)]\t\tLoss: 0.93700\n",
            "Training Progress: \tEpoch 31 [960/1692 (56.60%)]\t\tLoss: 0.90933\n",
            "Training Progress: \tEpoch 31 [1280/1692 (75.47%)]\t\tLoss: 1.05277\n",
            "Training Progress: \tEpoch 31 [1600/1692 (94.34%)]\t\tLoss: 0.79074\n",
            "\tTrain loss: 0.02739, Accuracy: 1072/1692 (63.36%)\n",
            "\tValidation loss: 0.00280, Accuracy: 212/423 (50.12%)\n",
            "\tTest loss: 0.00264, Accuracy: 230/443 (51.92%)\n",
            "\n",
            "Training Progress: \tEpoch 32 [0/1692 (0.00%)]\t\tLoss: 1.02480\n",
            "Training Progress: \tEpoch 32 [320/1692 (18.87%)]\t\tLoss: 0.96770\n",
            "Training Progress: \tEpoch 32 [640/1692 (37.74%)]\t\tLoss: 1.09327\n",
            "Training Progress: \tEpoch 32 [960/1692 (56.60%)]\t\tLoss: 0.77432\n",
            "Training Progress: \tEpoch 32 [1280/1692 (75.47%)]\t\tLoss: 1.00570\n",
            "Training Progress: \tEpoch 32 [1600/1692 (94.34%)]\t\tLoss: 0.88252\n",
            "\tTrain loss: 0.02905, Accuracy: 1025/1692 (60.58%)\n",
            "\tValidation loss: 0.00296, Accuracy: 197/423 (46.57%)\n",
            "\tTest loss: 0.00273, Accuracy: 220/443 (49.66%)\n",
            "\n",
            "Training Progress: \tEpoch 33 [0/1692 (0.00%)]\t\tLoss: 1.12414\n",
            "Training Progress: \tEpoch 33 [320/1692 (18.87%)]\t\tLoss: 1.02048\n",
            "Training Progress: \tEpoch 33 [640/1692 (37.74%)]\t\tLoss: 0.87224\n",
            "Training Progress: \tEpoch 33 [960/1692 (56.60%)]\t\tLoss: 0.84224\n",
            "Training Progress: \tEpoch 33 [1280/1692 (75.47%)]\t\tLoss: 1.01396\n",
            "Training Progress: \tEpoch 33 [1600/1692 (94.34%)]\t\tLoss: 0.88126\n",
            "\tTrain loss: 0.02672, Accuracy: 1077/1692 (63.65%)\n",
            "\tValidation loss: 0.00285, Accuracy: 212/423 (50.12%)\n",
            "\tTest loss: 0.00263, Accuracy: 221/443 (49.89%)\n",
            "\n",
            "Training Progress: \tEpoch 34 [0/1692 (0.00%)]\t\tLoss: 1.11360\n",
            "Training Progress: \tEpoch 34 [320/1692 (18.87%)]\t\tLoss: 1.04730\n",
            "Training Progress: \tEpoch 34 [640/1692 (37.74%)]\t\tLoss: 0.95194\n",
            "Training Progress: \tEpoch 34 [960/1692 (56.60%)]\t\tLoss: 0.63491\n",
            "Training Progress: \tEpoch 34 [1280/1692 (75.47%)]\t\tLoss: 1.04372\n",
            "Training Progress: \tEpoch 34 [1600/1692 (94.34%)]\t\tLoss: 0.89326\n",
            "\tTrain loss: 0.02608, Accuracy: 1081/1692 (63.89%)\n",
            "\tValidation loss: 0.00279, Accuracy: 219/423 (51.77%)\n",
            "\tTest loss: 0.00260, Accuracy: 223/443 (50.34%)\n",
            "\n",
            "Training Progress: \tEpoch 35 [0/1692 (0.00%)]\t\tLoss: 1.00695\n",
            "Training Progress: \tEpoch 35 [320/1692 (18.87%)]\t\tLoss: 0.96858\n",
            "Training Progress: \tEpoch 35 [640/1692 (37.74%)]\t\tLoss: 0.81293\n",
            "Training Progress: \tEpoch 35 [960/1692 (56.60%)]\t\tLoss: 0.66865\n",
            "Training Progress: \tEpoch 35 [1280/1692 (75.47%)]\t\tLoss: 1.04056\n",
            "Training Progress: \tEpoch 35 [1600/1692 (94.34%)]\t\tLoss: 0.73059\n",
            "\tTrain loss: 0.02526, Accuracy: 1126/1692 (66.55%)\n",
            "\tValidation loss: 0.00272, Accuracy: 229/423 (54.14%)\n",
            "\tTest loss: 0.00259, Accuracy: 230/443 (51.92%)\n",
            "\n",
            "Training Progress: \tEpoch 36 [0/1692 (0.00%)]\t\tLoss: 1.15897\n",
            "Training Progress: \tEpoch 36 [320/1692 (18.87%)]\t\tLoss: 0.94647\n",
            "Training Progress: \tEpoch 36 [640/1692 (37.74%)]\t\tLoss: 0.78432\n",
            "Training Progress: \tEpoch 36 [960/1692 (56.60%)]\t\tLoss: 0.88594\n",
            "Training Progress: \tEpoch 36 [1280/1692 (75.47%)]\t\tLoss: 1.00989\n",
            "Training Progress: \tEpoch 36 [1600/1692 (94.34%)]\t\tLoss: 0.59749\n",
            "\tTrain loss: 0.02590, Accuracy: 1116/1692 (65.96%)\n",
            "\tValidation loss: 0.00286, Accuracy: 210/423 (49.65%)\n",
            "\tTest loss: 0.00271, Accuracy: 229/443 (51.69%)\n",
            "\n",
            "Training Progress: \tEpoch 37 [0/1692 (0.00%)]\t\tLoss: 1.07027\n",
            "Training Progress: \tEpoch 37 [320/1692 (18.87%)]\t\tLoss: 0.97120\n",
            "Training Progress: \tEpoch 37 [640/1692 (37.74%)]\t\tLoss: 1.01855\n",
            "Training Progress: \tEpoch 37 [960/1692 (56.60%)]\t\tLoss: 1.02152\n",
            "Training Progress: \tEpoch 37 [1280/1692 (75.47%)]\t\tLoss: 0.80230\n",
            "Training Progress: \tEpoch 37 [1600/1692 (94.34%)]\t\tLoss: 0.88560\n",
            "\tTrain loss: 0.02315, Accuracy: 1167/1692 (68.97%)\n",
            "\tValidation loss: 0.00281, Accuracy: 220/423 (52.01%)\n",
            "\tTest loss: 0.00265, Accuracy: 225/443 (50.79%)\n",
            "\n",
            "Training Progress: \tEpoch 38 [0/1692 (0.00%)]\t\tLoss: 0.90568\n",
            "Training Progress: \tEpoch 38 [320/1692 (18.87%)]\t\tLoss: 1.02209\n",
            "Training Progress: \tEpoch 38 [640/1692 (37.74%)]\t\tLoss: 0.79826\n",
            "Training Progress: \tEpoch 38 [960/1692 (56.60%)]\t\tLoss: 0.89785\n",
            "Training Progress: \tEpoch 38 [1280/1692 (75.47%)]\t\tLoss: 0.89469\n",
            "Training Progress: \tEpoch 38 [1600/1692 (94.34%)]\t\tLoss: 0.74369\n",
            "\tTrain loss: 0.02531, Accuracy: 1119/1692 (66.13%)\n",
            "\tValidation loss: 0.00303, Accuracy: 212/423 (50.12%)\n",
            "\tTest loss: 0.00276, Accuracy: 217/443 (48.98%)\n",
            "\n",
            "Training Progress: \tEpoch 39 [0/1692 (0.00%)]\t\tLoss: 0.95339\n",
            "Training Progress: \tEpoch 39 [320/1692 (18.87%)]\t\tLoss: 1.04779\n",
            "Training Progress: \tEpoch 39 [640/1692 (37.74%)]\t\tLoss: 1.00477\n",
            "Training Progress: \tEpoch 39 [960/1692 (56.60%)]\t\tLoss: 0.81031\n",
            "Training Progress: \tEpoch 39 [1280/1692 (75.47%)]\t\tLoss: 1.17326\n",
            "Training Progress: \tEpoch 39 [1600/1692 (94.34%)]\t\tLoss: 0.62383\n",
            "\tTrain loss: 0.02474, Accuracy: 1141/1692 (67.43%)\n",
            "\tValidation loss: 0.00292, Accuracy: 209/423 (49.41%)\n",
            "\tTest loss: 0.00273, Accuracy: 228/443 (51.47%)\n",
            "\n",
            "Training Progress: \tEpoch 40 [0/1692 (0.00%)]\t\tLoss: 0.94685\n",
            "Training Progress: \tEpoch 40 [320/1692 (18.87%)]\t\tLoss: 0.68366\n",
            "Training Progress: \tEpoch 40 [640/1692 (37.74%)]\t\tLoss: 1.08395\n",
            "Training Progress: \tEpoch 40 [960/1692 (56.60%)]\t\tLoss: 0.72714\n",
            "Training Progress: \tEpoch 40 [1280/1692 (75.47%)]\t\tLoss: 1.05745\n",
            "Training Progress: \tEpoch 40 [1600/1692 (94.34%)]\t\tLoss: 0.77018\n",
            "\tTrain loss: 0.02389, Accuracy: 1165/1692 (68.85%)\n",
            "\tValidation loss: 0.00281, Accuracy: 222/423 (52.48%)\n",
            "\tTest loss: 0.00261, Accuracy: 218/443 (49.21%)\n",
            "\n",
            "Training Progress: \tEpoch 41 [0/1692 (0.00%)]\t\tLoss: 1.00451\n",
            "Training Progress: \tEpoch 41 [320/1692 (18.87%)]\t\tLoss: 0.81389\n",
            "Training Progress: \tEpoch 41 [640/1692 (37.74%)]\t\tLoss: 1.10799\n",
            "Training Progress: \tEpoch 41 [960/1692 (56.60%)]\t\tLoss: 0.62919\n",
            "Training Progress: \tEpoch 41 [1280/1692 (75.47%)]\t\tLoss: 1.12060\n",
            "Training Progress: \tEpoch 41 [1600/1692 (94.34%)]\t\tLoss: 0.70075\n",
            "\tTrain loss: 0.02383, Accuracy: 1136/1692 (67.14%)\n",
            "\tValidation loss: 0.00299, Accuracy: 211/423 (49.88%)\n",
            "\tTest loss: 0.00274, Accuracy: 224/443 (50.56%)\n",
            "\n",
            "Training Progress: \tEpoch 42 [0/1692 (0.00%)]\t\tLoss: 0.94671\n",
            "Training Progress: \tEpoch 42 [320/1692 (18.87%)]\t\tLoss: 0.76881\n",
            "Training Progress: \tEpoch 42 [640/1692 (37.74%)]\t\tLoss: 0.83313\n",
            "Training Progress: \tEpoch 42 [960/1692 (56.60%)]\t\tLoss: 0.87519\n",
            "Training Progress: \tEpoch 42 [1280/1692 (75.47%)]\t\tLoss: 1.04874\n",
            "Training Progress: \tEpoch 42 [1600/1692 (94.34%)]\t\tLoss: 0.59501\n",
            "\tTrain loss: 0.02370, Accuracy: 1158/1692 (68.44%)\n",
            "\tValidation loss: 0.00299, Accuracy: 213/423 (50.35%)\n",
            "\tTest loss: 0.00274, Accuracy: 227/443 (51.24%)\n",
            "\n",
            "Training Progress: \tEpoch 43 [0/1692 (0.00%)]\t\tLoss: 0.90693\n",
            "Training Progress: \tEpoch 43 [320/1692 (18.87%)]\t\tLoss: 0.85883\n",
            "Training Progress: \tEpoch 43 [640/1692 (37.74%)]\t\tLoss: 0.78336\n",
            "Training Progress: \tEpoch 43 [960/1692 (56.60%)]\t\tLoss: 0.71684\n",
            "Training Progress: \tEpoch 43 [1280/1692 (75.47%)]\t\tLoss: 0.94971\n",
            "Training Progress: \tEpoch 43 [1600/1692 (94.34%)]\t\tLoss: 0.84948\n",
            "\tTrain loss: 0.02614, Accuracy: 1094/1692 (64.66%)\n",
            "\tValidation loss: 0.00320, Accuracy: 193/423 (45.63%)\n",
            "\tTest loss: 0.00297, Accuracy: 227/443 (51.24%)\n",
            "\n",
            "Training Progress: \tEpoch 44 [0/1692 (0.00%)]\t\tLoss: 1.04332\n",
            "Training Progress: \tEpoch 44 [320/1692 (18.87%)]\t\tLoss: 0.94506\n",
            "Training Progress: \tEpoch 44 [640/1692 (37.74%)]\t\tLoss: 1.03510\n",
            "Training Progress: \tEpoch 44 [960/1692 (56.60%)]\t\tLoss: 0.68273\n",
            "Training Progress: \tEpoch 44 [1280/1692 (75.47%)]\t\tLoss: 1.00167\n",
            "Training Progress: \tEpoch 44 [1600/1692 (94.34%)]\t\tLoss: 0.65964\n",
            "\tTrain loss: 0.02346, Accuracy: 1186/1692 (70.09%)\n",
            "\tValidation loss: 0.00295, Accuracy: 209/423 (49.41%)\n",
            "\tTest loss: 0.00279, Accuracy: 224/443 (50.56%)\n",
            "\n",
            "Training Progress: \tEpoch 45 [0/1692 (0.00%)]\t\tLoss: 0.84873\n",
            "Training Progress: \tEpoch 45 [320/1692 (18.87%)]\t\tLoss: 0.76022\n",
            "Training Progress: \tEpoch 45 [640/1692 (37.74%)]\t\tLoss: 0.78925\n",
            "Training Progress: \tEpoch 45 [960/1692 (56.60%)]\t\tLoss: 0.73101\n",
            "Training Progress: \tEpoch 45 [1280/1692 (75.47%)]\t\tLoss: 0.99294\n",
            "Training Progress: \tEpoch 45 [1600/1692 (94.34%)]\t\tLoss: 0.61758\n",
            "\tTrain loss: 0.02333, Accuracy: 1177/1692 (69.56%)\n",
            "\tValidation loss: 0.00307, Accuracy: 210/423 (49.65%)\n",
            "\tTest loss: 0.00282, Accuracy: 225/443 (50.79%)\n",
            "\n",
            "Training Progress: \tEpoch 46 [0/1692 (0.00%)]\t\tLoss: 1.05259\n",
            "Training Progress: \tEpoch 46 [320/1692 (18.87%)]\t\tLoss: 0.75977\n",
            "Training Progress: \tEpoch 46 [640/1692 (37.74%)]\t\tLoss: 0.81032\n",
            "Training Progress: \tEpoch 46 [960/1692 (56.60%)]\t\tLoss: 0.69697\n",
            "Training Progress: \tEpoch 46 [1280/1692 (75.47%)]\t\tLoss: 0.95834\n",
            "Training Progress: \tEpoch 46 [1600/1692 (94.34%)]\t\tLoss: 0.69647\n",
            "\tTrain loss: 0.02322, Accuracy: 1170/1692 (69.15%)\n",
            "\tValidation loss: 0.00321, Accuracy: 204/423 (48.23%)\n",
            "\tTest loss: 0.00285, Accuracy: 233/443 (52.60%)\n",
            "\n",
            "Training Progress: \tEpoch 47 [0/1692 (0.00%)]\t\tLoss: 1.23604\n",
            "Training Progress: \tEpoch 47 [320/1692 (18.87%)]\t\tLoss: 0.70352\n",
            "Training Progress: \tEpoch 47 [640/1692 (37.74%)]\t\tLoss: 0.60055\n",
            "Training Progress: \tEpoch 47 [960/1692 (56.60%)]\t\tLoss: 0.59032\n",
            "Training Progress: \tEpoch 47 [1280/1692 (75.47%)]\t\tLoss: 0.69133\n",
            "Training Progress: \tEpoch 47 [1600/1692 (94.34%)]\t\tLoss: 0.82183\n",
            "\tTrain loss: 0.02319, Accuracy: 1170/1692 (69.15%)\n",
            "\tValidation loss: 0.00311, Accuracy: 203/423 (47.99%)\n",
            "\tTest loss: 0.00277, Accuracy: 236/443 (53.27%)\n",
            "\n",
            "Training Progress: \tEpoch 48 [0/1692 (0.00%)]\t\tLoss: 1.09257\n",
            "Training Progress: \tEpoch 48 [320/1692 (18.87%)]\t\tLoss: 0.89948\n",
            "Training Progress: \tEpoch 48 [640/1692 (37.74%)]\t\tLoss: 0.83161\n",
            "Training Progress: \tEpoch 48 [960/1692 (56.60%)]\t\tLoss: 0.58096\n",
            "Training Progress: \tEpoch 48 [1280/1692 (75.47%)]\t\tLoss: 1.08988\n",
            "Training Progress: \tEpoch 48 [1600/1692 (94.34%)]\t\tLoss: 0.80612\n",
            "\tTrain loss: 0.02313, Accuracy: 1194/1692 (70.57%)\n",
            "\tValidation loss: 0.00307, Accuracy: 211/423 (49.88%)\n",
            "\tTest loss: 0.00289, Accuracy: 239/443 (53.95%)\n",
            "\n",
            "Training Progress: \tEpoch 49 [0/1692 (0.00%)]\t\tLoss: 0.94044\n",
            "Training Progress: \tEpoch 49 [320/1692 (18.87%)]\t\tLoss: 0.73175\n",
            "Training Progress: \tEpoch 49 [640/1692 (37.74%)]\t\tLoss: 0.83515\n",
            "Training Progress: \tEpoch 49 [960/1692 (56.60%)]\t\tLoss: 0.66126\n",
            "Training Progress: \tEpoch 49 [1280/1692 (75.47%)]\t\tLoss: 0.84124\n",
            "Training Progress: \tEpoch 49 [1600/1692 (94.34%)]\t\tLoss: 0.51113\n",
            "\tTrain loss: 0.02188, Accuracy: 1215/1692 (71.81%)\n",
            "\tValidation loss: 0.00305, Accuracy: 208/423 (49.17%)\n",
            "\tTest loss: 0.00290, Accuracy: 221/443 (49.89%)\n",
            "\n",
            "Training Progress: \tEpoch 50 [0/1692 (0.00%)]\t\tLoss: 0.82350\n",
            "Training Progress: \tEpoch 50 [320/1692 (18.87%)]\t\tLoss: 0.89668\n",
            "Training Progress: \tEpoch 50 [640/1692 (37.74%)]\t\tLoss: 0.80156\n",
            "Training Progress: \tEpoch 50 [960/1692 (56.60%)]\t\tLoss: 0.59704\n",
            "Training Progress: \tEpoch 50 [1280/1692 (75.47%)]\t\tLoss: 0.97955\n",
            "Training Progress: \tEpoch 50 [1600/1692 (94.34%)]\t\tLoss: 0.49551\n",
            "\tTrain loss: 0.01956, Accuracy: 1256/1692 (74.23%)\n",
            "\tValidation loss: 0.00292, Accuracy: 225/423 (53.19%)\n",
            "\tTest loss: 0.00291, Accuracy: 226/443 (51.02%)\n",
            "\n",
            "Training Progress: \tEpoch 51 [0/1692 (0.00%)]\t\tLoss: 0.97242\n",
            "Training Progress: \tEpoch 51 [320/1692 (18.87%)]\t\tLoss: 0.76215\n",
            "Training Progress: \tEpoch 51 [640/1692 (37.74%)]\t\tLoss: 0.76921\n",
            "Training Progress: \tEpoch 51 [960/1692 (56.60%)]\t\tLoss: 0.69059\n",
            "Training Progress: \tEpoch 51 [1280/1692 (75.47%)]\t\tLoss: 1.04301\n",
            "Training Progress: \tEpoch 51 [1600/1692 (94.34%)]\t\tLoss: 0.69151\n",
            "\tTrain loss: 0.02083, Accuracy: 1228/1692 (72.58%)\n",
            "\tValidation loss: 0.00310, Accuracy: 214/423 (50.59%)\n",
            "\tTest loss: 0.00287, Accuracy: 230/443 (51.92%)\n",
            "\n",
            "Training Progress: \tEpoch 52 [0/1692 (0.00%)]\t\tLoss: 0.92665\n",
            "Training Progress: \tEpoch 52 [320/1692 (18.87%)]\t\tLoss: 0.74041\n",
            "Training Progress: \tEpoch 52 [640/1692 (37.74%)]\t\tLoss: 0.69483\n",
            "Training Progress: \tEpoch 52 [960/1692 (56.60%)]\t\tLoss: 0.46827\n",
            "Training Progress: \tEpoch 52 [1280/1692 (75.47%)]\t\tLoss: 0.89350\n",
            "Training Progress: \tEpoch 52 [1600/1692 (94.34%)]\t\tLoss: 1.07821\n",
            "\tTrain loss: 0.02043, Accuracy: 1248/1692 (73.76%)\n",
            "\tValidation loss: 0.00314, Accuracy: 205/423 (48.46%)\n",
            "\tTest loss: 0.00300, Accuracy: 217/443 (48.98%)\n",
            "\n",
            "Training Progress: \tEpoch 53 [0/1692 (0.00%)]\t\tLoss: 0.81918\n",
            "Training Progress: \tEpoch 53 [320/1692 (18.87%)]\t\tLoss: 0.64803\n",
            "Training Progress: \tEpoch 53 [640/1692 (37.74%)]\t\tLoss: 0.77214\n",
            "Training Progress: \tEpoch 53 [960/1692 (56.60%)]\t\tLoss: 0.61952\n",
            "Training Progress: \tEpoch 53 [1280/1692 (75.47%)]\t\tLoss: 0.88661\n",
            "Training Progress: \tEpoch 53 [1600/1692 (94.34%)]\t\tLoss: 0.59281\n",
            "\tTrain loss: 0.01928, Accuracy: 1275/1692 (75.35%)\n",
            "\tValidation loss: 0.00302, Accuracy: 213/423 (50.35%)\n",
            "\tTest loss: 0.00284, Accuracy: 223/443 (50.34%)\n",
            "\n",
            "Training Progress: \tEpoch 54 [0/1692 (0.00%)]\t\tLoss: 0.85393\n",
            "Training Progress: \tEpoch 54 [320/1692 (18.87%)]\t\tLoss: 0.66931\n",
            "Training Progress: \tEpoch 54 [640/1692 (37.74%)]\t\tLoss: 1.09118\n",
            "Training Progress: \tEpoch 54 [960/1692 (56.60%)]\t\tLoss: 0.52890\n",
            "Training Progress: \tEpoch 54 [1280/1692 (75.47%)]\t\tLoss: 1.00787\n",
            "Training Progress: \tEpoch 54 [1600/1692 (94.34%)]\t\tLoss: 0.57466\n",
            "\tTrain loss: 0.02110, Accuracy: 1218/1692 (71.99%)\n",
            "\tValidation loss: 0.00308, Accuracy: 199/423 (47.04%)\n",
            "\tTest loss: 0.00297, Accuracy: 216/443 (48.76%)\n",
            "\n",
            "Training Progress: \tEpoch 55 [0/1692 (0.00%)]\t\tLoss: 0.93888\n",
            "Training Progress: \tEpoch 55 [320/1692 (18.87%)]\t\tLoss: 0.68007\n",
            "Training Progress: \tEpoch 55 [640/1692 (37.74%)]\t\tLoss: 0.77598\n",
            "Training Progress: \tEpoch 55 [960/1692 (56.60%)]\t\tLoss: 0.69122\n",
            "Training Progress: \tEpoch 55 [1280/1692 (75.47%)]\t\tLoss: 0.99134\n",
            "Training Progress: \tEpoch 55 [1600/1692 (94.34%)]\t\tLoss: 0.60687\n",
            "\tTrain loss: 0.01878, Accuracy: 1292/1692 (76.36%)\n",
            "\tValidation loss: 0.00305, Accuracy: 214/423 (50.59%)\n",
            "\tTest loss: 0.00286, Accuracy: 227/443 (51.24%)\n",
            "\n",
            "Training Progress: \tEpoch 56 [0/1692 (0.00%)]\t\tLoss: 0.96587\n",
            "Training Progress: \tEpoch 56 [320/1692 (18.87%)]\t\tLoss: 0.71472\n",
            "Training Progress: \tEpoch 56 [640/1692 (37.74%)]\t\tLoss: 0.92012\n",
            "Training Progress: \tEpoch 56 [960/1692 (56.60%)]\t\tLoss: 0.53208\n",
            "Training Progress: \tEpoch 56 [1280/1692 (75.47%)]\t\tLoss: 1.07493\n",
            "Training Progress: \tEpoch 56 [1600/1692 (94.34%)]\t\tLoss: 0.63873\n",
            "\tTrain loss: 0.01965, Accuracy: 1263/1692 (74.65%)\n",
            "\tValidation loss: 0.00309, Accuracy: 203/423 (47.99%)\n",
            "\tTest loss: 0.00291, Accuracy: 213/443 (48.08%)\n",
            "\n",
            "Training Progress: \tEpoch 57 [0/1692 (0.00%)]\t\tLoss: 0.83050\n",
            "Training Progress: \tEpoch 57 [320/1692 (18.87%)]\t\tLoss: 0.56658\n",
            "Training Progress: \tEpoch 57 [640/1692 (37.74%)]\t\tLoss: 0.74263\n",
            "Training Progress: \tEpoch 57 [960/1692 (56.60%)]\t\tLoss: 0.69477\n",
            "Training Progress: \tEpoch 57 [1280/1692 (75.47%)]\t\tLoss: 0.94949\n",
            "Training Progress: \tEpoch 57 [1600/1692 (94.34%)]\t\tLoss: 0.71638\n",
            "\tTrain loss: 0.01790, Accuracy: 1309/1692 (77.36%)\n",
            "\tValidation loss: 0.00306, Accuracy: 220/423 (52.01%)\n",
            "\tTest loss: 0.00295, Accuracy: 223/443 (50.34%)\n",
            "\n",
            "Training Progress: \tEpoch 58 [0/1692 (0.00%)]\t\tLoss: 0.98125\n",
            "Training Progress: \tEpoch 58 [320/1692 (18.87%)]\t\tLoss: 0.67638\n",
            "Training Progress: \tEpoch 58 [640/1692 (37.74%)]\t\tLoss: 0.58701\n",
            "Training Progress: \tEpoch 58 [960/1692 (56.60%)]\t\tLoss: 0.52555\n",
            "Training Progress: \tEpoch 58 [1280/1692 (75.47%)]\t\tLoss: 0.90173\n",
            "Training Progress: \tEpoch 58 [1600/1692 (94.34%)]\t\tLoss: 0.57842\n",
            "\tTrain loss: 0.01944, Accuracy: 1276/1692 (75.41%)\n",
            "\tValidation loss: 0.00330, Accuracy: 203/423 (47.99%)\n",
            "\tTest loss: 0.00306, Accuracy: 220/443 (49.66%)\n",
            "\n",
            "Training Progress: \tEpoch 59 [0/1692 (0.00%)]\t\tLoss: 0.80902\n",
            "Training Progress: \tEpoch 59 [320/1692 (18.87%)]\t\tLoss: 0.87137\n",
            "Training Progress: \tEpoch 59 [640/1692 (37.74%)]\t\tLoss: 0.58222\n",
            "Training Progress: \tEpoch 59 [960/1692 (56.60%)]\t\tLoss: 0.57028\n",
            "Training Progress: \tEpoch 59 [1280/1692 (75.47%)]\t\tLoss: 0.97984\n",
            "Training Progress: \tEpoch 59 [1600/1692 (94.34%)]\t\tLoss: 0.50821\n",
            "\tTrain loss: 0.01783, Accuracy: 1294/1692 (76.48%)\n",
            "\tValidation loss: 0.00320, Accuracy: 202/423 (47.75%)\n",
            "\tTest loss: 0.00303, Accuracy: 218/443 (49.21%)\n",
            "\n",
            "Training Progress: \tEpoch 60 [0/1692 (0.00%)]\t\tLoss: 1.01001\n",
            "Training Progress: \tEpoch 60 [320/1692 (18.87%)]\t\tLoss: 0.66364\n",
            "Training Progress: \tEpoch 60 [640/1692 (37.74%)]\t\tLoss: 0.75483\n",
            "Training Progress: \tEpoch 60 [960/1692 (56.60%)]\t\tLoss: 0.55542\n",
            "Training Progress: \tEpoch 60 [1280/1692 (75.47%)]\t\tLoss: 0.78279\n",
            "Training Progress: \tEpoch 60 [1600/1692 (94.34%)]\t\tLoss: 0.46510\n",
            "\tTrain loss: 0.01858, Accuracy: 1290/1692 (76.24%)\n",
            "\tValidation loss: 0.00329, Accuracy: 207/423 (48.94%)\n",
            "\tTest loss: 0.00310, Accuracy: 214/443 (48.31%)\n",
            "\n",
            "Training Progress: \tEpoch 61 [0/1692 (0.00%)]\t\tLoss: 0.88954\n",
            "Training Progress: \tEpoch 61 [320/1692 (18.87%)]\t\tLoss: 0.59655\n",
            "Training Progress: \tEpoch 61 [640/1692 (37.74%)]\t\tLoss: 0.76178\n",
            "Training Progress: \tEpoch 61 [960/1692 (56.60%)]\t\tLoss: 0.65866\n",
            "Training Progress: \tEpoch 61 [1280/1692 (75.47%)]\t\tLoss: 0.84692\n",
            "Training Progress: \tEpoch 61 [1600/1692 (94.34%)]\t\tLoss: 0.49829\n",
            "\tTrain loss: 0.01891, Accuracy: 1284/1692 (75.89%)\n",
            "\tValidation loss: 0.00321, Accuracy: 206/423 (48.70%)\n",
            "\tTest loss: 0.00314, Accuracy: 209/443 (47.18%)\n",
            "\n",
            "Training Progress: \tEpoch 62 [0/1692 (0.00%)]\t\tLoss: 0.87439\n",
            "Training Progress: \tEpoch 62 [320/1692 (18.87%)]\t\tLoss: 0.75579\n",
            "Training Progress: \tEpoch 62 [640/1692 (37.74%)]\t\tLoss: 0.30069\n",
            "Training Progress: \tEpoch 62 [960/1692 (56.60%)]\t\tLoss: 0.60565\n",
            "Training Progress: \tEpoch 62 [1280/1692 (75.47%)]\t\tLoss: 0.84419\n",
            "Training Progress: \tEpoch 62 [1600/1692 (94.34%)]\t\tLoss: 0.58794\n",
            "\tTrain loss: 0.01671, Accuracy: 1347/1692 (79.61%)\n",
            "\tValidation loss: 0.00325, Accuracy: 206/423 (48.70%)\n",
            "\tTest loss: 0.00310, Accuracy: 219/443 (49.44%)\n",
            "\n",
            "Training Progress: \tEpoch 63 [0/1692 (0.00%)]\t\tLoss: 0.78462\n",
            "Training Progress: \tEpoch 63 [320/1692 (18.87%)]\t\tLoss: 0.81379\n",
            "Training Progress: \tEpoch 63 [640/1692 (37.74%)]\t\tLoss: 0.57102\n",
            "Training Progress: \tEpoch 63 [960/1692 (56.60%)]\t\tLoss: 0.70846\n",
            "Training Progress: \tEpoch 63 [1280/1692 (75.47%)]\t\tLoss: 0.66666\n",
            "Training Progress: \tEpoch 63 [1600/1692 (94.34%)]\t\tLoss: 0.60733\n",
            "\tTrain loss: 0.01605, Accuracy: 1391/1692 (82.21%)\n",
            "\tValidation loss: 0.00313, Accuracy: 217/423 (51.30%)\n",
            "\tTest loss: 0.00308, Accuracy: 215/443 (48.53%)\n",
            "\n",
            "Training Progress: \tEpoch 64 [0/1692 (0.00%)]\t\tLoss: 1.07095\n",
            "Training Progress: \tEpoch 64 [320/1692 (18.87%)]\t\tLoss: 0.75138\n",
            "Training Progress: \tEpoch 64 [640/1692 (37.74%)]\t\tLoss: 0.71264\n",
            "Training Progress: \tEpoch 64 [960/1692 (56.60%)]\t\tLoss: 0.45599\n",
            "Training Progress: \tEpoch 64 [1280/1692 (75.47%)]\t\tLoss: 0.88885\n",
            "Training Progress: \tEpoch 64 [1600/1692 (94.34%)]\t\tLoss: 0.66557\n",
            "\tTrain loss: 0.01622, Accuracy: 1351/1692 (79.85%)\n",
            "\tValidation loss: 0.00305, Accuracy: 223/423 (52.72%)\n",
            "\tTest loss: 0.00303, Accuracy: 209/443 (47.18%)\n",
            "\n",
            "Training Progress: \tEpoch 65 [0/1692 (0.00%)]\t\tLoss: 0.71223\n",
            "Training Progress: \tEpoch 65 [320/1692 (18.87%)]\t\tLoss: 0.80969\n",
            "Training Progress: \tEpoch 65 [640/1692 (37.74%)]\t\tLoss: 0.52721\n",
            "Training Progress: \tEpoch 65 [960/1692 (56.60%)]\t\tLoss: 0.51561\n",
            "Training Progress: \tEpoch 65 [1280/1692 (75.47%)]\t\tLoss: 0.73409\n",
            "Training Progress: \tEpoch 65 [1600/1692 (94.34%)]\t\tLoss: 0.55175\n",
            "\tTrain loss: 0.01608, Accuracy: 1337/1692 (79.02%)\n",
            "\tValidation loss: 0.00313, Accuracy: 212/423 (50.12%)\n",
            "\tTest loss: 0.00304, Accuracy: 214/443 (48.31%)\n",
            "\n",
            "Training Progress: \tEpoch 66 [0/1692 (0.00%)]\t\tLoss: 0.79703\n",
            "Training Progress: \tEpoch 66 [320/1692 (18.87%)]\t\tLoss: 0.69465\n",
            "Training Progress: \tEpoch 66 [640/1692 (37.74%)]\t\tLoss: 0.60551\n",
            "Training Progress: \tEpoch 66 [960/1692 (56.60%)]\t\tLoss: 0.41690\n",
            "Training Progress: \tEpoch 66 [1280/1692 (75.47%)]\t\tLoss: 0.83284\n",
            "Training Progress: \tEpoch 66 [1600/1692 (94.34%)]\t\tLoss: 0.45645\n",
            "\tTrain loss: 0.01524, Accuracy: 1362/1692 (80.50%)\n",
            "\tValidation loss: 0.00339, Accuracy: 207/423 (48.94%)\n",
            "\tTest loss: 0.00311, Accuracy: 228/443 (51.47%)\n",
            "\n",
            "Training Progress: \tEpoch 67 [0/1692 (0.00%)]\t\tLoss: 0.84552\n",
            "Training Progress: \tEpoch 67 [320/1692 (18.87%)]\t\tLoss: 0.83291\n",
            "Training Progress: \tEpoch 67 [640/1692 (37.74%)]\t\tLoss: 0.60409\n",
            "Training Progress: \tEpoch 67 [960/1692 (56.60%)]\t\tLoss: 0.58590\n",
            "Training Progress: \tEpoch 67 [1280/1692 (75.47%)]\t\tLoss: 0.70097\n",
            "Training Progress: \tEpoch 67 [1600/1692 (94.34%)]\t\tLoss: 0.53830\n",
            "\tTrain loss: 0.01589, Accuracy: 1357/1692 (80.20%)\n",
            "\tValidation loss: 0.00326, Accuracy: 222/423 (52.48%)\n",
            "\tTest loss: 0.00313, Accuracy: 226/443 (51.02%)\n",
            "\n",
            "Training Progress: \tEpoch 68 [0/1692 (0.00%)]\t\tLoss: 0.75789\n",
            "Training Progress: \tEpoch 68 [320/1692 (18.87%)]\t\tLoss: 0.55747\n",
            "Training Progress: \tEpoch 68 [640/1692 (37.74%)]\t\tLoss: 0.55265\n",
            "Training Progress: \tEpoch 68 [960/1692 (56.60%)]\t\tLoss: 0.31712\n",
            "Training Progress: \tEpoch 68 [1280/1692 (75.47%)]\t\tLoss: 0.81504\n",
            "Training Progress: \tEpoch 68 [1600/1692 (94.34%)]\t\tLoss: 0.47669\n",
            "\tTrain loss: 0.01544, Accuracy: 1371/1692 (81.03%)\n",
            "\tValidation loss: 0.00319, Accuracy: 219/423 (51.77%)\n",
            "\tTest loss: 0.00307, Accuracy: 229/443 (51.69%)\n",
            "\n",
            "Training Progress: \tEpoch 69 [0/1692 (0.00%)]\t\tLoss: 0.69805\n",
            "Training Progress: \tEpoch 69 [320/1692 (18.87%)]\t\tLoss: 0.36645\n",
            "Training Progress: \tEpoch 69 [640/1692 (37.74%)]\t\tLoss: 0.39427\n",
            "Training Progress: \tEpoch 69 [960/1692 (56.60%)]\t\tLoss: 0.56603\n",
            "Training Progress: \tEpoch 69 [1280/1692 (75.47%)]\t\tLoss: 0.93214\n",
            "Training Progress: \tEpoch 69 [1600/1692 (94.34%)]\t\tLoss: 0.55338\n",
            "\tTrain loss: 0.01525, Accuracy: 1376/1692 (81.32%)\n",
            "\tValidation loss: 0.00316, Accuracy: 229/423 (54.14%)\n",
            "\tTest loss: 0.00311, Accuracy: 218/443 (49.21%)\n",
            "\n",
            "Training Progress: \tEpoch 70 [0/1692 (0.00%)]\t\tLoss: 0.87407\n",
            "Training Progress: \tEpoch 70 [320/1692 (18.87%)]\t\tLoss: 0.63075\n",
            "Training Progress: \tEpoch 70 [640/1692 (37.74%)]\t\tLoss: 0.60282\n",
            "Training Progress: \tEpoch 70 [960/1692 (56.60%)]\t\tLoss: 0.42470\n",
            "Training Progress: \tEpoch 70 [1280/1692 (75.47%)]\t\tLoss: 0.64787\n",
            "Training Progress: \tEpoch 70 [1600/1692 (94.34%)]\t\tLoss: 0.56807\n",
            "\tTrain loss: 0.01508, Accuracy: 1371/1692 (81.03%)\n",
            "\tValidation loss: 0.00331, Accuracy: 214/423 (50.59%)\n",
            "\tTest loss: 0.00320, Accuracy: 214/443 (48.31%)\n",
            "\n",
            "Training Progress: \tEpoch 71 [0/1692 (0.00%)]\t\tLoss: 0.85068\n",
            "Training Progress: \tEpoch 71 [320/1692 (18.87%)]\t\tLoss: 0.60660\n",
            "Training Progress: \tEpoch 71 [640/1692 (37.74%)]\t\tLoss: 0.49790\n",
            "Training Progress: \tEpoch 71 [960/1692 (56.60%)]\t\tLoss: 0.49213\n",
            "Training Progress: \tEpoch 71 [1280/1692 (75.47%)]\t\tLoss: 0.57004\n",
            "Training Progress: \tEpoch 71 [1600/1692 (94.34%)]\t\tLoss: 0.53793\n",
            "\tTrain loss: 0.01290, Accuracy: 1424/1692 (84.16%)\n",
            "\tValidation loss: 0.00301, Accuracy: 224/423 (52.96%)\n",
            "\tTest loss: 0.00303, Accuracy: 219/443 (49.44%)\n",
            "\n",
            "Training Progress: \tEpoch 72 [0/1692 (0.00%)]\t\tLoss: 0.67567\n",
            "Training Progress: \tEpoch 72 [320/1692 (18.87%)]\t\tLoss: 0.66686\n",
            "Training Progress: \tEpoch 72 [640/1692 (37.74%)]\t\tLoss: 0.94153\n",
            "Training Progress: \tEpoch 72 [960/1692 (56.60%)]\t\tLoss: 0.50449\n",
            "Training Progress: \tEpoch 72 [1280/1692 (75.47%)]\t\tLoss: 0.73231\n",
            "Training Progress: \tEpoch 72 [1600/1692 (94.34%)]\t\tLoss: 0.46354\n",
            "\tTrain loss: 0.01533, Accuracy: 1354/1692 (80.02%)\n",
            "\tValidation loss: 0.00326, Accuracy: 214/423 (50.59%)\n",
            "\tTest loss: 0.00319, Accuracy: 212/443 (47.86%)\n",
            "\n",
            "Training Progress: \tEpoch 73 [0/1692 (0.00%)]\t\tLoss: 0.73361\n",
            "Training Progress: \tEpoch 73 [320/1692 (18.87%)]\t\tLoss: 0.57673\n",
            "Training Progress: \tEpoch 73 [640/1692 (37.74%)]\t\tLoss: 0.50858\n",
            "Training Progress: \tEpoch 73 [960/1692 (56.60%)]\t\tLoss: 0.62668\n",
            "Training Progress: \tEpoch 73 [1280/1692 (75.47%)]\t\tLoss: 0.67065\n",
            "Training Progress: \tEpoch 73 [1600/1692 (94.34%)]\t\tLoss: 0.73820\n",
            "\tTrain loss: 0.01302, Accuracy: 1431/1692 (84.57%)\n",
            "\tValidation loss: 0.00315, Accuracy: 217/423 (51.30%)\n",
            "\tTest loss: 0.00318, Accuracy: 207/443 (46.73%)\n",
            "\n",
            "Training Progress: \tEpoch 74 [0/1692 (0.00%)]\t\tLoss: 0.71501\n",
            "Training Progress: \tEpoch 74 [320/1692 (18.87%)]\t\tLoss: 0.64418\n",
            "Training Progress: \tEpoch 74 [640/1692 (37.74%)]\t\tLoss: 0.46736\n",
            "Training Progress: \tEpoch 74 [960/1692 (56.60%)]\t\tLoss: 0.60296\n",
            "Training Progress: \tEpoch 74 [1280/1692 (75.47%)]\t\tLoss: 0.78526\n",
            "Training Progress: \tEpoch 74 [1600/1692 (94.34%)]\t\tLoss: 0.50185\n",
            "\tTrain loss: 0.01532, Accuracy: 1382/1692 (81.68%)\n",
            "\tValidation loss: 0.00324, Accuracy: 212/423 (50.12%)\n",
            "\tTest loss: 0.00327, Accuracy: 209/443 (47.18%)\n",
            "\n",
            "Training Progress: \tEpoch 75 [0/1692 (0.00%)]\t\tLoss: 0.66079\n",
            "Training Progress: \tEpoch 75 [320/1692 (18.87%)]\t\tLoss: 0.49068\n",
            "Training Progress: \tEpoch 75 [640/1692 (37.74%)]\t\tLoss: 0.54796\n",
            "Training Progress: \tEpoch 75 [960/1692 (56.60%)]\t\tLoss: 0.66785\n",
            "Training Progress: \tEpoch 75 [1280/1692 (75.47%)]\t\tLoss: 0.65819\n",
            "Training Progress: \tEpoch 75 [1600/1692 (94.34%)]\t\tLoss: 0.65472\n",
            "\tTrain loss: 0.01393, Accuracy: 1425/1692 (84.22%)\n",
            "\tValidation loss: 0.00325, Accuracy: 225/423 (53.19%)\n",
            "\tTest loss: 0.00325, Accuracy: 213/443 (48.08%)\n",
            "\n",
            "Training Progress: \tEpoch 76 [0/1692 (0.00%)]\t\tLoss: 0.91486\n",
            "Training Progress: \tEpoch 76 [320/1692 (18.87%)]\t\tLoss: 0.77445\n",
            "Training Progress: \tEpoch 76 [640/1692 (37.74%)]\t\tLoss: 0.70563\n",
            "Training Progress: \tEpoch 76 [960/1692 (56.60%)]\t\tLoss: 0.48735\n",
            "Training Progress: \tEpoch 76 [1280/1692 (75.47%)]\t\tLoss: 0.66914\n",
            "Training Progress: \tEpoch 76 [1600/1692 (94.34%)]\t\tLoss: 0.25561\n",
            "\tTrain loss: 0.01301, Accuracy: 1428/1692 (84.40%)\n",
            "\tValidation loss: 0.00319, Accuracy: 217/423 (51.30%)\n",
            "\tTest loss: 0.00330, Accuracy: 212/443 (47.86%)\n",
            "\n",
            "Training Progress: \tEpoch 77 [0/1692 (0.00%)]\t\tLoss: 0.62367\n",
            "Training Progress: \tEpoch 77 [320/1692 (18.87%)]\t\tLoss: 0.52143\n",
            "Training Progress: \tEpoch 77 [640/1692 (37.74%)]\t\tLoss: 0.45367\n",
            "Training Progress: \tEpoch 77 [960/1692 (56.60%)]\t\tLoss: 0.33079\n",
            "Training Progress: \tEpoch 77 [1280/1692 (75.47%)]\t\tLoss: 0.64852\n",
            "Training Progress: \tEpoch 77 [1600/1692 (94.34%)]\t\tLoss: 0.34025\n",
            "\tTrain loss: 0.01226, Accuracy: 1441/1692 (85.17%)\n",
            "\tValidation loss: 0.00321, Accuracy: 222/423 (52.48%)\n",
            "\tTest loss: 0.00324, Accuracy: 223/443 (50.34%)\n",
            "\n",
            "Training Progress: \tEpoch 78 [0/1692 (0.00%)]\t\tLoss: 0.77080\n",
            "Training Progress: \tEpoch 78 [320/1692 (18.87%)]\t\tLoss: 0.77316\n",
            "Training Progress: \tEpoch 78 [640/1692 (37.74%)]\t\tLoss: 0.51389\n",
            "Training Progress: \tEpoch 78 [960/1692 (56.60%)]\t\tLoss: 0.44325\n",
            "Training Progress: \tEpoch 78 [1280/1692 (75.47%)]\t\tLoss: 0.71179\n",
            "Training Progress: \tEpoch 78 [1600/1692 (94.34%)]\t\tLoss: 0.38598\n",
            "\tTrain loss: 0.01108, Accuracy: 1474/1692 (87.12%)\n",
            "\tValidation loss: 0.00319, Accuracy: 229/423 (54.14%)\n",
            "\tTest loss: 0.00327, Accuracy: 228/443 (51.47%)\n",
            "\n",
            "Training Progress: \tEpoch 79 [0/1692 (0.00%)]\t\tLoss: 0.79348\n",
            "Training Progress: \tEpoch 79 [320/1692 (18.87%)]\t\tLoss: 0.63856\n",
            "Training Progress: \tEpoch 79 [640/1692 (37.74%)]\t\tLoss: 0.54422\n",
            "Training Progress: \tEpoch 79 [960/1692 (56.60%)]\t\tLoss: 0.60072\n",
            "Training Progress: \tEpoch 79 [1280/1692 (75.47%)]\t\tLoss: 0.75389\n",
            "Training Progress: \tEpoch 79 [1600/1692 (94.34%)]\t\tLoss: 0.51770\n",
            "\tTrain loss: 0.01314, Accuracy: 1438/1692 (84.99%)\n",
            "\tValidation loss: 0.00334, Accuracy: 205/423 (48.46%)\n",
            "\tTest loss: 0.00332, Accuracy: 211/443 (47.63%)\n",
            "\n",
            "Training Progress: \tEpoch 80 [0/1692 (0.00%)]\t\tLoss: 0.81436\n",
            "Training Progress: \tEpoch 80 [320/1692 (18.87%)]\t\tLoss: 0.61891\n",
            "Training Progress: \tEpoch 80 [640/1692 (37.74%)]\t\tLoss: 0.39308\n",
            "Training Progress: \tEpoch 80 [960/1692 (56.60%)]\t\tLoss: 0.64083\n",
            "Training Progress: \tEpoch 80 [1280/1692 (75.47%)]\t\tLoss: 0.56833\n",
            "Training Progress: \tEpoch 80 [1600/1692 (94.34%)]\t\tLoss: 0.41635\n",
            "\tTrain loss: 0.01258, Accuracy: 1437/1692 (84.93%)\n",
            "\tValidation loss: 0.00335, Accuracy: 212/423 (50.12%)\n",
            "\tTest loss: 0.00347, Accuracy: 209/443 (47.18%)\n",
            "\n",
            "Training Progress: \tEpoch 81 [0/1692 (0.00%)]\t\tLoss: 0.72095\n",
            "Training Progress: \tEpoch 81 [320/1692 (18.87%)]\t\tLoss: 0.73785\n",
            "Training Progress: \tEpoch 81 [640/1692 (37.74%)]\t\tLoss: 0.67092\n",
            "Training Progress: \tEpoch 81 [960/1692 (56.60%)]\t\tLoss: 0.44666\n",
            "Training Progress: \tEpoch 81 [1280/1692 (75.47%)]\t\tLoss: 0.71170\n",
            "Training Progress: \tEpoch 81 [1600/1692 (94.34%)]\t\tLoss: 0.36190\n",
            "\tTrain loss: 0.01193, Accuracy: 1460/1692 (86.29%)\n",
            "\tValidation loss: 0.00323, Accuracy: 218/423 (51.54%)\n",
            "\tTest loss: 0.00330, Accuracy: 212/443 (47.86%)\n",
            "\n",
            "Training Progress: \tEpoch 82 [0/1692 (0.00%)]\t\tLoss: 0.67220\n",
            "Training Progress: \tEpoch 82 [320/1692 (18.87%)]\t\tLoss: 0.51636\n",
            "Training Progress: \tEpoch 82 [640/1692 (37.74%)]\t\tLoss: 0.52126\n",
            "Training Progress: \tEpoch 82 [960/1692 (56.60%)]\t\tLoss: 0.35024\n",
            "Training Progress: \tEpoch 82 [1280/1692 (75.47%)]\t\tLoss: 0.67858\n",
            "Training Progress: \tEpoch 82 [1600/1692 (94.34%)]\t\tLoss: 0.40255\n",
            "\tTrain loss: 0.01211, Accuracy: 1448/1692 (85.58%)\n",
            "\tValidation loss: 0.00340, Accuracy: 212/423 (50.12%)\n",
            "\tTest loss: 0.00348, Accuracy: 214/443 (48.31%)\n",
            "\n",
            "Training Progress: \tEpoch 83 [0/1692 (0.00%)]\t\tLoss: 0.65989\n",
            "Training Progress: \tEpoch 83 [320/1692 (18.87%)]\t\tLoss: 0.60590\n",
            "Training Progress: \tEpoch 83 [640/1692 (37.74%)]\t\tLoss: 0.48353\n",
            "Training Progress: \tEpoch 83 [960/1692 (56.60%)]\t\tLoss: 0.45309\n",
            "Training Progress: \tEpoch 83 [1280/1692 (75.47%)]\t\tLoss: 0.72596\n",
            "Training Progress: \tEpoch 83 [1600/1692 (94.34%)]\t\tLoss: 0.34790\n",
            "\tTrain loss: 0.01148, Accuracy: 1475/1692 (87.17%)\n",
            "\tValidation loss: 0.00328, Accuracy: 214/423 (50.59%)\n",
            "\tTest loss: 0.00328, Accuracy: 218/443 (49.21%)\n",
            "\n",
            "Training Progress: \tEpoch 84 [0/1692 (0.00%)]\t\tLoss: 0.75100\n",
            "Training Progress: \tEpoch 84 [320/1692 (18.87%)]\t\tLoss: 0.60422\n",
            "Training Progress: \tEpoch 84 [640/1692 (37.74%)]\t\tLoss: 0.63038\n",
            "Training Progress: \tEpoch 84 [960/1692 (56.60%)]\t\tLoss: 0.48958\n",
            "Training Progress: \tEpoch 84 [1280/1692 (75.47%)]\t\tLoss: 0.74903\n",
            "Training Progress: \tEpoch 84 [1600/1692 (94.34%)]\t\tLoss: 0.46398\n",
            "\tTrain loss: 0.01154, Accuracy: 1466/1692 (86.64%)\n",
            "\tValidation loss: 0.00333, Accuracy: 211/423 (49.88%)\n",
            "\tTest loss: 0.00336, Accuracy: 211/443 (47.63%)\n",
            "\n",
            "Training Progress: \tEpoch 85 [0/1692 (0.00%)]\t\tLoss: 0.83960\n",
            "Training Progress: \tEpoch 85 [320/1692 (18.87%)]\t\tLoss: 0.63286\n",
            "Training Progress: \tEpoch 85 [640/1692 (37.74%)]\t\tLoss: 0.74647\n",
            "Training Progress: \tEpoch 85 [960/1692 (56.60%)]\t\tLoss: 0.62723\n",
            "Training Progress: \tEpoch 85 [1280/1692 (75.47%)]\t\tLoss: 0.58905\n",
            "Training Progress: \tEpoch 85 [1600/1692 (94.34%)]\t\tLoss: 0.51333\n",
            "\tTrain loss: 0.01263, Accuracy: 1454/1692 (85.93%)\n",
            "\tValidation loss: 0.00329, Accuracy: 212/423 (50.12%)\n",
            "\tTest loss: 0.00345, Accuracy: 206/443 (46.50%)\n",
            "\n",
            "Training Progress: \tEpoch 86 [0/1692 (0.00%)]\t\tLoss: 0.85710\n",
            "Training Progress: \tEpoch 86 [320/1692 (18.87%)]\t\tLoss: 0.67236\n",
            "Training Progress: \tEpoch 86 [640/1692 (37.74%)]\t\tLoss: 0.43989\n",
            "Training Progress: \tEpoch 86 [960/1692 (56.60%)]\t\tLoss: 0.32280\n",
            "Training Progress: \tEpoch 86 [1280/1692 (75.47%)]\t\tLoss: 0.73195\n",
            "Training Progress: \tEpoch 86 [1600/1692 (94.34%)]\t\tLoss: 0.57160\n",
            "\tTrain loss: 0.00977, Accuracy: 1504/1692 (88.89%)\n",
            "\tValidation loss: 0.00323, Accuracy: 210/423 (49.65%)\n",
            "\tTest loss: 0.00338, Accuracy: 217/443 (48.98%)\n",
            "\n",
            "Training Progress: \tEpoch 87 [0/1692 (0.00%)]\t\tLoss: 0.64712\n",
            "Training Progress: \tEpoch 87 [320/1692 (18.87%)]\t\tLoss: 0.77449\n",
            "Training Progress: \tEpoch 87 [640/1692 (37.74%)]\t\tLoss: 0.36252\n",
            "Training Progress: \tEpoch 87 [960/1692 (56.60%)]\t\tLoss: 0.47731\n",
            "Training Progress: \tEpoch 87 [1280/1692 (75.47%)]\t\tLoss: 0.47621\n",
            "Training Progress: \tEpoch 87 [1600/1692 (94.34%)]\t\tLoss: 0.49137\n",
            "\tTrain loss: 0.00952, Accuracy: 1514/1692 (89.48%)\n",
            "\tValidation loss: 0.00317, Accuracy: 221/423 (52.25%)\n",
            "\tTest loss: 0.00339, Accuracy: 212/443 (47.86%)\n",
            "\n",
            "Training Progress: \tEpoch 88 [0/1692 (0.00%)]\t\tLoss: 0.68901\n",
            "Training Progress: \tEpoch 88 [320/1692 (18.87%)]\t\tLoss: 0.53271\n",
            "Training Progress: \tEpoch 88 [640/1692 (37.74%)]\t\tLoss: 0.40150\n",
            "Training Progress: \tEpoch 88 [960/1692 (56.60%)]\t\tLoss: 0.42959\n",
            "Training Progress: \tEpoch 88 [1280/1692 (75.47%)]\t\tLoss: 0.81259\n",
            "Training Progress: \tEpoch 88 [1600/1692 (94.34%)]\t\tLoss: 0.49022\n",
            "\tTrain loss: 0.01082, Accuracy: 1486/1692 (87.83%)\n",
            "\tValidation loss: 0.00326, Accuracy: 213/423 (50.35%)\n",
            "\tTest loss: 0.00330, Accuracy: 211/443 (47.63%)\n",
            "\n",
            "Training Progress: \tEpoch 89 [0/1692 (0.00%)]\t\tLoss: 0.63775\n",
            "Training Progress: \tEpoch 89 [320/1692 (18.87%)]\t\tLoss: 0.44738\n",
            "Training Progress: \tEpoch 89 [640/1692 (37.74%)]\t\tLoss: 0.74675\n",
            "Training Progress: \tEpoch 89 [960/1692 (56.60%)]\t\tLoss: 0.48501\n",
            "Training Progress: \tEpoch 89 [1280/1692 (75.47%)]\t\tLoss: 0.75854\n",
            "Training Progress: \tEpoch 89 [1600/1692 (94.34%)]\t\tLoss: 0.42816\n",
            "\tTrain loss: 0.00903, Accuracy: 1526/1692 (90.19%)\n",
            "\tValidation loss: 0.00320, Accuracy: 214/423 (50.59%)\n",
            "\tTest loss: 0.00337, Accuracy: 210/443 (47.40%)\n",
            "\n",
            "Training Progress: \tEpoch 90 [0/1692 (0.00%)]\t\tLoss: 0.46339\n",
            "Training Progress: \tEpoch 90 [320/1692 (18.87%)]\t\tLoss: 0.64777\n",
            "Training Progress: \tEpoch 90 [640/1692 (37.74%)]\t\tLoss: 0.38866\n",
            "Training Progress: \tEpoch 90 [960/1692 (56.60%)]\t\tLoss: 0.58325\n",
            "Training Progress: \tEpoch 90 [1280/1692 (75.47%)]\t\tLoss: 0.52741\n",
            "Training Progress: \tEpoch 90 [1600/1692 (94.34%)]\t\tLoss: 0.33717\n",
            "\tTrain loss: 0.00868, Accuracy: 1523/1692 (90.01%)\n",
            "\tValidation loss: 0.00329, Accuracy: 215/423 (50.83%)\n",
            "\tTest loss: 0.00349, Accuracy: 228/443 (51.47%)\n",
            "\n",
            "Training Progress: \tEpoch 91 [0/1692 (0.00%)]\t\tLoss: 0.58394\n",
            "Training Progress: \tEpoch 91 [320/1692 (18.87%)]\t\tLoss: 0.47416\n",
            "Training Progress: \tEpoch 91 [640/1692 (37.74%)]\t\tLoss: 0.37804\n",
            "Training Progress: \tEpoch 91 [960/1692 (56.60%)]\t\tLoss: 0.45537\n",
            "Training Progress: \tEpoch 91 [1280/1692 (75.47%)]\t\tLoss: 0.71521\n",
            "Training Progress: \tEpoch 91 [1600/1692 (94.34%)]\t\tLoss: 0.42274\n",
            "\tTrain loss: 0.00966, Accuracy: 1519/1692 (89.78%)\n",
            "\tValidation loss: 0.00326, Accuracy: 214/423 (50.59%)\n",
            "\tTest loss: 0.00347, Accuracy: 218/443 (49.21%)\n",
            "\n",
            "Training Progress: \tEpoch 92 [0/1692 (0.00%)]\t\tLoss: 0.63590\n",
            "Training Progress: \tEpoch 92 [320/1692 (18.87%)]\t\tLoss: 0.51466\n",
            "Training Progress: \tEpoch 92 [640/1692 (37.74%)]\t\tLoss: 0.42213\n",
            "Training Progress: \tEpoch 92 [960/1692 (56.60%)]\t\tLoss: 0.24374\n",
            "Training Progress: \tEpoch 92 [1280/1692 (75.47%)]\t\tLoss: 0.47301\n",
            "Training Progress: \tEpoch 92 [1600/1692 (94.34%)]\t\tLoss: 0.33275\n",
            "\tTrain loss: 0.00824, Accuracy: 1531/1692 (90.48%)\n",
            "\tValidation loss: 0.00332, Accuracy: 215/423 (50.83%)\n",
            "\tTest loss: 0.00353, Accuracy: 218/443 (49.21%)\n",
            "\n",
            "Training Progress: \tEpoch 93 [0/1692 (0.00%)]\t\tLoss: 0.75847\n",
            "Training Progress: \tEpoch 93 [320/1692 (18.87%)]\t\tLoss: 0.61054\n",
            "Training Progress: \tEpoch 93 [640/1692 (37.74%)]\t\tLoss: 0.47257\n",
            "Training Progress: \tEpoch 93 [960/1692 (56.60%)]\t\tLoss: 0.31630\n",
            "Training Progress: \tEpoch 93 [1280/1692 (75.47%)]\t\tLoss: 0.64608\n",
            "Training Progress: \tEpoch 93 [1600/1692 (94.34%)]\t\tLoss: 0.52454\n",
            "\tTrain loss: 0.00929, Accuracy: 1518/1692 (89.72%)\n",
            "\tValidation loss: 0.00347, Accuracy: 202/423 (47.75%)\n",
            "\tTest loss: 0.00364, Accuracy: 214/443 (48.31%)\n",
            "\n",
            "Training Progress: \tEpoch 94 [0/1692 (0.00%)]\t\tLoss: 0.48324\n",
            "Training Progress: \tEpoch 94 [320/1692 (18.87%)]\t\tLoss: 0.52743\n",
            "Training Progress: \tEpoch 94 [640/1692 (37.74%)]\t\tLoss: 0.38354\n",
            "Training Progress: \tEpoch 94 [960/1692 (56.60%)]\t\tLoss: 0.38186\n",
            "Training Progress: \tEpoch 94 [1280/1692 (75.47%)]\t\tLoss: 0.61580\n",
            "Training Progress: \tEpoch 94 [1600/1692 (94.34%)]\t\tLoss: 0.34231\n",
            "\tTrain loss: 0.00904, Accuracy: 1508/1692 (89.13%)\n",
            "\tValidation loss: 0.00336, Accuracy: 205/423 (48.46%)\n",
            "\tTest loss: 0.00350, Accuracy: 219/443 (49.44%)\n",
            "\n",
            "Training Progress: \tEpoch 95 [0/1692 (0.00%)]\t\tLoss: 0.69548\n",
            "Training Progress: \tEpoch 95 [320/1692 (18.87%)]\t\tLoss: 0.41271\n",
            "Training Progress: \tEpoch 95 [640/1692 (37.74%)]\t\tLoss: 0.53297\n",
            "Training Progress: \tEpoch 95 [960/1692 (56.60%)]\t\tLoss: 0.47662\n",
            "Training Progress: \tEpoch 95 [1280/1692 (75.47%)]\t\tLoss: 0.61636\n",
            "Training Progress: \tEpoch 95 [1600/1692 (94.34%)]\t\tLoss: 0.33719\n",
            "\tTrain loss: 0.00899, Accuracy: 1524/1692 (90.07%)\n",
            "\tValidation loss: 0.00343, Accuracy: 206/423 (48.70%)\n",
            "\tTest loss: 0.00350, Accuracy: 220/443 (49.66%)\n",
            "\n",
            "Training Progress: \tEpoch 96 [0/1692 (0.00%)]\t\tLoss: 0.81024\n",
            "Training Progress: \tEpoch 96 [320/1692 (18.87%)]\t\tLoss: 0.51426\n",
            "Training Progress: \tEpoch 96 [640/1692 (37.74%)]\t\tLoss: 0.53647\n",
            "Training Progress: \tEpoch 96 [960/1692 (56.60%)]\t\tLoss: 0.43490\n",
            "Training Progress: \tEpoch 96 [1280/1692 (75.47%)]\t\tLoss: 0.62404\n",
            "Training Progress: \tEpoch 96 [1600/1692 (94.34%)]\t\tLoss: 0.46509\n",
            "\tTrain loss: 0.00811, Accuracy: 1540/1692 (91.02%)\n",
            "\tValidation loss: 0.00348, Accuracy: 199/423 (47.04%)\n",
            "\tTest loss: 0.00354, Accuracy: 210/443 (47.40%)\n",
            "\n",
            "Training Progress: \tEpoch 97 [0/1692 (0.00%)]\t\tLoss: 0.77089\n",
            "Training Progress: \tEpoch 97 [320/1692 (18.87%)]\t\tLoss: 0.62493\n",
            "Training Progress: \tEpoch 97 [640/1692 (37.74%)]\t\tLoss: 0.45219\n",
            "Training Progress: \tEpoch 97 [960/1692 (56.60%)]\t\tLoss: 0.39806\n",
            "Training Progress: \tEpoch 97 [1280/1692 (75.47%)]\t\tLoss: 0.52051\n",
            "Training Progress: \tEpoch 97 [1600/1692 (94.34%)]\t\tLoss: 0.32799\n",
            "\tTrain loss: 0.00769, Accuracy: 1551/1692 (91.67%)\n",
            "\tValidation loss: 0.00347, Accuracy: 210/423 (49.65%)\n",
            "\tTest loss: 0.00347, Accuracy: 218/443 (49.21%)\n",
            "\n",
            "Training Progress: \tEpoch 98 [0/1692 (0.00%)]\t\tLoss: 0.41368\n",
            "Training Progress: \tEpoch 98 [320/1692 (18.87%)]\t\tLoss: 0.64529\n",
            "Training Progress: \tEpoch 98 [640/1692 (37.74%)]\t\tLoss: 0.44369\n",
            "Training Progress: \tEpoch 98 [960/1692 (56.60%)]\t\tLoss: 0.31050\n",
            "Training Progress: \tEpoch 98 [1280/1692 (75.47%)]\t\tLoss: 0.71476\n",
            "Training Progress: \tEpoch 98 [1600/1692 (94.34%)]\t\tLoss: 0.36264\n",
            "\tTrain loss: 0.00911, Accuracy: 1517/1692 (89.66%)\n",
            "\tValidation loss: 0.00350, Accuracy: 208/423 (49.17%)\n",
            "\tTest loss: 0.00363, Accuracy: 206/443 (46.50%)\n",
            "\n",
            "Training Progress: \tEpoch 99 [0/1692 (0.00%)]\t\tLoss: 0.58848\n",
            "Training Progress: \tEpoch 99 [320/1692 (18.87%)]\t\tLoss: 0.50294\n",
            "Training Progress: \tEpoch 99 [640/1692 (37.74%)]\t\tLoss: 0.44431\n",
            "Training Progress: \tEpoch 99 [960/1692 (56.60%)]\t\tLoss: 0.44830\n",
            "Training Progress: \tEpoch 99 [1280/1692 (75.47%)]\t\tLoss: 0.66817\n",
            "Training Progress: \tEpoch 99 [1600/1692 (94.34%)]\t\tLoss: 0.49490\n",
            "\tTrain loss: 0.00785, Accuracy: 1534/1692 (90.66%)\n",
            "\tValidation loss: 0.00355, Accuracy: 211/423 (49.88%)\n",
            "\tTest loss: 0.00373, Accuracy: 215/443 (48.53%)\n",
            "\n",
            "Training Progress: \tEpoch 100 [0/1692 (0.00%)]\t\tLoss: 0.67705\n",
            "Training Progress: \tEpoch 100 [320/1692 (18.87%)]\t\tLoss: 0.42276\n",
            "Training Progress: \tEpoch 100 [640/1692 (37.74%)]\t\tLoss: 0.34243\n",
            "Training Progress: \tEpoch 100 [960/1692 (56.60%)]\t\tLoss: 0.44028\n",
            "Training Progress: \tEpoch 100 [1280/1692 (75.47%)]\t\tLoss: 0.33090\n",
            "Training Progress: \tEpoch 100 [1600/1692 (94.34%)]\t\tLoss: 0.39492\n",
            "\tTrain loss: 0.00684, Accuracy: 1565/1692 (92.49%)\n",
            "\tValidation loss: 0.00367, Accuracy: 204/423 (48.23%)\n",
            "\tTest loss: 0.00382, Accuracy: 212/443 (47.86%)\n",
            "\n",
            "[[tensor(0.0434), 0.26832151300236406], [tensor(0.0434), 0.266548463356974], [tensor(0.0437), 0.27541371158392436], [tensor(0.0431), 0.28841607565011823], [tensor(0.0407), 0.382387706855792], [tensor(0.0411), 0.3599290780141844], [tensor(0.0405), 0.37115839243498816], [tensor(0.0391), 0.4107565011820331], [tensor(0.0389), 0.42257683215130026], [tensor(0.0377), 0.45685579196217496], [tensor(0.0394), 0.425531914893617], [tensor(0.0352), 0.5076832151300237], [tensor(0.0358), 0.48286052009456265], [tensor(0.0375), 0.4657210401891253], [tensor(0.0393), 0.44858156028368795], [tensor(0.0359), 0.4946808510638298], [tensor(0.0344), 0.5212765957446809], [tensor(0.0350), 0.514775413711584], [tensor(0.0326), 0.5543735224586288], [tensor(0.0324), 0.5656028368794326], [tensor(0.0350), 0.5242316784869976], [tensor(0.0335), 0.5419621749408984], [tensor(0.0348), 0.5076832151300237], [tensor(0.0322), 0.5520094562647754], [tensor(0.0305), 0.5868794326241135], [tensor(0.0302), 0.5851063829787234], [tensor(0.0302), 0.5762411347517731], [tensor(0.0313), 0.58274231678487], [tensor(0.0298), 0.5921985815602837], [tensor(0.0322), 0.5697399527186762], [tensor(0.0274), 0.6335697399527187], [tensor(0.0290), 0.6057919621749409], [tensor(0.0267), 0.6365248226950354], [tensor(0.0261), 0.6388888888888888], [tensor(0.0253), 0.66548463356974], [tensor(0.0259), 0.6595744680851063], [tensor(0.0231), 0.6897163120567376], [tensor(0.0253), 0.6613475177304965], [tensor(0.0247), 0.6743498817966903], [tensor(0.0239), 0.6885342789598109], [tensor(0.0238), 0.6713947990543735], [tensor(0.0237), 0.6843971631205674], [tensor(0.0261), 0.6465721040189125], [tensor(0.0235), 0.7009456264775413], [tensor(0.0233), 0.6956264775413712], [tensor(0.0232), 0.6914893617021277], [tensor(0.0232), 0.6914893617021277], [tensor(0.0231), 0.7056737588652482], [tensor(0.0219), 0.7180851063829787], [tensor(0.0196), 0.7423167848699763], [tensor(0.0208), 0.7257683215130024], [tensor(0.0204), 0.7375886524822695], [tensor(0.0193), 0.7535460992907801], [tensor(0.0211), 0.7198581560283688], [tensor(0.0188), 0.7635933806146572], [tensor(0.0197), 0.7464539007092199], [tensor(0.0179), 0.7736406619385343], [tensor(0.0194), 0.7541371158392435], [tensor(0.0178), 0.764775413711584], [tensor(0.0186), 0.7624113475177305], [tensor(0.0189), 0.7588652482269503], [tensor(0.0167), 0.7960992907801419], [tensor(0.0160), 0.8221040189125296], [tensor(0.0162), 0.7984633569739953], [tensor(0.0161), 0.7901891252955082], [tensor(0.0152), 0.8049645390070922], [tensor(0.0159), 0.8020094562647754], [tensor(0.0154), 0.8102836879432624], [tensor(0.0153), 0.8132387706855791], [tensor(0.0151), 0.8102836879432624], [tensor(0.0129), 0.8416075650118203], [tensor(0.0153), 0.8002364066193853], [tensor(0.0130), 0.8457446808510638], [tensor(0.0153), 0.8167848699763594], [tensor(0.0139), 0.8421985815602837], [tensor(0.0130), 0.8439716312056738], [tensor(0.0123), 0.8516548463356974], [tensor(0.0111), 0.8711583924349882], [tensor(0.0131), 0.8498817966903073], [tensor(0.0126), 0.849290780141844], [tensor(0.0119), 0.8628841607565012], [tensor(0.0121), 0.8557919621749409], [tensor(0.0115), 0.8717494089834515], [tensor(0.0115), 0.8664302600472813], [tensor(0.0126), 0.859338061465721], [tensor(0.0098), 0.8888888888888888], [tensor(0.0095), 0.8947990543735225], [tensor(0.0108), 0.8782505910165485], [tensor(0.0090), 0.9018912529550828], [tensor(0.0087), 0.9001182033096927], [tensor(0.0097), 0.8977541371158393], [tensor(0.0082), 0.9048463356973995], [tensor(0.0093), 0.8971631205673759], [tensor(0.0090), 0.8912529550827423], [tensor(0.0090), 0.900709219858156], [tensor(0.0081), 0.9101654846335697], [tensor(0.0077), 0.9166666666666666], [tensor(0.0091), 0.8965721040189125], [tensor(0.0079), 0.9066193853427896], [tensor(0.0068), 0.9249408983451537]]\n",
            "<class 'list'>\n",
            "[[tensor(0.0033), 0.28132387706855794], [tensor(0.0033), 0.2718676122931442], [tensor(0.0033), 0.28132387706855794], [tensor(0.0033), 0.28841607565011823], [tensor(0.0031), 0.35697399527186763], [tensor(0.0032), 0.31678486997635935], [tensor(0.0032), 0.35933806146572106], [tensor(0.0031), 0.3664302600472813], [tensor(0.0030), 0.3947990543735225], [tensor(0.0030), 0.40425531914893614], [tensor(0.0031), 0.3971631205673759], [tensor(0.0029), 0.4373522458628842], [tensor(0.0029), 0.4397163120567376], [tensor(0.0030), 0.45390070921985815], [tensor(0.0032), 0.4160756501182033], [tensor(0.0030), 0.458628841607565], [tensor(0.0029), 0.46808510638297873], [tensor(0.0030), 0.4444444444444444], [tensor(0.0028), 0.491725768321513], [tensor(0.0029), 0.475177304964539], [tensor(0.0030), 0.45390070921985815], [tensor(0.0030), 0.45390070921985815], [tensor(0.0031), 0.44680851063829785], [tensor(0.0030), 0.4728132387706856], [tensor(0.0028), 0.48699763593380613], [tensor(0.0028), 0.48936170212765956], [tensor(0.0029), 0.475177304964539], [tensor(0.0030), 0.4657210401891253], [tensor(0.0029), 0.48936170212765956], [tensor(0.0031), 0.46099290780141844], [tensor(0.0028), 0.5011820330969267], [tensor(0.0030), 0.4657210401891253], [tensor(0.0029), 0.5011820330969267], [tensor(0.0028), 0.5177304964539007], [tensor(0.0027), 0.541371158392435], [tensor(0.0029), 0.49645390070921985], [tensor(0.0028), 0.5200945626477541], [tensor(0.0030), 0.5011820330969267], [tensor(0.0029), 0.4940898345153664], [tensor(0.0028), 0.524822695035461], [tensor(0.0030), 0.4988179669030733], [tensor(0.0030), 0.5035460992907801], [tensor(0.0032), 0.4562647754137116], [tensor(0.0030), 0.4940898345153664], [tensor(0.0031), 0.49645390070921985], [tensor(0.0032), 0.48226950354609927], [tensor(0.0031), 0.4799054373522459], [tensor(0.0031), 0.4988179669030733], [tensor(0.0030), 0.491725768321513], [tensor(0.0029), 0.5319148936170213], [tensor(0.0031), 0.5059101654846335], [tensor(0.0031), 0.4846335697399527], [tensor(0.0030), 0.5035460992907801], [tensor(0.0031), 0.47044917257683216], [tensor(0.0031), 0.5059101654846335], [tensor(0.0031), 0.4799054373522459], [tensor(0.0031), 0.5200945626477541], [tensor(0.0033), 0.4799054373522459], [tensor(0.0032), 0.47754137115839246], [tensor(0.0033), 0.48936170212765956], [tensor(0.0032), 0.48699763593380613], [tensor(0.0033), 0.48699763593380613], [tensor(0.0031), 0.5130023640661938], [tensor(0.0030), 0.5271867612293144], [tensor(0.0031), 0.5011820330969267], [tensor(0.0034), 0.48936170212765956], [tensor(0.0033), 0.524822695035461], [tensor(0.0032), 0.5177304964539007], [tensor(0.0032), 0.541371158392435], [tensor(0.0033), 0.5059101654846335], [tensor(0.0030), 0.5295508274231678], [tensor(0.0033), 0.5059101654846335], [tensor(0.0032), 0.5130023640661938], [tensor(0.0032), 0.5011820330969267], [tensor(0.0032), 0.5319148936170213], [tensor(0.0032), 0.5130023640661938], [tensor(0.0032), 0.524822695035461], [tensor(0.0032), 0.541371158392435], [tensor(0.0033), 0.4846335697399527], [tensor(0.0033), 0.5011820330969267], [tensor(0.0032), 0.5153664302600472], [tensor(0.0034), 0.5011820330969267], [tensor(0.0033), 0.5059101654846335], [tensor(0.0033), 0.4988179669030733], [tensor(0.0033), 0.5011820330969267], [tensor(0.0032), 0.49645390070921985], [tensor(0.0032), 0.5224586288416075], [tensor(0.0033), 0.5035460992907801], [tensor(0.0032), 0.5059101654846335], [tensor(0.0033), 0.508274231678487], [tensor(0.0033), 0.5059101654846335], [tensor(0.0033), 0.508274231678487], [tensor(0.0035), 0.47754137115839246], [tensor(0.0034), 0.4846335697399527], [tensor(0.0034), 0.48699763593380613], [tensor(0.0035), 0.47044917257683216], [tensor(0.0035), 0.49645390070921985], [tensor(0.0035), 0.491725768321513], [tensor(0.0035), 0.4988179669030733], [tensor(0.0037), 0.48226950354609927]]\n",
            "<class 'list'>\n",
            "[[tensor(0.0031), 0.27539503386004516], [tensor(0.0031), 0.26410835214446954], [tensor(0.0031), 0.2708803611738149], [tensor(0.0031), 0.29345372460496616], [tensor(0.0029), 0.38148984198645597], [tensor(0.0030), 0.36794582392776526], [tensor(0.0029), 0.36794582392776526], [tensor(0.0029), 0.41309255079006774], [tensor(0.0028), 0.417607223476298], [tensor(0.0028), 0.45598194130925507], [tensor(0.0029), 0.44469525959367945], [tensor(0.0027), 0.4853273137697517], [tensor(0.0027), 0.48081264108352145], [tensor(0.0027), 0.4762979683972912], [tensor(0.0029), 0.4492099322799097], [tensor(0.0027), 0.48081264108352145], [tensor(0.0026), 0.5056433408577878], [tensor(0.0027), 0.4785553047404063], [tensor(0.0026), 0.4966139954853273], [tensor(0.0026), 0.5011286681715575], [tensor(0.0027), 0.4898419864559819], [tensor(0.0027), 0.46952595936794583], [tensor(0.0028), 0.4966139954853273], [tensor(0.0027), 0.48758465011286684], [tensor(0.0026), 0.510158013544018], [tensor(0.0026), 0.510158013544018], [tensor(0.0027), 0.5011286681715575], [tensor(0.0027), 0.4898419864559819], [tensor(0.0026), 0.4966139954853273], [tensor(0.0028), 0.48306997742663654], [tensor(0.0026), 0.5191873589164786], [tensor(0.0027), 0.4966139954853273], [tensor(0.0026), 0.49887133182844245], [tensor(0.0026), 0.5033860045146726], [tensor(0.0026), 0.5191873589164786], [tensor(0.0027), 0.5169300225733634], [tensor(0.0027), 0.5079006772009029], [tensor(0.0028), 0.4898419864559819], [tensor(0.0027), 0.5146726862302483], [tensor(0.0026), 0.49209932279909707], [tensor(0.0027), 0.5056433408577878], [tensor(0.0027), 0.5124153498871332], [tensor(0.0030), 0.5124153498871332], [tensor(0.0028), 0.5056433408577878], [tensor(0.0028), 0.5079006772009029], [tensor(0.0028), 0.5259593679458239], [tensor(0.0028), 0.5327313769751693], [tensor(0.0029), 0.5395033860045146], [tensor(0.0029), 0.49887133182844245], [tensor(0.0029), 0.510158013544018], [tensor(0.0029), 0.5191873589164786], [tensor(0.0030), 0.4898419864559819], [tensor(0.0028), 0.5033860045146726], [tensor(0.0030), 0.48758465011286684], [tensor(0.0029), 0.5124153498871332], [tensor(0.0029), 0.48081264108352145], [tensor(0.0029), 0.5033860045146726], [tensor(0.0031), 0.4966139954853273], [tensor(0.0030), 0.49209932279909707], [tensor(0.0031), 0.48306997742663654], [tensor(0.0031), 0.4717832957110609], [tensor(0.0031), 0.49435665914221216], [tensor(0.0031), 0.4853273137697517], [tensor(0.0030), 0.4717832957110609], [tensor(0.0030), 0.48306997742663654], [tensor(0.0031), 0.5146726862302483], [tensor(0.0031), 0.510158013544018], [tensor(0.0031), 0.5169300225733634], [tensor(0.0031), 0.49209932279909707], [tensor(0.0032), 0.48306997742663654], [tensor(0.0030), 0.49435665914221216], [tensor(0.0032), 0.4785553047404063], [tensor(0.0032), 0.4672686230248307], [tensor(0.0033), 0.4717832957110609], [tensor(0.0032), 0.48081264108352145], [tensor(0.0033), 0.4785553047404063], [tensor(0.0032), 0.5033860045146726], [tensor(0.0033), 0.5146726862302483], [tensor(0.0033), 0.4762979683972912], [tensor(0.0035), 0.4717832957110609], [tensor(0.0033), 0.4785553047404063], [tensor(0.0035), 0.48306997742663654], [tensor(0.0033), 0.49209932279909707], [tensor(0.0034), 0.4762979683972912], [tensor(0.0035), 0.4650112866817156], [tensor(0.0034), 0.4898419864559819], [tensor(0.0034), 0.4785553047404063], [tensor(0.0033), 0.4762979683972912], [tensor(0.0034), 0.47404063205417607], [tensor(0.0035), 0.5146726862302483], [tensor(0.0035), 0.49209932279909707], [tensor(0.0035), 0.49209932279909707], [tensor(0.0036), 0.48306997742663654], [tensor(0.0035), 0.49435665914221216], [tensor(0.0035), 0.4966139954853273], [tensor(0.0035), 0.47404063205417607], [tensor(0.0035), 0.49209932279909707], [tensor(0.0036), 0.4650112866817156], [tensor(0.0037), 0.4853273137697517], [tensor(0.0038), 0.4785553047404063]]\n",
            "<class 'list'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Best validation accuracy:\n",
        "0.5555\n",
        "\n",
        "Best test accuracy:\n",
        "0.5395\n",
        "\n",
        "## Plotting Metrics v/s Number of Epochs: \n",
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "Y2HSLEBvmOAF"
      }
    }
  ]
}